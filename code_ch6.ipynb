{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rnnlm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "#from common.time_layers import *\n",
    "#from common.base_model import BaseModel\n",
    "\n",
    "\n",
    "class Rnnlm(BaseModel):\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        # 가중치 초기화\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layer = self.layers[1]\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        score = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.lstm_layer.reset_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# better_rnnlm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "#from common.time_layers import *\n",
    "#from common.np import *  # import numpy as np\n",
    "#from common.base_model import BaseModel\n",
    "\n",
    "\n",
    "class BetterRnnlm(BaseModel):\n",
    "    '''\n",
    "     LSTM 계층을 2개 사용하고 각 층에 드롭아웃을 적용한 모델이다.\n",
    "     아래 [1]에서 제안한 모델을 기초로 하였고, [2]와 [3]의 가중치 공유(weight tying)를 적용했다.\n",
    "\n",
    "     [1] Recurrent Neural Network Regularization (https://arxiv.org/abs/1409.2329)\n",
    "     [2] Using the Output Embedding to Improve Language Models (https://arxiv.org/abs/1608.05859)\n",
    "     [3] Tying Word Vectors and Word Classifiers (https://arxiv.org/pdf/1611.01462.pdf)\n",
    "    '''\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=650,\n",
    "                 hidden_size=650, dropout_ratio=0.5):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx1 = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh1 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b1 = np.zeros(4 * H).astype('f')\n",
    "        lstm_Wx2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_Wh2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b2 = np.zeros(4 * H).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=True),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=True),\n",
    "            TimeDropout(dropout_ratio),\n",
    "            TimeAffine(embed_W.T, affine_b)  # weight tying!!\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layers = [self.layers[2], self.layers[4]]\n",
    "        self.drop_layers = [self.layers[1], self.layers[3], self.layers[5]]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs, train_flg=False):\n",
    "        for layer in self.drop_layers:\n",
    "            layer.train_flg = train_flg\n",
    "\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts, train_flg=True):\n",
    "        score = self.predict(xs, train_flg)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        for layer in self.lstm_layers:\n",
    "            layer.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clip_grads.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: [5.04887101 1.39367805 0.26751679 9.66814467 0.17304446 9.06564587\n",
      " 0.55717293 1.99489946 0.27001697]\n",
      "after: [1.37950872 0.38079622 0.07309391 2.64163808 0.04728113 2.47701665\n",
      " 0.15223699 0.54506863 0.07377704]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dW1 = np.random.rand(3, 3) * 10\n",
    "dW2 = np.random.rand(3, 3) * 10\n",
    "grads = [dW1, dW2]\n",
    "max_norm = 5.0\n",
    "\n",
    "\n",
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate\n",
    "\n",
    "\n",
    "print('before:', dW1.flatten())\n",
    "clip_grads(grads, max_norm)\n",
    "print('after:', dW1.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rnn_gradient_graph.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.4684068094579303, 3.3357049741610365, 4.783279375373182, 6.279587332087612, 8.080776465019053, 10.251163032292936, 12.936063506609896, 16.276861327786712, 20.45482961834598, 25.688972842084684, 32.25315718048336, 40.48895641683869, 50.8244073070191, 63.79612654485427, 80.07737014308985, 100.5129892205125, 126.16331847536823, 158.35920648258823, 198.7710796761195, 249.495615421267]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 49884 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 44036 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 53356 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 44592 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 45432 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 47492 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 49884 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 44036 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 53356 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 44592 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 45432 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 47492 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N = 2   # 미니배치 크기\n",
    "H = 3   # 은닉 상태 벡터의 차원 수\n",
    "T = 20  # 시계열 데이터의 길이\n",
    "\n",
    "dh = np.ones((N, H))\n",
    "\n",
    "np.random.seed(3) # 재현할 수 있도록 난수의 시드 고정\n",
    "\n",
    "Wh = np.random.randn(H, H)\n",
    "#Wh = np.random.randn(H, H) * 0.5\n",
    "\n",
    "norm_list = []\n",
    "for t in range(T):\n",
    "    dh = np.dot(dh, Wh.T)\n",
    "    norm = np.sqrt(np.sum(dh**2)) / N\n",
    "    norm_list.append(norm)\n",
    "\n",
    "print(norm_list)\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.plot(np.arange(len(norm_list)), norm_list)\n",
    "plt.xticks([0, 4, 9, 14, 19], [1, 5, 10, 15, 20])\n",
    "plt.xlabel('시간 크기(time step)')\n",
    "plt.ylabel('노름(norm)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_rnnlm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 1327 | 시간 1[s] | 퍼플렉서티 9996.42\n",
      "| 에폭 1 |  반복 21 / 1327 | 시간 17[s] | 퍼플렉서티 2871.38\n",
      "| 에폭 1 |  반복 41 / 1327 | 시간 32[s] | 퍼플렉서티 1209.36\n",
      "| 에폭 1 |  반복 61 / 1327 | 시간 42[s] | 퍼플렉서티 956.09\n",
      "| 에폭 1 |  반복 81 / 1327 | 시간 52[s] | 퍼플렉서티 802.61\n",
      "| 에폭 1 |  반복 101 / 1327 | 시간 62[s] | 퍼플렉서티 645.99\n",
      "| 에폭 1 |  반복 121 / 1327 | 시간 74[s] | 퍼플렉서티 646.51\n",
      "| 에폭 1 |  반복 141 / 1327 | 시간 85[s] | 퍼플렉서티 610.41\n",
      "| 에폭 1 |  반복 161 / 1327 | 시간 97[s] | 퍼플렉서티 566.09\n",
      "| 에폭 1 |  반복 181 / 1327 | 시간 108[s] | 퍼플렉서티 597.39\n",
      "| 에폭 1 |  반복 201 / 1327 | 시간 118[s] | 퍼플렉서티 511.05\n",
      "| 에폭 1 |  반복 221 / 1327 | 시간 128[s] | 퍼플렉서티 488.65\n",
      "| 에폭 1 |  반복 241 / 1327 | 시간 139[s] | 퍼플렉서티 452.54\n",
      "| 에폭 1 |  반복 261 / 1327 | 시간 149[s] | 퍼플렉서티 466.79\n",
      "| 에폭 1 |  반복 281 / 1327 | 시간 159[s] | 퍼플렉서티 455.55\n",
      "| 에폭 1 |  반복 301 / 1327 | 시간 169[s] | 퍼플렉서티 391.53\n",
      "| 에폭 1 |  반복 321 / 1327 | 시간 180[s] | 퍼플렉서티 350.47\n",
      "| 에폭 1 |  반복 341 / 1327 | 시간 189[s] | 퍼플렉서티 400.35\n",
      "| 에폭 1 |  반복 361 / 1327 | 시간 200[s] | 퍼플렉서티 404.54\n",
      "| 에폭 1 |  반복 381 / 1327 | 시간 212[s] | 퍼플렉서티 340.74\n",
      "| 에폭 1 |  반복 401 / 1327 | 시간 223[s] | 퍼플렉서티 351.58\n",
      "| 에폭 1 |  반복 421 / 1327 | 시간 235[s] | 퍼플렉서티 343.35\n",
      "| 에폭 1 |  반복 441 / 1327 | 시간 247[s] | 퍼플렉서티 333.37\n",
      "| 에폭 1 |  반복 461 / 1327 | 시간 260[s] | 퍼플렉서티 322.75\n",
      "| 에폭 1 |  반복 481 / 1327 | 시간 269[s] | 퍼플렉서티 300.20\n",
      "| 에폭 1 |  반복 501 / 1327 | 시간 280[s] | 퍼플렉서티 307.12\n",
      "| 에폭 1 |  반복 521 / 1327 | 시간 291[s] | 퍼플렉서티 301.28\n",
      "| 에폭 1 |  반복 541 / 1327 | 시간 300[s] | 퍼플렉서티 314.97\n",
      "| 에폭 1 |  반복 561 / 1327 | 시간 309[s] | 퍼플렉서티 284.79\n",
      "| 에폭 1 |  반복 581 / 1327 | 시간 317[s] | 퍼플렉서티 255.90\n",
      "| 에폭 1 |  반복 601 / 1327 | 시간 326[s] | 퍼플렉서티 333.54\n",
      "| 에폭 1 |  반복 621 / 1327 | 시간 335[s] | 퍼플렉서티 305.43\n",
      "| 에폭 1 |  반복 641 / 1327 | 시간 343[s] | 퍼플렉서티 281.06\n",
      "| 에폭 1 |  반복 661 / 1327 | 시간 352[s] | 퍼플렉서티 265.18\n",
      "| 에폭 1 |  반복 681 / 1327 | 시간 360[s] | 퍼플렉서티 225.14\n",
      "| 에폭 1 |  반복 701 / 1327 | 시간 369[s] | 퍼플렉서티 248.17\n",
      "| 에폭 1 |  반복 721 / 1327 | 시간 378[s] | 퍼플렉서티 257.91\n",
      "| 에폭 1 |  반복 741 / 1327 | 시간 386[s] | 퍼플렉서티 216.82\n",
      "| 에폭 1 |  반복 761 / 1327 | 시간 395[s] | 퍼플렉서티 231.75\n",
      "| 에폭 1 |  반복 781 / 1327 | 시간 403[s] | 퍼플렉서티 217.34\n",
      "| 에폭 1 |  반복 801 / 1327 | 시간 412[s] | 퍼플렉서티 240.33\n",
      "| 에폭 1 |  반복 821 / 1327 | 시간 421[s] | 퍼플렉서티 222.60\n",
      "| 에폭 1 |  반복 841 / 1327 | 시간 429[s] | 퍼플렉서티 226.80\n",
      "| 에폭 1 |  반복 861 / 1327 | 시간 438[s] | 퍼플렉서티 218.47\n",
      "| 에폭 1 |  반복 881 / 1327 | 시간 446[s] | 퍼플렉서티 203.57\n",
      "| 에폭 1 |  반복 901 / 1327 | 시간 455[s] | 퍼플렉서티 250.47\n",
      "| 에폭 1 |  반복 921 / 1327 | 시간 463[s] | 퍼플렉서티 224.69\n",
      "| 에폭 1 |  반복 941 / 1327 | 시간 472[s] | 퍼플렉서티 227.66\n",
      "| 에폭 1 |  반복 961 / 1327 | 시간 481[s] | 퍼플렉서티 243.35\n",
      "| 에폭 1 |  반복 981 / 1327 | 시간 489[s] | 퍼플렉서티 227.33\n",
      "| 에폭 1 |  반복 1001 / 1327 | 시간 498[s] | 퍼플렉서티 191.82\n",
      "| 에폭 1 |  반복 1021 / 1327 | 시간 507[s] | 퍼플렉서티 222.93\n",
      "| 에폭 1 |  반복 1041 / 1327 | 시간 515[s] | 퍼플렉서티 203.26\n",
      "| 에폭 1 |  반복 1061 / 1327 | 시간 524[s] | 퍼플렉서티 193.00\n",
      "| 에폭 1 |  반복 1081 / 1327 | 시간 533[s] | 퍼플렉서티 164.56\n",
      "| 에폭 1 |  반복 1101 / 1327 | 시간 541[s] | 퍼플렉서티 187.68\n",
      "| 에폭 1 |  반복 1121 / 1327 | 시간 550[s] | 퍼플렉서티 224.68\n",
      "| 에폭 1 |  반복 1141 / 1327 | 시간 558[s] | 퍼플렉서티 206.42\n",
      "| 에폭 1 |  반복 1161 / 1327 | 시간 567[s] | 퍼플렉서티 196.77\n",
      "| 에폭 1 |  반복 1181 / 1327 | 시간 575[s] | 퍼플렉서티 186.31\n",
      "| 에폭 1 |  반복 1201 / 1327 | 시간 584[s] | 퍼플렉서티 159.00\n",
      "| 에폭 1 |  반복 1221 / 1327 | 시간 592[s] | 퍼플렉서티 157.22\n",
      "| 에폭 1 |  반복 1241 / 1327 | 시간 601[s] | 퍼플렉서티 184.63\n",
      "| 에폭 1 |  반복 1261 / 1327 | 시간 609[s] | 퍼플렉서티 168.66\n",
      "| 에폭 1 |  반복 1281 / 1327 | 시간 615[s] | 퍼플렉서티 176.74\n",
      "| 에폭 1 |  반복 1301 / 1327 | 시간 626[s] | 퍼플렉서티 219.93\n",
      "| 에폭 1 |  반복 1321 / 1327 | 시간 634[s] | 퍼플렉서티 208.66\n",
      "| 에폭 2 |  반복 1 / 1327 | 시간 638[s] | 퍼플렉서티 220.30\n",
      "| 에폭 2 |  반복 21 / 1327 | 시간 644[s] | 퍼플렉서티 201.39\n",
      "| 에폭 2 |  반복 41 / 1327 | 시간 650[s] | 퍼플렉서티 186.53\n",
      "| 에폭 2 |  반복 61 / 1327 | 시간 656[s] | 퍼플렉서티 173.17\n",
      "| 에폭 2 |  반복 81 / 1327 | 시간 662[s] | 퍼플렉서티 156.09\n",
      "| 에폭 2 |  반복 101 / 1327 | 시간 668[s] | 퍼플렉서티 150.83\n",
      "| 에폭 2 |  반복 121 / 1327 | 시간 674[s] | 퍼플렉서티 157.54\n",
      "| 에폭 2 |  반복 141 / 1327 | 시간 680[s] | 퍼플렉서티 174.85\n",
      "| 에폭 2 |  반복 161 / 1327 | 시간 686[s] | 퍼플렉서티 189.06\n",
      "| 에폭 2 |  반복 181 / 1327 | 시간 692[s] | 퍼플렉서티 198.87\n",
      "| 에폭 2 |  반복 201 / 1327 | 시간 698[s] | 퍼플렉서티 183.73\n",
      "| 에폭 2 |  반복 221 / 1327 | 시간 703[s] | 퍼플렉서티 181.54\n",
      "| 에폭 2 |  반복 241 / 1327 | 시간 709[s] | 퍼플렉서티 174.37\n",
      "| 에폭 2 |  반복 261 / 1327 | 시간 715[s] | 퍼플렉서티 183.84\n",
      "| 에폭 2 |  반복 281 / 1327 | 시간 721[s] | 퍼플렉서티 180.84\n",
      "| 에폭 2 |  반복 301 / 1327 | 시간 727[s] | 퍼플렉서티 165.28\n",
      "| 에폭 2 |  반복 321 / 1327 | 시간 733[s] | 퍼플렉서티 135.38\n",
      "| 에폭 2 |  반복 341 / 1327 | 시간 739[s] | 퍼플렉서티 170.21\n",
      "| 에폭 2 |  반복 361 / 1327 | 시간 745[s] | 퍼플렉서티 193.39\n",
      "| 에폭 2 |  반복 381 / 1327 | 시간 750[s] | 퍼플렉서티 150.73\n",
      "| 에폭 2 |  반복 401 / 1327 | 시간 756[s] | 퍼플렉서티 165.71\n",
      "| 에폭 2 |  반복 421 / 1327 | 시간 762[s] | 퍼플렉서티 152.27\n",
      "| 에폭 2 |  반복 441 / 1327 | 시간 768[s] | 퍼플렉서티 161.07\n",
      "| 에폭 2 |  반복 461 / 1327 | 시간 774[s] | 퍼플렉서티 154.59\n",
      "| 에폭 2 |  반복 481 / 1327 | 시간 780[s] | 퍼플렉서티 153.99\n",
      "| 에폭 2 |  반복 501 / 1327 | 시간 786[s] | 퍼플렉서티 167.63\n",
      "| 에폭 2 |  반복 521 / 1327 | 시간 792[s] | 퍼플렉서티 170.99\n",
      "| 에폭 2 |  반복 541 / 1327 | 시간 798[s] | 퍼플렉서티 170.68\n",
      "| 에폭 2 |  반복 561 / 1327 | 시간 804[s] | 퍼플렉서티 155.43\n",
      "| 에폭 2 |  반복 581 / 1327 | 시간 810[s] | 퍼플렉서티 137.56\n",
      "| 에폭 2 |  반복 601 / 1327 | 시간 816[s] | 퍼플렉서티 188.16\n",
      "| 에폭 2 |  반복 621 / 1327 | 시간 822[s] | 퍼플렉서티 177.78\n",
      "| 에폭 2 |  반복 641 / 1327 | 시간 827[s] | 퍼플렉서티 161.75\n",
      "| 에폭 2 |  반복 661 / 1327 | 시간 833[s] | 퍼플렉서티 152.28\n",
      "| 에폭 2 |  반복 681 / 1327 | 시간 839[s] | 퍼플렉서티 128.13\n",
      "| 에폭 2 |  반복 701 / 1327 | 시간 845[s] | 퍼플렉서티 148.91\n",
      "| 에폭 2 |  반복 721 / 1327 | 시간 851[s] | 퍼플렉서티 156.50\n",
      "| 에폭 2 |  반복 741 / 1327 | 시간 857[s] | 퍼플렉서티 130.82\n",
      "| 에폭 2 |  반복 761 / 1327 | 시간 863[s] | 퍼플렉서티 126.90\n",
      "| 에폭 2 |  반복 781 / 1327 | 시간 869[s] | 퍼플렉서티 132.03\n",
      "| 에폭 2 |  반복 801 / 1327 | 시간 875[s] | 퍼플렉서티 146.82\n",
      "| 에폭 2 |  반복 821 / 1327 | 시간 881[s] | 퍼플렉서티 142.69\n",
      "| 에폭 2 |  반복 841 / 1327 | 시간 887[s] | 퍼플렉서티 142.94\n",
      "| 에폭 2 |  반복 861 / 1327 | 시간 892[s] | 퍼플렉서티 143.97\n",
      "| 에폭 2 |  반복 881 / 1327 | 시간 898[s] | 퍼플렉서티 128.11\n",
      "| 에폭 2 |  반복 901 / 1327 | 시간 904[s] | 퍼플렉서티 165.41\n",
      "| 에폭 2 |  반복 921 / 1327 | 시간 910[s] | 퍼플렉서티 145.82\n",
      "| 에폭 2 |  반복 941 / 1327 | 시간 916[s] | 퍼플렉서티 150.93\n",
      "| 에폭 2 |  반복 961 / 1327 | 시간 922[s] | 퍼플렉서티 162.22\n",
      "| 에폭 2 |  반복 981 / 1327 | 시간 928[s] | 퍼플렉서티 152.87\n",
      "| 에폭 2 |  반복 1001 / 1327 | 시간 934[s] | 퍼플렉서티 130.45\n",
      "| 에폭 2 |  반복 1021 / 1327 | 시간 940[s] | 퍼플렉서티 155.32\n",
      "| 에폭 2 |  반복 1041 / 1327 | 시간 946[s] | 퍼플렉서티 139.98\n",
      "| 에폭 2 |  반복 1061 / 1327 | 시간 953[s] | 퍼플렉서티 125.97\n",
      "| 에폭 2 |  반복 1081 / 1327 | 시간 958[s] | 퍼플렉서티 109.93\n",
      "| 에폭 2 |  반복 1101 / 1327 | 시간 964[s] | 퍼플렉서티 117.97\n",
      "| 에폭 2 |  반복 1121 / 1327 | 시간 970[s] | 퍼플렉서티 151.04\n",
      "| 에폭 2 |  반복 1141 / 1327 | 시간 976[s] | 퍼플렉서티 140.26\n",
      "| 에폭 2 |  반복 1161 / 1327 | 시간 982[s] | 퍼플렉서티 132.36\n",
      "| 에폭 2 |  반복 1181 / 1327 | 시간 988[s] | 퍼플렉서티 131.08\n",
      "| 에폭 2 |  반복 1201 / 1327 | 시간 994[s] | 퍼플렉서티 110.88\n",
      "| 에폭 2 |  반복 1221 / 1327 | 시간 999[s] | 퍼플렉서티 108.43\n",
      "| 에폭 2 |  반복 1241 / 1327 | 시간 1005[s] | 퍼플렉서티 128.71\n",
      "| 에폭 2 |  반복 1261 / 1327 | 시간 1011[s] | 퍼플렉서티 122.00\n",
      "| 에폭 2 |  반복 1281 / 1327 | 시간 1017[s] | 퍼플렉서티 123.32\n",
      "| 에폭 2 |  반복 1301 / 1327 | 시간 1023[s] | 퍼플렉서티 154.89\n",
      "| 에폭 2 |  반복 1321 / 1327 | 시간 1029[s] | 퍼플렉서티 151.35\n",
      "| 에폭 3 |  반복 1 / 1327 | 시간 1031[s] | 퍼플렉서티 159.66\n",
      "| 에폭 3 |  반복 21 / 1327 | 시간 1037[s] | 퍼플렉서티 142.07\n",
      "| 에폭 3 |  반복 41 / 1327 | 시간 1042[s] | 퍼플렉서티 133.34\n",
      "| 에폭 3 |  반복 61 / 1327 | 시간 1048[s] | 퍼플렉서티 124.70\n",
      "| 에폭 3 |  반복 81 / 1327 | 시간 1054[s] | 퍼플렉서티 115.39\n",
      "| 에폭 3 |  반복 101 / 1327 | 시간 1060[s] | 퍼플렉서티 103.98\n",
      "| 에폭 3 |  반복 121 / 1327 | 시간 1066[s] | 퍼플렉서티 115.81\n",
      "| 에폭 3 |  반복 141 / 1327 | 시간 1072[s] | 퍼플렉서티 123.90\n",
      "| 에폭 3 |  반복 161 / 1327 | 시간 1078[s] | 퍼플렉서티 140.46\n",
      "| 에폭 3 |  반복 181 / 1327 | 시간 1083[s] | 퍼플렉서티 150.90\n",
      "| 에폭 3 |  반복 201 / 1327 | 시간 1089[s] | 퍼플렉서티 139.69\n",
      "| 에폭 3 |  반복 221 / 1327 | 시간 1095[s] | 퍼플렉서티 138.81\n",
      "| 에폭 3 |  반복 241 / 1327 | 시간 1101[s] | 퍼플렉서티 133.44\n",
      "| 에폭 3 |  반복 261 / 1327 | 시간 1107[s] | 퍼플렉서티 137.37\n",
      "| 에폭 3 |  반복 281 / 1327 | 시간 1113[s] | 퍼플렉서티 140.67\n",
      "| 에폭 3 |  반복 301 / 1327 | 시간 1119[s] | 퍼플렉서티 123.25\n",
      "| 에폭 3 |  반복 321 / 1327 | 시간 1125[s] | 퍼플렉서티 100.69\n",
      "| 에폭 3 |  반복 341 / 1327 | 시간 1131[s] | 퍼플렉서티 123.83\n",
      "| 에폭 3 |  반복 361 / 1327 | 시간 1137[s] | 퍼플렉서티 149.65\n",
      "| 에폭 3 |  반복 381 / 1327 | 시간 1143[s] | 퍼플렉서티 113.97\n",
      "| 에폭 3 |  반복 401 / 1327 | 시간 1149[s] | 퍼플렉서티 129.24\n",
      "| 에폭 3 |  반복 421 / 1327 | 시간 1155[s] | 퍼플렉서티 112.57\n",
      "| 에폭 3 |  반복 441 / 1327 | 시간 1161[s] | 퍼플렉서티 122.72\n",
      "| 에폭 3 |  반복 461 / 1327 | 시간 1167[s] | 퍼플렉서티 117.02\n",
      "| 에폭 3 |  반복 481 / 1327 | 시간 1173[s] | 퍼플렉서티 118.34\n",
      "| 에폭 3 |  반복 501 / 1327 | 시간 1178[s] | 퍼플렉서티 127.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 3 |  반복 521 / 1327 | 시간 1184[s] | 퍼플렉서티 136.31\n",
      "| 에폭 3 |  반복 541 / 1327 | 시간 1190[s] | 퍼플렉서티 134.81\n",
      "| 에폭 3 |  반복 561 / 1327 | 시간 1196[s] | 퍼플렉서티 117.81\n",
      "| 에폭 3 |  반복 581 / 1327 | 시간 1202[s] | 퍼플렉서티 105.03\n",
      "| 에폭 3 |  반복 601 / 1327 | 시간 1208[s] | 퍼플렉서티 147.72\n",
      "| 에폭 3 |  반복 621 / 1327 | 시간 1214[s] | 퍼플렉서티 140.19\n",
      "| 에폭 3 |  반복 641 / 1327 | 시간 1220[s] | 퍼플렉서티 127.22\n",
      "| 에폭 3 |  반복 661 / 1327 | 시간 1226[s] | 퍼플렉서티 118.98\n",
      "| 에폭 3 |  반복 681 / 1327 | 시간 1232[s] | 퍼플렉서티 98.33\n",
      "| 에폭 3 |  반복 701 / 1327 | 시간 1238[s] | 퍼플렉서티 116.83\n",
      "| 에폭 3 |  반복 721 / 1327 | 시간 1244[s] | 퍼플렉서티 125.21\n",
      "| 에폭 3 |  반복 741 / 1327 | 시간 1250[s] | 퍼플렉서티 106.88\n",
      "| 에폭 3 |  반복 761 / 1327 | 시간 1256[s] | 퍼플렉서티 101.10\n",
      "| 에폭 3 |  반복 781 / 1327 | 시간 1262[s] | 퍼플렉서티 103.05\n",
      "| 에폭 3 |  반복 801 / 1327 | 시간 1268[s] | 퍼플렉서티 116.22\n",
      "| 에폭 3 |  반복 821 / 1327 | 시간 1274[s] | 퍼플렉서티 116.60\n",
      "| 에폭 3 |  반복 841 / 1327 | 시간 1280[s] | 퍼플렉서티 112.89\n",
      "| 에폭 3 |  반복 861 / 1327 | 시간 1285[s] | 퍼플렉서티 118.68\n",
      "| 에폭 3 |  반복 881 / 1327 | 시간 1291[s] | 퍼플렉서티 104.57\n",
      "| 에폭 3 |  반복 901 / 1327 | 시간 1297[s] | 퍼플렉서티 131.28\n",
      "| 에폭 3 |  반복 921 / 1327 | 시간 1303[s] | 퍼플렉서티 117.74\n",
      "| 에폭 3 |  반복 941 / 1327 | 시간 1309[s] | 퍼플렉서티 125.82\n",
      "| 에폭 3 |  반복 961 / 1327 | 시간 1315[s] | 퍼플렉서티 130.09\n",
      "| 에폭 3 |  반복 981 / 1327 | 시간 1321[s] | 퍼플렉서티 122.31\n",
      "| 에폭 3 |  반복 1001 / 1327 | 시간 1327[s] | 퍼플렉서티 109.05\n",
      "| 에폭 3 |  반복 1021 / 1327 | 시간 1332[s] | 퍼플렉서티 128.19\n",
      "| 에폭 3 |  반복 1041 / 1327 | 시간 1338[s] | 퍼플렉서티 117.41\n",
      "| 에폭 3 |  반복 1061 / 1327 | 시간 1344[s] | 퍼플렉서티 101.86\n",
      "| 에폭 3 |  반복 1081 / 1327 | 시간 1350[s] | 퍼플렉서티 89.03\n",
      "| 에폭 3 |  반복 1101 / 1327 | 시간 1356[s] | 퍼플렉서티 93.64\n",
      "| 에폭 3 |  반복 1121 / 1327 | 시간 1362[s] | 퍼플렉서티 120.87\n",
      "| 에폭 3 |  반복 1141 / 1327 | 시간 1368[s] | 퍼플렉서티 113.84\n",
      "| 에폭 3 |  반복 1161 / 1327 | 시간 1373[s] | 퍼플렉서티 105.25\n",
      "| 에폭 3 |  반복 1181 / 1327 | 시간 1379[s] | 퍼플렉서티 109.26\n",
      "| 에폭 3 |  반복 1201 / 1327 | 시간 1385[s] | 퍼플렉서티 93.38\n",
      "| 에폭 3 |  반복 1221 / 1327 | 시간 1391[s] | 퍼플렉서티 89.02\n",
      "| 에폭 3 |  반복 1241 / 1327 | 시간 1398[s] | 퍼플렉서티 105.01\n",
      "| 에폭 3 |  반복 1261 / 1327 | 시간 1403[s] | 퍼플렉서티 104.58\n",
      "| 에폭 3 |  반복 1281 / 1327 | 시간 1409[s] | 퍼플렉서티 102.17\n",
      "| 에폭 3 |  반복 1301 / 1327 | 시간 1415[s] | 퍼플렉서티 127.12\n",
      "| 에폭 3 |  반복 1321 / 1327 | 시간 1421[s] | 퍼플렉서티 125.17\n",
      "| 에폭 4 |  반복 1 / 1327 | 시간 1423[s] | 퍼플렉서티 131.56\n",
      "| 에폭 4 |  반복 21 / 1327 | 시간 1429[s] | 퍼플렉서티 120.48\n",
      "| 에폭 4 |  반복 41 / 1327 | 시간 1435[s] | 퍼플렉서티 107.15\n",
      "| 에폭 4 |  반복 61 / 1327 | 시간 1441[s] | 퍼플렉서티 105.60\n",
      "| 에폭 4 |  반복 81 / 1327 | 시간 1447[s] | 퍼플렉서티 96.69\n",
      "| 에폭 4 |  반복 101 / 1327 | 시간 1453[s] | 퍼플렉서티 84.88\n",
      "| 에폭 4 |  반복 121 / 1327 | 시간 1459[s] | 퍼플렉서티 95.89\n",
      "| 에폭 4 |  반복 141 / 1327 | 시간 1465[s] | 퍼플렉서티 102.46\n",
      "| 에폭 4 |  반복 161 / 1327 | 시간 1470[s] | 퍼플렉서티 117.31\n",
      "| 에폭 4 |  반복 181 / 1327 | 시간 1476[s] | 퍼플렉서티 129.53\n",
      "| 에폭 4 |  반복 201 / 1327 | 시간 1482[s] | 퍼플렉서티 118.79\n",
      "| 에폭 4 |  반복 221 / 1327 | 시간 1488[s] | 퍼플렉서티 120.80\n",
      "| 에폭 4 |  반복 241 / 1327 | 시간 1494[s] | 퍼플렉서티 114.48\n",
      "| 에폭 4 |  반복 261 / 1327 | 시간 1500[s] | 퍼플렉서티 113.56\n",
      "| 에폭 4 |  반복 281 / 1327 | 시간 1506[s] | 퍼플렉서티 121.29\n",
      "| 에폭 4 |  반복 301 / 1327 | 시간 1512[s] | 퍼플렉서티 104.06\n",
      "| 에폭 4 |  반복 321 / 1327 | 시간 1518[s] | 퍼플렉서티 84.65\n",
      "| 에폭 4 |  반복 341 / 1327 | 시간 1524[s] | 퍼플렉서티 101.39\n",
      "| 에폭 4 |  반복 361 / 1327 | 시간 1530[s] | 퍼플렉서티 126.96\n",
      "| 에폭 4 |  반복 381 / 1327 | 시간 1536[s] | 퍼플렉서티 97.63\n",
      "| 에폭 4 |  반복 401 / 1327 | 시간 1541[s] | 퍼플렉서티 110.72\n",
      "| 에폭 4 |  반복 421 / 1327 | 시간 1547[s] | 퍼플렉서티 94.68\n",
      "| 에폭 4 |  반복 441 / 1327 | 시간 1554[s] | 퍼플렉서티 102.74\n",
      "| 에폭 4 |  반복 461 / 1327 | 시간 1560[s] | 퍼플렉서티 100.19\n",
      "| 에폭 4 |  반복 481 / 1327 | 시간 1566[s] | 퍼플렉서티 102.02\n",
      "| 에폭 4 |  반복 501 / 1327 | 시간 1571[s] | 퍼플렉서티 108.87\n",
      "| 에폭 4 |  반복 521 / 1327 | 시간 1577[s] | 퍼플렉서티 116.01\n",
      "| 에폭 4 |  반복 541 / 1327 | 시간 1583[s] | 퍼플렉서티 111.65\n",
      "| 에폭 4 |  반복 561 / 1327 | 시간 1589[s] | 퍼플렉서티 102.41\n",
      "| 에폭 4 |  반복 581 / 1327 | 시간 1595[s] | 퍼플렉서티 89.00\n",
      "| 에폭 4 |  반복 601 / 1327 | 시간 1601[s] | 퍼플렉서티 126.01\n",
      "| 에폭 4 |  반복 621 / 1327 | 시간 1607[s] | 퍼플렉서티 120.08\n",
      "| 에폭 4 |  반복 641 / 1327 | 시간 1613[s] | 퍼플렉서티 110.74\n",
      "| 에폭 4 |  반복 661 / 1327 | 시간 1619[s] | 퍼플렉서티 102.64\n",
      "| 에폭 4 |  반복 681 / 1327 | 시간 1624[s] | 퍼플렉서티 83.96\n",
      "| 에폭 4 |  반복 701 / 1327 | 시간 1630[s] | 퍼플렉서티 100.89\n",
      "| 에폭 4 |  반복 721 / 1327 | 시간 1636[s] | 퍼플렉서티 106.49\n",
      "| 에폭 4 |  반복 741 / 1327 | 시간 1642[s] | 퍼플렉서티 96.57\n",
      "| 에폭 4 |  반복 761 / 1327 | 시간 1648[s] | 퍼플렉서티 88.27\n",
      "| 에폭 4 |  반복 781 / 1327 | 시간 1654[s] | 퍼플렉서티 87.33\n",
      "| 에폭 4 |  반복 801 / 1327 | 시간 1659[s] | 퍼플렉서티 98.64\n",
      "| 에폭 4 |  반복 821 / 1327 | 시간 1665[s] | 퍼플렉서티 103.47\n",
      "| 에폭 4 |  반복 841 / 1327 | 시간 1671[s] | 퍼플렉서티 98.31\n",
      "| 에폭 4 |  반복 861 / 1327 | 시간 1677[s] | 퍼플렉서티 104.15\n",
      "| 에폭 4 |  반복 881 / 1327 | 시간 1683[s] | 퍼플렉서티 90.74\n",
      "| 에폭 4 |  반복 901 / 1327 | 시간 1689[s] | 퍼플렉서티 114.99\n",
      "| 에폭 4 |  반복 921 / 1327 | 시간 1695[s] | 퍼플렉서티 103.24\n",
      "| 에폭 4 |  반복 941 / 1327 | 시간 1701[s] | 퍼플렉서티 111.70\n",
      "| 에폭 4 |  반복 961 / 1327 | 시간 1707[s] | 퍼플렉서티 110.80\n",
      "| 에폭 4 |  반복 981 / 1327 | 시간 1713[s] | 퍼플렉서티 106.48\n",
      "| 에폭 4 |  반복 1001 / 1327 | 시간 1718[s] | 퍼플렉서티 97.44\n",
      "| 에폭 4 |  반복 1021 / 1327 | 시간 1724[s] | 퍼플렉서티 113.34\n",
      "| 에폭 4 |  반복 1041 / 1327 | 시간 1730[s] | 퍼플렉서티 102.88\n",
      "| 에폭 4 |  반복 1061 / 1327 | 시간 1736[s] | 퍼플렉서티 88.17\n",
      "| 에폭 4 |  반복 1081 / 1327 | 시간 1742[s] | 퍼플렉서티 79.08\n",
      "| 에폭 4 |  반복 1101 / 1327 | 시간 1748[s] | 퍼플렉서티 79.09\n",
      "| 에폭 4 |  반복 1121 / 1327 | 시간 1754[s] | 퍼플렉서티 104.01\n",
      "| 에폭 4 |  반복 1141 / 1327 | 시간 1760[s] | 퍼플렉서티 99.70\n",
      "| 에폭 4 |  반복 1161 / 1327 | 시간 1766[s] | 퍼플렉서티 91.51\n",
      "| 에폭 4 |  반복 1181 / 1327 | 시간 1771[s] | 퍼플렉서티 94.46\n",
      "| 에폭 4 |  반복 1201 / 1327 | 시간 1777[s] | 퍼플렉서티 83.78\n",
      "| 에폭 4 |  반복 1221 / 1327 | 시간 1783[s] | 퍼플렉서티 76.41\n",
      "| 에폭 4 |  반복 1241 / 1327 | 시간 1789[s] | 퍼플렉서티 91.73\n",
      "| 에폭 4 |  반복 1261 / 1327 | 시간 1795[s] | 퍼플렉서티 93.91\n",
      "| 에폭 4 |  반복 1281 / 1327 | 시간 1801[s] | 퍼플렉서티 91.05\n",
      "| 에폭 4 |  반복 1301 / 1327 | 시간 1807[s] | 퍼플렉서티 110.92\n",
      "| 에폭 4 |  반복 1321 / 1327 | 시간 1812[s] | 퍼플렉서티 108.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 48152 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 48373 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 54140 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 54540 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 47113 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 49436 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 54000 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 48152 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 48373 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 54140 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 54540 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 47113 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 49436 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 54000 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "퍼플렉서티 평가 중 ...\n",
      "234 / 235\n",
      "테스트 퍼플렉서티:  134.59310124416652\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "#from common.optimizer import SGD\n",
    "#from common.trainer import RnnlmTrainer\n",
    "#from common.util import eval_perplexity\n",
    "from dataset import ptb\n",
    "#from rnnlm import Rnnlm\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 20\n",
    "wordvec_size = 100\n",
    "hidden_size = 100  # RNN의 은닉 상태 벡터의 원소 수\n",
    "time_size = 35     # RNN을 펼치는 크기\n",
    "lr = 20.0\n",
    "max_epoch = 4\n",
    "max_grad = 0.25\n",
    "\n",
    "# 학습 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_test, _, _ = ptb.load_data('test')\n",
    "vocab_size = len(word_to_id)\n",
    "xs = corpus[:-1]\n",
    "ts = corpus[1:]\n",
    "\n",
    "# 모델 생성\n",
    "model = Rnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)\n",
    "trainer = RnnlmTrainer(model, optimizer)\n",
    "\n",
    "# 기울기 클리핑을 적용하여 학습\n",
    "trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad,\n",
    "            eval_interval=20)\n",
    "trainer.plot(ylim=(0, 500))\n",
    "\n",
    "# 테스트 데이터로 평가\n",
    "model.reset_state()\n",
    "ppl_test = eval_perplexity(model, corpus_test)\n",
    "print('테스트 퍼플렉서티: ', ppl_test)\n",
    "\n",
    "# 매개변수 저장\n",
    "model.save_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "| 에폭 1 |  반복 1 / 1327 | 시간 1[s] | 퍼플렉서티 9996.42\n",
    "| 에폭 1 |  반복 21 / 1327 | 시간 17[s] | 퍼플렉서티 2871.38\n",
    "| 에폭 1 |  반복 41 / 1327 | 시간 32[s] | 퍼플렉서티 1209.36\n",
    "| 에폭 1 |  반복 61 / 1327 | 시간 42[s] | 퍼플렉서티 956.09\n",
    "| 에폭 1 |  반복 81 / 1327 | 시간 52[s] | 퍼플렉서티 802.61\n",
    "| 에폭 1 |  반복 101 / 1327 | 시간 62[s] | 퍼플렉서티 645.99\n",
    "| 에폭 1 |  반복 121 / 1327 | 시간 74[s] | 퍼플렉서티 646.51\n",
    "| 에폭 1 |  반복 141 / 1327 | 시간 85[s] | 퍼플렉서티 610.41\n",
    "| 에폭 1 |  반복 161 / 1327 | 시간 97[s] | 퍼플렉서티 566.09\n",
    "| 에폭 1 |  반복 181 / 1327 | 시간 108[s] | 퍼플렉서티 597.39\n",
    "| 에폭 1 |  반복 201 / 1327 | 시간 118[s] | 퍼플렉서티 511.05\n",
    "| 에폭 1 |  반복 221 / 1327 | 시간 128[s] | 퍼플렉서티 488.65\n",
    "| 에폭 1 |  반복 241 / 1327 | 시간 139[s] | 퍼플렉서티 452.54\n",
    "| 에폭 1 |  반복 261 / 1327 | 시간 149[s] | 퍼플렉서티 466.79\n",
    "| 에폭 1 |  반복 281 / 1327 | 시간 159[s] | 퍼플렉서티 455.55\n",
    "| 에폭 1 |  반복 301 / 1327 | 시간 169[s] | 퍼플렉서티 391.53\n",
    "| 에폭 1 |  반복 321 / 1327 | 시간 180[s] | 퍼플렉서티 350.47\n",
    "| 에폭 1 |  반복 341 / 1327 | 시간 189[s] | 퍼플렉서티 400.35\n",
    "| 에폭 1 |  반복 361 / 1327 | 시간 200[s] | 퍼플렉서티 404.54\n",
    "| 에폭 1 |  반복 381 / 1327 | 시간 212[s] | 퍼플렉서티 340.74\n",
    "| 에폭 1 |  반복 401 / 1327 | 시간 223[s] | 퍼플렉서티 351.58\n",
    "| 에폭 1 |  반복 421 / 1327 | 시간 235[s] | 퍼플렉서티 343.35\n",
    "| 에폭 1 |  반복 441 / 1327 | 시간 247[s] | 퍼플렉서티 333.37\n",
    "| 에폭 1 |  반복 461 / 1327 | 시간 260[s] | 퍼플렉서티 322.75\n",
    "| 에폭 1 |  반복 481 / 1327 | 시간 269[s] | 퍼플렉서티 300.20\n",
    "| 에폭 1 |  반복 501 / 1327 | 시간 280[s] | 퍼플렉서티 307.12\n",
    "| 에폭 1 |  반복 521 / 1327 | 시간 291[s] | 퍼플렉서티 301.28\n",
    "| 에폭 1 |  반복 541 / 1327 | 시간 300[s] | 퍼플렉서티 314.97\n",
    "| 에폭 1 |  반복 561 / 1327 | 시간 309[s] | 퍼플렉서티 284.79\n",
    "| 에폭 1 |  반복 581 / 1327 | 시간 317[s] | 퍼플렉서티 255.90\n",
    "| 에폭 1 |  반복 601 / 1327 | 시간 326[s] | 퍼플렉서티 333.54\n",
    "| 에폭 1 |  반복 621 / 1327 | 시간 335[s] | 퍼플렉서티 305.43\n",
    "| 에폭 1 |  반복 641 / 1327 | 시간 343[s] | 퍼플렉서티 281.06\n",
    "| 에폭 1 |  반복 661 / 1327 | 시간 352[s] | 퍼플렉서티 265.18\n",
    "| 에폭 1 |  반복 681 / 1327 | 시간 360[s] | 퍼플렉서티 225.14\n",
    "| 에폭 1 |  반복 701 / 1327 | 시간 369[s] | 퍼플렉서티 248.17\n",
    "| 에폭 1 |  반복 721 / 1327 | 시간 378[s] | 퍼플렉서티 257.91\n",
    "| 에폭 1 |  반복 741 / 1327 | 시간 386[s] | 퍼플렉서티 216.82\n",
    "| 에폭 1 |  반복 761 / 1327 | 시간 395[s] | 퍼플렉서티 231.75\n",
    "| 에폭 1 |  반복 781 / 1327 | 시간 403[s] | 퍼플렉서티 217.34\n",
    "| 에폭 1 |  반복 801 / 1327 | 시간 412[s] | 퍼플렉서티 240.33\n",
    "| 에폭 1 |  반복 821 / 1327 | 시간 421[s] | 퍼플렉서티 222.60\n",
    "| 에폭 1 |  반복 841 / 1327 | 시간 429[s] | 퍼플렉서티 226.80\n",
    "| 에폭 1 |  반복 861 / 1327 | 시간 438[s] | 퍼플렉서티 218.47\n",
    "| 에폭 1 |  반복 881 / 1327 | 시간 446[s] | 퍼플렉서티 203.57\n",
    "| 에폭 1 |  반복 901 / 1327 | 시간 455[s] | 퍼플렉서티 250.47\n",
    "| 에폭 1 |  반복 921 / 1327 | 시간 463[s] | 퍼플렉서티 224.69\n",
    "| 에폭 1 |  반복 941 / 1327 | 시간 472[s] | 퍼플렉서티 227.66\n",
    "| 에폭 1 |  반복 961 / 1327 | 시간 481[s] | 퍼플렉서티 243.35\n",
    "| 에폭 1 |  반복 981 / 1327 | 시간 489[s] | 퍼플렉서티 227.33\n",
    "| 에폭 1 |  반복 1001 / 1327 | 시간 498[s] | 퍼플렉서티 191.82\n",
    "| 에폭 1 |  반복 1021 / 1327 | 시간 507[s] | 퍼플렉서티 222.93\n",
    "| 에폭 1 |  반복 1041 / 1327 | 시간 515[s] | 퍼플렉서티 203.26\n",
    "| 에폭 1 |  반복 1061 / 1327 | 시간 524[s] | 퍼플렉서티 193.00\n",
    "| 에폭 1 |  반복 1081 / 1327 | 시간 533[s] | 퍼플렉서티 164.56\n",
    "| 에폭 1 |  반복 1101 / 1327 | 시간 541[s] | 퍼플렉서티 187.68\n",
    "| 에폭 1 |  반복 1121 / 1327 | 시간 550[s] | 퍼플렉서티 224.68\n",
    "| 에폭 1 |  반복 1141 / 1327 | 시간 558[s] | 퍼플렉서티 206.42\n",
    "| 에폭 1 |  반복 1161 / 1327 | 시간 567[s] | 퍼플렉서티 196.77\n",
    "| 에폭 1 |  반복 1181 / 1327 | 시간 575[s] | 퍼플렉서티 186.31\n",
    "| 에폭 1 |  반복 1201 / 1327 | 시간 584[s] | 퍼플렉서티 159.00\n",
    "| 에폭 1 |  반복 1221 / 1327 | 시간 592[s] | 퍼플렉서티 157.22\n",
    "| 에폭 1 |  반복 1241 / 1327 | 시간 601[s] | 퍼플렉서티 184.63\n",
    "| 에폭 1 |  반복 1261 / 1327 | 시간 609[s] | 퍼플렉서티 168.66\n",
    "| 에폭 1 |  반복 1281 / 1327 | 시간 615[s] | 퍼플렉서티 176.74\n",
    "| 에폭 1 |  반복 1301 / 1327 | 시간 626[s] | 퍼플렉서티 219.93\n",
    "| 에폭 1 |  반복 1321 / 1327 | 시간 634[s] | 퍼플렉서티 208.66\n",
    "| 에폭 2 |  반복 1 / 1327 | 시간 638[s] | 퍼플렉서티 220.30\n",
    "| 에폭 2 |  반복 21 / 1327 | 시간 644[s] | 퍼플렉서티 201.39\n",
    "| 에폭 2 |  반복 41 / 1327 | 시간 650[s] | 퍼플렉서티 186.53\n",
    "| 에폭 2 |  반복 61 / 1327 | 시간 656[s] | 퍼플렉서티 173.17\n",
    "| 에폭 2 |  반복 81 / 1327 | 시간 662[s] | 퍼플렉서티 156.09\n",
    "| 에폭 2 |  반복 101 / 1327 | 시간 668[s] | 퍼플렉서티 150.83\n",
    "| 에폭 2 |  반복 121 / 1327 | 시간 674[s] | 퍼플렉서티 157.54\n",
    "| 에폭 2 |  반복 141 / 1327 | 시간 680[s] | 퍼플렉서티 174.85\n",
    "| 에폭 2 |  반복 161 / 1327 | 시간 686[s] | 퍼플렉서티 189.06\n",
    "| 에폭 2 |  반복 181 / 1327 | 시간 692[s] | 퍼플렉서티 198.87\n",
    "| 에폭 2 |  반복 201 / 1327 | 시간 698[s] | 퍼플렉서티 183.73\n",
    "| 에폭 2 |  반복 221 / 1327 | 시간 703[s] | 퍼플렉서티 181.54\n",
    "| 에폭 2 |  반복 241 / 1327 | 시간 709[s] | 퍼플렉서티 174.37\n",
    "| 에폭 2 |  반복 261 / 1327 | 시간 715[s] | 퍼플렉서티 183.84\n",
    "| 에폭 2 |  반복 281 / 1327 | 시간 721[s] | 퍼플렉서티 180.84\n",
    "| 에폭 2 |  반복 301 / 1327 | 시간 727[s] | 퍼플렉서티 165.28\n",
    "| 에폭 2 |  반복 321 / 1327 | 시간 733[s] | 퍼플렉서티 135.38\n",
    "| 에폭 2 |  반복 341 / 1327 | 시간 739[s] | 퍼플렉서티 170.21\n",
    "| 에폭 2 |  반복 361 / 1327 | 시간 745[s] | 퍼플렉서티 193.39\n",
    "| 에폭 2 |  반복 381 / 1327 | 시간 750[s] | 퍼플렉서티 150.73\n",
    "| 에폭 2 |  반복 401 / 1327 | 시간 756[s] | 퍼플렉서티 165.71\n",
    "| 에폭 2 |  반복 421 / 1327 | 시간 762[s] | 퍼플렉서티 152.27\n",
    "| 에폭 2 |  반복 441 / 1327 | 시간 768[s] | 퍼플렉서티 161.07\n",
    "| 에폭 2 |  반복 461 / 1327 | 시간 774[s] | 퍼플렉서티 154.59\n",
    "| 에폭 2 |  반복 481 / 1327 | 시간 780[s] | 퍼플렉서티 153.99\n",
    "| 에폭 2 |  반복 501 / 1327 | 시간 786[s] | 퍼플렉서티 167.63\n",
    "| 에폭 2 |  반복 521 / 1327 | 시간 792[s] | 퍼플렉서티 170.99\n",
    "| 에폭 2 |  반복 541 / 1327 | 시간 798[s] | 퍼플렉서티 170.68\n",
    "| 에폭 2 |  반복 561 / 1327 | 시간 804[s] | 퍼플렉서티 155.43\n",
    "| 에폭 2 |  반복 581 / 1327 | 시간 810[s] | 퍼플렉서티 137.56\n",
    "| 에폭 2 |  반복 601 / 1327 | 시간 816[s] | 퍼플렉서티 188.16\n",
    "| 에폭 2 |  반복 621 / 1327 | 시간 822[s] | 퍼플렉서티 177.78\n",
    "| 에폭 2 |  반복 641 / 1327 | 시간 827[s] | 퍼플렉서티 161.75\n",
    "| 에폭 2 |  반복 661 / 1327 | 시간 833[s] | 퍼플렉서티 152.28\n",
    "| 에폭 2 |  반복 681 / 1327 | 시간 839[s] | 퍼플렉서티 128.13\n",
    "| 에폭 2 |  반복 701 / 1327 | 시간 845[s] | 퍼플렉서티 148.91\n",
    "| 에폭 2 |  반복 721 / 1327 | 시간 851[s] | 퍼플렉서티 156.50\n",
    "| 에폭 2 |  반복 741 / 1327 | 시간 857[s] | 퍼플렉서티 130.82\n",
    "| 에폭 2 |  반복 761 / 1327 | 시간 863[s] | 퍼플렉서티 126.90\n",
    "| 에폭 2 |  반복 781 / 1327 | 시간 869[s] | 퍼플렉서티 132.03\n",
    "| 에폭 2 |  반복 801 / 1327 | 시간 875[s] | 퍼플렉서티 146.82\n",
    "| 에폭 2 |  반복 821 / 1327 | 시간 881[s] | 퍼플렉서티 142.69\n",
    "| 에폭 2 |  반복 841 / 1327 | 시간 887[s] | 퍼플렉서티 142.94\n",
    "| 에폭 2 |  반복 861 / 1327 | 시간 892[s] | 퍼플렉서티 143.97\n",
    "| 에폭 2 |  반복 881 / 1327 | 시간 898[s] | 퍼플렉서티 128.11\n",
    "| 에폭 2 |  반복 901 / 1327 | 시간 904[s] | 퍼플렉서티 165.41\n",
    "| 에폭 2 |  반복 921 / 1327 | 시간 910[s] | 퍼플렉서티 145.82\n",
    "| 에폭 2 |  반복 941 / 1327 | 시간 916[s] | 퍼플렉서티 150.93\n",
    "| 에폭 2 |  반복 961 / 1327 | 시간 922[s] | 퍼플렉서티 162.22\n",
    "| 에폭 2 |  반복 981 / 1327 | 시간 928[s] | 퍼플렉서티 152.87\n",
    "| 에폭 2 |  반복 1001 / 1327 | 시간 934[s] | 퍼플렉서티 130.45\n",
    "| 에폭 2 |  반복 1021 / 1327 | 시간 940[s] | 퍼플렉서티 155.32\n",
    "| 에폭 2 |  반복 1041 / 1327 | 시간 946[s] | 퍼플렉서티 139.98\n",
    "| 에폭 2 |  반복 1061 / 1327 | 시간 953[s] | 퍼플렉서티 125.97\n",
    "| 에폭 2 |  반복 1081 / 1327 | 시간 958[s] | 퍼플렉서티 109.93\n",
    "| 에폭 2 |  반복 1101 / 1327 | 시간 964[s] | 퍼플렉서티 117.97\n",
    "| 에폭 2 |  반복 1121 / 1327 | 시간 970[s] | 퍼플렉서티 151.04\n",
    "| 에폭 2 |  반복 1141 / 1327 | 시간 976[s] | 퍼플렉서티 140.26\n",
    "| 에폭 2 |  반복 1161 / 1327 | 시간 982[s] | 퍼플렉서티 132.36\n",
    "| 에폭 2 |  반복 1181 / 1327 | 시간 988[s] | 퍼플렉서티 131.08\n",
    "| 에폭 2 |  반복 1201 / 1327 | 시간 994[s] | 퍼플렉서티 110.88\n",
    "| 에폭 2 |  반복 1221 / 1327 | 시간 999[s] | 퍼플렉서티 108.43\n",
    "| 에폭 2 |  반복 1241 / 1327 | 시간 1005[s] | 퍼플렉서티 128.71\n",
    "| 에폭 2 |  반복 1261 / 1327 | 시간 1011[s] | 퍼플렉서티 122.00\n",
    "| 에폭 2 |  반복 1281 / 1327 | 시간 1017[s] | 퍼플렉서티 123.32\n",
    "| 에폭 2 |  반복 1301 / 1327 | 시간 1023[s] | 퍼플렉서티 154.89\n",
    "| 에폭 2 |  반복 1321 / 1327 | 시간 1029[s] | 퍼플렉서티 151.35\n",
    "| 에폭 3 |  반복 1 / 1327 | 시간 1031[s] | 퍼플렉서티 159.66\n",
    "| 에폭 3 |  반복 21 / 1327 | 시간 1037[s] | 퍼플렉서티 142.07\n",
    "| 에폭 3 |  반복 41 / 1327 | 시간 1042[s] | 퍼플렉서티 133.34\n",
    "| 에폭 3 |  반복 61 / 1327 | 시간 1048[s] | 퍼플렉서티 124.70\n",
    "| 에폭 3 |  반복 81 / 1327 | 시간 1054[s] | 퍼플렉서티 115.39\n",
    "| 에폭 3 |  반복 101 / 1327 | 시간 1060[s] | 퍼플렉서티 103.98\n",
    "| 에폭 3 |  반복 121 / 1327 | 시간 1066[s] | 퍼플렉서티 115.81\n",
    "| 에폭 3 |  반복 141 / 1327 | 시간 1072[s] | 퍼플렉서티 123.90\n",
    "| 에폭 3 |  반복 161 / 1327 | 시간 1078[s] | 퍼플렉서티 140.46\n",
    "| 에폭 3 |  반복 181 / 1327 | 시간 1083[s] | 퍼플렉서티 150.90\n",
    "| 에폭 3 |  반복 201 / 1327 | 시간 1089[s] | 퍼플렉서티 139.69\n",
    "| 에폭 3 |  반복 221 / 1327 | 시간 1095[s] | 퍼플렉서티 138.81\n",
    "| 에폭 3 |  반복 241 / 1327 | 시간 1101[s] | 퍼플렉서티 133.44\n",
    "| 에폭 3 |  반복 261 / 1327 | 시간 1107[s] | 퍼플렉서티 137.37\n",
    "| 에폭 3 |  반복 281 / 1327 | 시간 1113[s] | 퍼플렉서티 140.67\n",
    "| 에폭 3 |  반복 301 / 1327 | 시간 1119[s] | 퍼플렉서티 123.25\n",
    "| 에폭 3 |  반복 321 / 1327 | 시간 1125[s] | 퍼플렉서티 100.69\n",
    "| 에폭 3 |  반복 341 / 1327 | 시간 1131[s] | 퍼플렉서티 123.83\n",
    "| 에폭 3 |  반복 361 / 1327 | 시간 1137[s] | 퍼플렉서티 149.65\n",
    "| 에폭 3 |  반복 381 / 1327 | 시간 1143[s] | 퍼플렉서티 113.97\n",
    "| 에폭 3 |  반복 401 / 1327 | 시간 1149[s] | 퍼플렉서티 129.24\n",
    "| 에폭 3 |  반복 421 / 1327 | 시간 1155[s] | 퍼플렉서티 112.57\n",
    "| 에폭 3 |  반복 441 / 1327 | 시간 1161[s] | 퍼플렉서티 122.72\n",
    "| 에폭 3 |  반복 461 / 1327 | 시간 1167[s] | 퍼플렉서티 117.02\n",
    "| 에폭 3 |  반복 481 / 1327 | 시간 1173[s] | 퍼플렉서티 118.34\n",
    "| 에폭 3 |  반복 501 / 1327 | 시간 1178[s] | 퍼플렉서티 127.90\n",
    "| 에폭 3 |  반복 521 / 1327 | 시간 1184[s] | 퍼플렉서티 136.31\n",
    "| 에폭 3 |  반복 541 / 1327 | 시간 1190[s] | 퍼플렉서티 134.81\n",
    "| 에폭 3 |  반복 561 / 1327 | 시간 1196[s] | 퍼플렉서티 117.81\n",
    "| 에폭 3 |  반복 581 / 1327 | 시간 1202[s] | 퍼플렉서티 105.03\n",
    "| 에폭 3 |  반복 601 / 1327 | 시간 1208[s] | 퍼플렉서티 147.72\n",
    "| 에폭 3 |  반복 621 / 1327 | 시간 1214[s] | 퍼플렉서티 140.19\n",
    "| 에폭 3 |  반복 641 / 1327 | 시간 1220[s] | 퍼플렉서티 127.22\n",
    "| 에폭 3 |  반복 661 / 1327 | 시간 1226[s] | 퍼플렉서티 118.98\n",
    "| 에폭 3 |  반복 681 / 1327 | 시간 1232[s] | 퍼플렉서티 98.33\n",
    "| 에폭 3 |  반복 701 / 1327 | 시간 1238[s] | 퍼플렉서티 116.83\n",
    "| 에폭 3 |  반복 721 / 1327 | 시간 1244[s] | 퍼플렉서티 125.21\n",
    "| 에폭 3 |  반복 741 / 1327 | 시간 1250[s] | 퍼플렉서티 106.88\n",
    "| 에폭 3 |  반복 761 / 1327 | 시간 1256[s] | 퍼플렉서티 101.10\n",
    "| 에폭 3 |  반복 781 / 1327 | 시간 1262[s] | 퍼플렉서티 103.05\n",
    "| 에폭 3 |  반복 801 / 1327 | 시간 1268[s] | 퍼플렉서티 116.22\n",
    "| 에폭 3 |  반복 821 / 1327 | 시간 1274[s] | 퍼플렉서티 116.60\n",
    "| 에폭 3 |  반복 841 / 1327 | 시간 1280[s] | 퍼플렉서티 112.89\n",
    "| 에폭 3 |  반복 861 / 1327 | 시간 1285[s] | 퍼플렉서티 118.68\n",
    "| 에폭 3 |  반복 881 / 1327 | 시간 1291[s] | 퍼플렉서티 104.57\n",
    "| 에폭 3 |  반복 901 / 1327 | 시간 1297[s] | 퍼플렉서티 131.28\n",
    "| 에폭 3 |  반복 921 / 1327 | 시간 1303[s] | 퍼플렉서티 117.74\n",
    "| 에폭 3 |  반복 941 / 1327 | 시간 1309[s] | 퍼플렉서티 125.82\n",
    "| 에폭 3 |  반복 961 / 1327 | 시간 1315[s] | 퍼플렉서티 130.09\n",
    "| 에폭 3 |  반복 981 / 1327 | 시간 1321[s] | 퍼플렉서티 122.31\n",
    "| 에폭 3 |  반복 1001 / 1327 | 시간 1327[s] | 퍼플렉서티 109.05\n",
    "| 에폭 3 |  반복 1021 / 1327 | 시간 1332[s] | 퍼플렉서티 128.19\n",
    "| 에폭 3 |  반복 1041 / 1327 | 시간 1338[s] | 퍼플렉서티 117.41\n",
    "| 에폭 3 |  반복 1061 / 1327 | 시간 1344[s] | 퍼플렉서티 101.86\n",
    "| 에폭 3 |  반복 1081 / 1327 | 시간 1350[s] | 퍼플렉서티 89.03\n",
    "| 에폭 3 |  반복 1101 / 1327 | 시간 1356[s] | 퍼플렉서티 93.64\n",
    "| 에폭 3 |  반복 1121 / 1327 | 시간 1362[s] | 퍼플렉서티 120.87\n",
    "| 에폭 3 |  반복 1141 / 1327 | 시간 1368[s] | 퍼플렉서티 113.84\n",
    "| 에폭 3 |  반복 1161 / 1327 | 시간 1373[s] | 퍼플렉서티 105.25\n",
    "| 에폭 3 |  반복 1181 / 1327 | 시간 1379[s] | 퍼플렉서티 109.26\n",
    "| 에폭 3 |  반복 1201 / 1327 | 시간 1385[s] | 퍼플렉서티 93.38\n",
    "| 에폭 3 |  반복 1221 / 1327 | 시간 1391[s] | 퍼플렉서티 89.02\n",
    "| 에폭 3 |  반복 1241 / 1327 | 시간 1398[s] | 퍼플렉서티 105.01\n",
    "| 에폭 3 |  반복 1261 / 1327 | 시간 1403[s] | 퍼플렉서티 104.58\n",
    "| 에폭 3 |  반복 1281 / 1327 | 시간 1409[s] | 퍼플렉서티 102.17\n",
    "| 에폭 3 |  반복 1301 / 1327 | 시간 1415[s] | 퍼플렉서티 127.12\n",
    "| 에폭 3 |  반복 1321 / 1327 | 시간 1421[s] | 퍼플렉서티 125.17\n",
    "| 에폭 4 |  반복 1 / 1327 | 시간 1423[s] | 퍼플렉서티 131.56\n",
    "| 에폭 4 |  반복 21 / 1327 | 시간 1429[s] | 퍼플렉서티 120.48\n",
    "| 에폭 4 |  반복 41 / 1327 | 시간 1435[s] | 퍼플렉서티 107.15\n",
    "| 에폭 4 |  반복 61 / 1327 | 시간 1441[s] | 퍼플렉서티 105.60\n",
    "| 에폭 4 |  반복 81 / 1327 | 시간 1447[s] | 퍼플렉서티 96.69\n",
    "| 에폭 4 |  반복 101 / 1327 | 시간 1453[s] | 퍼플렉서티 84.88\n",
    "| 에폭 4 |  반복 121 / 1327 | 시간 1459[s] | 퍼플렉서티 95.89\n",
    "| 에폭 4 |  반복 141 / 1327 | 시간 1465[s] | 퍼플렉서티 102.46\n",
    "| 에폭 4 |  반복 161 / 1327 | 시간 1470[s] | 퍼플렉서티 117.31\n",
    "| 에폭 4 |  반복 181 / 1327 | 시간 1476[s] | 퍼플렉서티 129.53\n",
    "| 에폭 4 |  반복 201 / 1327 | 시간 1482[s] | 퍼플렉서티 118.79\n",
    "| 에폭 4 |  반복 221 / 1327 | 시간 1488[s] | 퍼플렉서티 120.80\n",
    "| 에폭 4 |  반복 241 / 1327 | 시간 1494[s] | 퍼플렉서티 114.48\n",
    "| 에폭 4 |  반복 261 / 1327 | 시간 1500[s] | 퍼플렉서티 113.56\n",
    "| 에폭 4 |  반복 281 / 1327 | 시간 1506[s] | 퍼플렉서티 121.29\n",
    "| 에폭 4 |  반복 301 / 1327 | 시간 1512[s] | 퍼플렉서티 104.06\n",
    "| 에폭 4 |  반복 321 / 1327 | 시간 1518[s] | 퍼플렉서티 84.65\n",
    "| 에폭 4 |  반복 341 / 1327 | 시간 1524[s] | 퍼플렉서티 101.39\n",
    "| 에폭 4 |  반복 361 / 1327 | 시간 1530[s] | 퍼플렉서티 126.96\n",
    "| 에폭 4 |  반복 381 / 1327 | 시간 1536[s] | 퍼플렉서티 97.63\n",
    "| 에폭 4 |  반복 401 / 1327 | 시간 1541[s] | 퍼플렉서티 110.72\n",
    "| 에폭 4 |  반복 421 / 1327 | 시간 1547[s] | 퍼플렉서티 94.68\n",
    "| 에폭 4 |  반복 441 / 1327 | 시간 1554[s] | 퍼플렉서티 102.74\n",
    "| 에폭 4 |  반복 461 / 1327 | 시간 1560[s] | 퍼플렉서티 100.19\n",
    "| 에폭 4 |  반복 481 / 1327 | 시간 1566[s] | 퍼플렉서티 102.02\n",
    "| 에폭 4 |  반복 501 / 1327 | 시간 1571[s] | 퍼플렉서티 108.87\n",
    "| 에폭 4 |  반복 521 / 1327 | 시간 1577[s] | 퍼플렉서티 116.01\n",
    "| 에폭 4 |  반복 541 / 1327 | 시간 1583[s] | 퍼플렉서티 111.65\n",
    "| 에폭 4 |  반복 561 / 1327 | 시간 1589[s] | 퍼플렉서티 102.41\n",
    "| 에폭 4 |  반복 581 / 1327 | 시간 1595[s] | 퍼플렉서티 89.00\n",
    "| 에폭 4 |  반복 601 / 1327 | 시간 1601[s] | 퍼플렉서티 126.01\n",
    "| 에폭 4 |  반복 621 / 1327 | 시간 1607[s] | 퍼플렉서티 120.08\n",
    "| 에폭 4 |  반복 641 / 1327 | 시간 1613[s] | 퍼플렉서티 110.74\n",
    "| 에폭 4 |  반복 661 / 1327 | 시간 1619[s] | 퍼플렉서티 102.64\n",
    "| 에폭 4 |  반복 681 / 1327 | 시간 1624[s] | 퍼플렉서티 83.96\n",
    "| 에폭 4 |  반복 701 / 1327 | 시간 1630[s] | 퍼플렉서티 100.89\n",
    "| 에폭 4 |  반복 721 / 1327 | 시간 1636[s] | 퍼플렉서티 106.49\n",
    "| 에폭 4 |  반복 741 / 1327 | 시간 1642[s] | 퍼플렉서티 96.57\n",
    "| 에폭 4 |  반복 761 / 1327 | 시간 1648[s] | 퍼플렉서티 88.27\n",
    "| 에폭 4 |  반복 781 / 1327 | 시간 1654[s] | 퍼플렉서티 87.33\n",
    "| 에폭 4 |  반복 801 / 1327 | 시간 1659[s] | 퍼플렉서티 98.64\n",
    "| 에폭 4 |  반복 821 / 1327 | 시간 1665[s] | 퍼플렉서티 103.47\n",
    "| 에폭 4 |  반복 841 / 1327 | 시간 1671[s] | 퍼플렉서티 98.31\n",
    "| 에폭 4 |  반복 861 / 1327 | 시간 1677[s] | 퍼플렉서티 104.15\n",
    "| 에폭 4 |  반복 881 / 1327 | 시간 1683[s] | 퍼플렉서티 90.74\n",
    "| 에폭 4 |  반복 901 / 1327 | 시간 1689[s] | 퍼플렉서티 114.99\n",
    "| 에폭 4 |  반복 921 / 1327 | 시간 1695[s] | 퍼플렉서티 103.24\n",
    "| 에폭 4 |  반복 941 / 1327 | 시간 1701[s] | 퍼플렉서티 111.70\n",
    "| 에폭 4 |  반복 961 / 1327 | 시간 1707[s] | 퍼플렉서티 110.80\n",
    "| 에폭 4 |  반복 981 / 1327 | 시간 1713[s] | 퍼플렉서티 106.48\n",
    "| 에폭 4 |  반복 1001 / 1327 | 시간 1718[s] | 퍼플렉서티 97.44\n",
    "| 에폭 4 |  반복 1021 / 1327 | 시간 1724[s] | 퍼플렉서티 113.34\n",
    "| 에폭 4 |  반복 1041 / 1327 | 시간 1730[s] | 퍼플렉서티 102.88\n",
    "| 에폭 4 |  반복 1061 / 1327 | 시간 1736[s] | 퍼플렉서티 88.17\n",
    "| 에폭 4 |  반복 1081 / 1327 | 시간 1742[s] | 퍼플렉서티 79.08\n",
    "| 에폭 4 |  반복 1101 / 1327 | 시간 1748[s] | 퍼플렉서티 79.09\n",
    "| 에폭 4 |  반복 1121 / 1327 | 시간 1754[s] | 퍼플렉서티 104.01\n",
    "| 에폭 4 |  반복 1141 / 1327 | 시간 1760[s] | 퍼플렉서티 99.70\n",
    "| 에폭 4 |  반복 1161 / 1327 | 시간 1766[s] | 퍼플렉서티 91.51\n",
    "| 에폭 4 |  반복 1181 / 1327 | 시간 1771[s] | 퍼플렉서티 94.46\n",
    "| 에폭 4 |  반복 1201 / 1327 | 시간 1777[s] | 퍼플렉서티 83.78\n",
    "| 에폭 4 |  반복 1221 / 1327 | 시간 1783[s] | 퍼플렉서티 76.41\n",
    "| 에폭 4 |  반복 1241 / 1327 | 시간 1789[s] | 퍼플렉서티 91.73\n",
    "| 에폭 4 |  반복 1261 / 1327 | 시간 1795[s] | 퍼플렉서티 93.91\n",
    "| 에폭 4 |  반복 1281 / 1327 | 시간 1801[s] | 퍼플렉서티 91.05\n",
    "| 에폭 4 |  반복 1301 / 1327 | 시간 1807[s] | 퍼플렉서티 110.92\n",
    "| 에폭 4 |  반복 1321 / 1327 | 시간 1812[s] | 퍼플렉서티 108.87\n",
    "\n",
    "퍼플렉서티 평가 중 ...\n",
    "234 / 235\n",
    "테스트 퍼플렉서티:  134.59310124416652\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_better_rnnlm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 1327 | 시간 4[s] | 퍼플렉서티 10000.03\n",
      "| 에폭 1 |  반복 21 / 1327 | 시간 66[s] | 퍼플렉서티 4261.86\n",
      "| 에폭 1 |  반복 41 / 1327 | 시간 145[s] | 퍼플렉서티 1822.62\n",
      "| 에폭 1 |  반복 61 / 1327 | 시간 214[s] | 퍼플렉서티 1209.78\n",
      "| 에폭 1 |  반복 81 / 1327 | 시간 298[s] | 퍼플렉서티 1019.93\n",
      "| 에폭 1 |  반복 101 / 1327 | 시간 369[s] | 퍼플렉서티 868.60\n",
      "| 에폭 1 |  반복 121 / 1327 | 시간 443[s] | 퍼플렉서티 812.09\n",
      "| 에폭 1 |  반복 141 / 1327 | 시간 515[s] | 퍼플렉서티 712.12\n",
      "| 에폭 1 |  반복 161 / 1327 | 시간 580[s] | 퍼플렉서티 693.27\n",
      "| 에폭 1 |  반복 181 / 1327 | 시간 642[s] | 퍼플렉서티 697.01\n",
      "| 에폭 1 |  반복 201 / 1327 | 시간 700[s] | 퍼플렉서티 586.71\n",
      "| 에폭 1 |  반복 221 / 1327 | 시간 752[s] | 퍼플렉서티 589.80\n",
      "| 에폭 1 |  반복 241 / 1327 | 시간 808[s] | 퍼플렉서티 523.37\n",
      "| 에폭 1 |  반복 261 / 1327 | 시간 861[s] | 퍼플렉서티 543.52\n",
      "| 에폭 1 |  반복 281 / 1327 | 시간 913[s] | 퍼플렉서티 528.79\n",
      "| 에폭 1 |  반복 301 / 1327 | 시간 965[s] | 퍼플렉서티 443.17\n",
      "| 에폭 1 |  반복 321 / 1327 | 시간 1017[s] | 퍼플렉서티 399.42\n",
      "| 에폭 1 |  반복 341 / 1327 | 시간 1069[s] | 퍼플렉서티 458.72\n",
      "| 에폭 1 |  반복 361 / 1327 | 시간 1121[s] | 퍼플렉서티 469.54\n",
      "| 에폭 1 |  반복 381 / 1327 | 시간 1172[s] | 퍼플렉서티 389.24\n",
      "| 에폭 1 |  반복 401 / 1327 | 시간 1224[s] | 퍼플렉서티 408.31\n",
      "| 에폭 1 |  반복 421 / 1327 | 시간 1276[s] | 퍼플렉서티 395.18\n",
      "| 에폭 1 |  반복 441 / 1327 | 시간 1327[s] | 퍼플렉서티 378.52\n",
      "| 에폭 1 |  반복 461 / 1327 | 시간 1380[s] | 퍼플렉서티 378.24\n",
      "| 에폭 1 |  반복 481 / 1327 | 시간 1431[s] | 퍼플렉서티 350.82\n",
      "| 에폭 1 |  반복 501 / 1327 | 시간 1482[s] | 퍼플렉서티 366.41\n",
      "| 에폭 1 |  반복 521 / 1327 | 시간 1533[s] | 퍼플렉서티 345.96\n",
      "| 에폭 1 |  반복 541 / 1327 | 시간 1584[s] | 퍼플렉서티 365.99\n",
      "| 에폭 1 |  반복 561 / 1327 | 시간 1636[s] | 퍼플렉서티 322.22\n",
      "| 에폭 1 |  반복 581 / 1327 | 시간 1688[s] | 퍼플렉서티 299.71\n",
      "| 에폭 1 |  반복 601 / 1327 | 시간 1740[s] | 퍼플렉서티 380.61\n",
      "| 에폭 1 |  반복 621 / 1327 | 시간 1791[s] | 퍼플렉서티 347.62\n",
      "| 에폭 1 |  반복 641 / 1327 | 시간 1843[s] | 퍼플렉서티 319.57\n",
      "| 에폭 1 |  반복 661 / 1327 | 시간 1894[s] | 퍼플렉서티 304.22\n",
      "| 에폭 1 |  반복 681 / 1327 | 시간 1945[s] | 퍼플렉서티 258.09\n",
      "| 에폭 1 |  반복 701 / 1327 | 시간 1997[s] | 퍼플렉서티 281.79\n",
      "| 에폭 1 |  반복 721 / 1327 | 시간 2049[s] | 퍼플렉서티 293.47\n",
      "| 에폭 1 |  반복 741 / 1327 | 시간 2100[s] | 퍼플렉서티 248.47\n",
      "| 에폭 1 |  반복 761 / 1327 | 시간 2152[s] | 퍼플렉서티 261.12\n",
      "| 에폭 1 |  반복 781 / 1327 | 시간 2203[s] | 퍼플렉서티 245.41\n",
      "| 에폭 1 |  반복 801 / 1327 | 시간 2255[s] | 퍼플렉서티 271.58\n",
      "| 에폭 1 |  반복 821 / 1327 | 시간 2307[s] | 퍼플렉서티 255.40\n",
      "| 에폭 1 |  반복 841 / 1327 | 시간 2358[s] | 퍼플렉서티 257.82\n",
      "| 에폭 1 |  반복 861 / 1327 | 시간 2410[s] | 퍼플렉서티 251.73\n",
      "| 에폭 1 |  반복 881 / 1327 | 시간 2461[s] | 퍼플렉서티 233.62\n",
      "| 에폭 1 |  반복 901 / 1327 | 시간 2513[s] | 퍼플렉서티 283.82\n",
      "| 에폭 1 |  반복 921 / 1327 | 시간 2565[s] | 퍼플렉서티 257.31\n",
      "| 에폭 1 |  반복 941 / 1327 | 시간 2616[s] | 퍼플렉서티 258.71\n",
      "| 에폭 1 |  반복 961 / 1327 | 시간 2668[s] | 퍼플렉서티 277.62\n",
      "| 에폭 1 |  반복 981 / 1327 | 시간 2733[s] | 퍼플렉서티 257.50\n",
      "| 에폭 1 |  반복 1001 / 1327 | 시간 2809[s] | 퍼플렉서티 217.60\n",
      "| 에폭 1 |  반복 1021 / 1327 | 시간 2883[s] | 퍼플렉서티 254.82\n",
      "| 에폭 1 |  반복 1041 / 1327 | 시간 2955[s] | 퍼플렉서티 233.56\n",
      "| 에폭 1 |  반복 1061 / 1327 | 시간 3028[s] | 퍼플렉서티 219.60\n",
      "| 에폭 1 |  반복 1081 / 1327 | 시간 3103[s] | 퍼플렉서티 191.04\n",
      "| 에폭 1 |  반복 1101 / 1327 | 시간 3177[s] | 퍼플렉서티 218.77\n",
      "| 에폭 1 |  반복 1121 / 1327 | 시간 3254[s] | 퍼플렉서티 257.18\n",
      "| 에폭 1 |  반복 1141 / 1327 | 시간 3343[s] | 퍼플렉서티 234.07\n",
      "| 에폭 1 |  반복 1161 / 1327 | 시간 3433[s] | 퍼플렉서티 221.21\n",
      "| 에폭 1 |  반복 1181 / 1327 | 시간 3541[s] | 퍼플렉서티 210.40\n",
      "| 에폭 1 |  반복 1201 / 1327 | 시간 3619[s] | 퍼플렉서티 180.87\n",
      "| 에폭 1 |  반복 1221 / 1327 | 시간 3698[s] | 퍼플렉서티 179.96\n",
      "| 에폭 1 |  반복 1241 / 1327 | 시간 3775[s] | 퍼플렉서티 209.90\n",
      "| 에폭 1 |  반복 1261 / 1327 | 시간 3851[s] | 퍼플렉서티 191.01\n",
      "| 에폭 1 |  반복 1281 / 1327 | 시간 3951[s] | 퍼플렉서티 198.83\n",
      "| 에폭 1 |  반복 1301 / 1327 | 시간 4095[s] | 퍼플렉서티 246.18\n",
      "| 에폭 1 |  반복 1321 / 1327 | 시간 4212[s] | 퍼플렉서티 234.31\n",
      "퍼플렉서티 평가 중 ...\n",
      "209 / 210\n",
      "검증 퍼플렉서티:  200.4967650718206\n",
      "--------------------------------------------------\n",
      "| 에폭 2 |  반복 1 / 1327 | 시간 4[s] | 퍼플렉서티 288.37\n",
      "| 에폭 2 |  반복 21 / 1327 | 시간 135[s] | 퍼플렉서티 230.39\n",
      "| 에폭 2 |  반복 41 / 1327 | 시간 252[s] | 퍼플렉서티 214.71\n",
      "| 에폭 2 |  반복 61 / 1327 | 시간 345[s] | 퍼플렉서티 197.11\n",
      "| 에폭 2 |  반복 81 / 1327 | 시간 445[s] | 퍼플렉서티 180.29\n",
      "| 에폭 2 |  반복 101 / 1327 | 시간 546[s] | 퍼플렉서티 170.13\n",
      "| 에폭 2 |  반복 121 / 1327 | 시간 645[s] | 퍼플렉서티 181.04\n",
      "| 에폭 2 |  반복 141 / 1327 | 시간 744[s] | 퍼플렉서티 200.22\n",
      "| 에폭 2 |  반복 161 / 1327 | 시간 834[s] | 퍼플렉서티 216.67\n",
      "| 에폭 2 |  반복 181 / 1327 | 시간 923[s] | 퍼플렉서티 226.62\n",
      "| 에폭 2 |  반복 201 / 1327 | 시간 1012[s] | 퍼플렉서티 209.62\n",
      "| 에폭 2 |  반복 221 / 1327 | 시간 1101[s] | 퍼플렉서티 203.73\n",
      "| 에폭 2 |  반복 241 / 1327 | 시간 1235[s] | 퍼플렉서티 199.13\n",
      "| 에폭 2 |  반복 261 / 1327 | 시간 1374[s] | 퍼플렉서티 213.73\n",
      "| 에폭 2 |  반복 281 / 1327 | 시간 1508[s] | 퍼플렉서티 206.90\n",
      "| 에폭 2 |  반복 301 / 1327 | 시간 1648[s] | 퍼플렉서티 190.06\n",
      "| 에폭 2 |  반복 321 / 1327 | 시간 1795[s] | 퍼플렉서티 156.15\n",
      "| 에폭 2 |  반복 341 / 1327 | 시간 1928[s] | 퍼플렉서티 198.22\n",
      "| 에폭 2 |  반복 361 / 1327 | 시간 2021[s] | 퍼플렉서티 216.70\n",
      "| 에폭 2 |  반복 381 / 1327 | 시간 2134[s] | 퍼플렉서티 172.24\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "#from common import config\n",
    "# GPU에서 실행하려면 아래 주석을 해제하세요(CuPy 필요).\n",
    "# ==============================================\n",
    "# config.GPU = True\n",
    "GPU = False\n",
    "# ==============================================\n",
    "#from common.optimizer import SGD\n",
    "#from common.trainer import RnnlmTrainer\n",
    "#from common.util import eval_perplexity, to_gpu\n",
    "from dataset import ptb\n",
    "#from better_rnnlm import BetterRnnlm\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 20\n",
    "wordvec_size = 650\n",
    "hidden_size = 650\n",
    "time_size = 35\n",
    "lr = 20.0\n",
    "max_epoch = 40\n",
    "max_grad = 0.25\n",
    "dropout = 0.5\n",
    "\n",
    "# 학습 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_val, _, _ = ptb.load_data('val')\n",
    "corpus_test, _, _ = ptb.load_data('test')\n",
    "\n",
    "if GPU:#config.GPU:\n",
    "    corpus = to_gpu(corpus)\n",
    "    corpus_val = to_gpu(corpus_val)\n",
    "    corpus_test = to_gpu(corpus_test)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "xs = corpus[:-1]\n",
    "ts = corpus[1:]\n",
    "\n",
    "model = BetterRnnlm(vocab_size, wordvec_size, hidden_size, dropout)\n",
    "optimizer = SGD(lr)\n",
    "trainer = RnnlmTrainer(model, optimizer)\n",
    "\n",
    "best_ppl = float('inf')\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(xs, ts, max_epoch=1, batch_size=batch_size,\n",
    "                time_size=time_size, max_grad=max_grad)\n",
    "\n",
    "    model.reset_state()\n",
    "    ppl = eval_perplexity(model, corpus_val)\n",
    "    print('검증 퍼플렉서티: ', ppl)\n",
    "\n",
    "    if best_ppl > ppl:\n",
    "        best_ppl = ppl\n",
    "        model.save_params()\n",
    "    else:\n",
    "        lr /= 4.0\n",
    "        optimizer.lr = lr\n",
    "\n",
    "    model.reset_state()\n",
    "    print('-' * 50)\n",
    "\n",
    "\n",
    "# 테스트 데이터로 평가\n",
    "model.reset_state()\n",
    "ppl_test = eval_perplexity(model, corpus_test)\n",
    "print('테스트 퍼플렉서티: ', ppl_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "| 에폭 1 |  반복 1 / 1327 | 시간 4[s] | 퍼플렉서티 10000.03\n",
    "| 에폭 1 |  반복 21 / 1327 | 시간 66[s] | 퍼플렉서티 4261.86\n",
    "| 에폭 1 |  반복 41 / 1327 | 시간 145[s] | 퍼플렉서티 1822.62\n",
    "| 에폭 1 |  반복 61 / 1327 | 시간 214[s] | 퍼플렉서티 1209.78\n",
    "| 에폭 1 |  반복 81 / 1327 | 시간 298[s] | 퍼플렉서티 1019.93\n",
    "| 에폭 1 |  반복 101 / 1327 | 시간 369[s] | 퍼플렉서티 868.60\n",
    "| 에폭 1 |  반복 121 / 1327 | 시간 443[s] | 퍼플렉서티 812.09\n",
    "| 에폭 1 |  반복 141 / 1327 | 시간 515[s] | 퍼플렉서티 712.12\n",
    "| 에폭 1 |  반복 161 / 1327 | 시간 580[s] | 퍼플렉서티 693.27\n",
    "| 에폭 1 |  반복 181 / 1327 | 시간 642[s] | 퍼플렉서티 697.01\n",
    "| 에폭 1 |  반복 201 / 1327 | 시간 700[s] | 퍼플렉서티 586.71\n",
    "| 에폭 1 |  반복 221 / 1327 | 시간 752[s] | 퍼플렉서티 589.80\n",
    "| 에폭 1 |  반복 241 / 1327 | 시간 808[s] | 퍼플렉서티 523.37\n",
    "| 에폭 1 |  반복 261 / 1327 | 시간 861[s] | 퍼플렉서티 543.52\n",
    "| 에폭 1 |  반복 281 / 1327 | 시간 913[s] | 퍼플렉서티 528.79\n",
    "| 에폭 1 |  반복 301 / 1327 | 시간 965[s] | 퍼플렉서티 443.17\n",
    "| 에폭 1 |  반복 321 / 1327 | 시간 1017[s] | 퍼플렉서티 399.42\n",
    "| 에폭 1 |  반복 341 / 1327 | 시간 1069[s] | 퍼플렉서티 458.72\n",
    "| 에폭 1 |  반복 361 / 1327 | 시간 1121[s] | 퍼플렉서티 469.54\n",
    "| 에폭 1 |  반복 381 / 1327 | 시간 1172[s] | 퍼플렉서티 389.24\n",
    "| 에폭 1 |  반복 401 / 1327 | 시간 1224[s] | 퍼플렉서티 408.31\n",
    "| 에폭 1 |  반복 421 / 1327 | 시간 1276[s] | 퍼플렉서티 395.18\n",
    "| 에폭 1 |  반복 441 / 1327 | 시간 1327[s] | 퍼플렉서티 378.52\n",
    "| 에폭 1 |  반복 461 / 1327 | 시간 1380[s] | 퍼플렉서티 378.24\n",
    "| 에폭 1 |  반복 481 / 1327 | 시간 1431[s] | 퍼플렉서티 350.82\n",
    "| 에폭 1 |  반복 501 / 1327 | 시간 1482[s] | 퍼플렉서티 366.41\n",
    "| 에폭 1 |  반복 521 / 1327 | 시간 1533[s] | 퍼플렉서티 345.96\n",
    "| 에폭 1 |  반복 541 / 1327 | 시간 1584[s] | 퍼플렉서티 365.99\n",
    "| 에폭 1 |  반복 561 / 1327 | 시간 1636[s] | 퍼플렉서티 322.22\n",
    "| 에폭 1 |  반복 581 / 1327 | 시간 1688[s] | 퍼플렉서티 299.71\n",
    "| 에폭 1 |  반복 601 / 1327 | 시간 1740[s] | 퍼플렉서티 380.61\n",
    "| 에폭 1 |  반복 621 / 1327 | 시간 1791[s] | 퍼플렉서티 347.62\n",
    "| 에폭 1 |  반복 641 / 1327 | 시간 1843[s] | 퍼플렉서티 319.57\n",
    "| 에폭 1 |  반복 661 / 1327 | 시간 1894[s] | 퍼플렉서티 304.22\n",
    "| 에폭 1 |  반복 681 / 1327 | 시간 1945[s] | 퍼플렉서티 258.09\n",
    "| 에폭 1 |  반복 701 / 1327 | 시간 1997[s] | 퍼플렉서티 281.79\n",
    "| 에폭 1 |  반복 721 / 1327 | 시간 2049[s] | 퍼플렉서티 293.47\n",
    "| 에폭 1 |  반복 741 / 1327 | 시간 2100[s] | 퍼플렉서티 248.47\n",
    "| 에폭 1 |  반복 761 / 1327 | 시간 2152[s] | 퍼플렉서티 261.12\n",
    "| 에폭 1 |  반복 781 / 1327 | 시간 2203[s] | 퍼플렉서티 245.41\n",
    "| 에폭 1 |  반복 801 / 1327 | 시간 2255[s] | 퍼플렉서티 271.58\n",
    "| 에폭 1 |  반복 821 / 1327 | 시간 2307[s] | 퍼플렉서티 255.40\n",
    "| 에폭 1 |  반복 841 / 1327 | 시간 2358[s] | 퍼플렉서티 257.82\n",
    "| 에폭 1 |  반복 861 / 1327 | 시간 2410[s] | 퍼플렉서티 251.73\n",
    "| 에폭 1 |  반복 881 / 1327 | 시간 2461[s] | 퍼플렉서티 233.62\n",
    "| 에폭 1 |  반복 901 / 1327 | 시간 2513[s] | 퍼플렉서티 283.82\n",
    "| 에폭 1 |  반복 921 / 1327 | 시간 2565[s] | 퍼플렉서티 257.31\n",
    "| 에폭 1 |  반복 941 / 1327 | 시간 2616[s] | 퍼플렉서티 258.71\n",
    "| 에폭 1 |  반복 961 / 1327 | 시간 2668[s] | 퍼플렉서티 277.62\n",
    "| 에폭 1 |  반복 981 / 1327 | 시간 2733[s] | 퍼플렉서티 257.50\n",
    "| 에폭 1 |  반복 1001 / 1327 | 시간 2809[s] | 퍼플렉서티 217.60\n",
    "| 에폭 1 |  반복 1021 / 1327 | 시간 2883[s] | 퍼플렉서티 254.82\n",
    "| 에폭 1 |  반복 1041 / 1327 | 시간 2955[s] | 퍼플렉서티 233.56\n",
    "| 에폭 1 |  반복 1061 / 1327 | 시간 3028[s] | 퍼플렉서티 219.60\n",
    "| 에폭 1 |  반복 1081 / 1327 | 시간 3103[s] | 퍼플렉서티 191.04\n",
    "| 에폭 1 |  반복 1101 / 1327 | 시간 3177[s] | 퍼플렉서티 218.77\n",
    "| 에폭 1 |  반복 1121 / 1327 | 시간 3254[s] | 퍼플렉서티 257.18\n",
    "| 에폭 1 |  반복 1141 / 1327 | 시간 3343[s] | 퍼플렉서티 234.07\n",
    "| 에폭 1 |  반복 1161 / 1327 | 시간 3433[s] | 퍼플렉서티 221.21\n",
    "| 에폭 1 |  반복 1181 / 1327 | 시간 3541[s] | 퍼플렉서티 210.40\n",
    "| 에폭 1 |  반복 1201 / 1327 | 시간 3619[s] | 퍼플렉서티 180.87\n",
    "| 에폭 1 |  반복 1221 / 1327 | 시간 3698[s] | 퍼플렉서티 179.96\n",
    "| 에폭 1 |  반복 1241 / 1327 | 시간 3775[s] | 퍼플렉서티 209.90\n",
    "| 에폭 1 |  반복 1261 / 1327 | 시간 3851[s] | 퍼플렉서티 191.01\n",
    "| 에폭 1 |  반복 1281 / 1327 | 시간 3951[s] | 퍼플렉서티 198.83\n",
    "| 에폭 1 |  반복 1301 / 1327 | 시간 4095[s] | 퍼플렉서티 246.18\n",
    "| 에폭 1 |  반복 1321 / 1327 | 시간 4212[s] | 퍼플렉서티 234.31\n",
    "퍼플렉서티 평가 중 ...\n",
    "209 / 210\n",
    "검증 퍼플렉서티:  200.4967650718206\n",
    "--------------------------------------------------\n",
    "| 에폭 2 |  반복 1 / 1327 | 시간 4[s] | 퍼플렉서티 288.37\n",
    "| 에폭 2 |  반복 21 / 1327 | 시간 135[s] | 퍼플렉서티 230.39\n",
    "| 에폭 2 |  반복 41 / 1327 | 시간 252[s] | 퍼플렉서티 214.71\n",
    "| 에폭 2 |  반복 61 / 1327 | 시간 345[s] | 퍼플렉서티 197.11\n",
    "| 에폭 2 |  반복 81 / 1327 | 시간 445[s] | 퍼플렉서티 180.29\n",
    "| 에폭 2 |  반복 101 / 1327 | 시간 546[s] | 퍼플렉서티 170.13\n",
    "| 에폭 2 |  반복 121 / 1327 | 시간 645[s] | 퍼플렉서티 181.04\n",
    "| 에폭 2 |  반복 141 / 1327 | 시간 744[s] | 퍼플렉서티 200.22\n",
    "| 에폭 2 |  반복 161 / 1327 | 시간 834[s] | 퍼플렉서티 216.67\n",
    "| 에폭 2 |  반복 181 / 1327 | 시간 923[s] | 퍼플렉서티 226.62\n",
    "| 에폭 2 |  반복 201 / 1327 | 시간 1012[s] | 퍼플렉서티 209.62\n",
    "| 에폭 2 |  반복 221 / 1327 | 시간 1101[s] | 퍼플렉서티 203.73\n",
    "| 에폭 2 |  반복 241 / 1327 | 시간 1235[s] | 퍼플렉서티 199.13\n",
    "| 에폭 2 |  반복 261 / 1327 | 시간 1374[s] | 퍼플렉서티 213.73\n",
    "| 에폭 2 |  반복 281 / 1327 | 시간 1508[s] | 퍼플렉서티 206.90\n",
    "| 에폭 2 |  반복 301 / 1327 | 시간 1648[s] | 퍼플렉서티 190.06\n",
    "| 에폭 2 |  반복 321 / 1327 | 시간 1795[s] | 퍼플렉서티 156.15\n",
    "| 에폭 2 |  반복 341 / 1327 | 시간 1928[s] | 퍼플렉서티 198.22\n",
    "| 에폭 2 |  반복 361 / 1327 | 시간 2021[s] | 퍼플렉서티 216.70\n",
    "| 에폭 2 |  반복 381 / 1327 | 시간 2134[s] | 퍼플렉서티 172.24\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "#from rnnlm import Rnnlm\n",
    "#from better_rnnlm import BetterRnnlm\n",
    "from dataset import ptb\n",
    "#from common.util import eval_perplexity\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = Rnnlm()\n",
    "    #model = BetterRnnlm()\n",
    "\n",
    "    # 학습된 매개변수 읽기\n",
    "    model.load_params()\n",
    "\n",
    "    corpus, _, _ = ptb.load_data('test')\n",
    "\n",
    "    model.reset_state()\n",
    "    ppl_test = eval_perplexity(model, corpus)\n",
    "    print('test perplexity: ', ppl_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# common_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================#\n",
    "#base_model.py\n",
    "#================================================================================#\n",
    "\n",
    "\n",
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import pickle\n",
    "#from common.np import *\n",
    "#from common.util import to_gpu, to_cpu\n",
    "\n",
    "\n",
    "class BaseModel:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = None, None\n",
    "\n",
    "    def forward(self, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save_params(self, file_name=None):\n",
    "        if file_name is None:\n",
    "            file_name = self.__class__.__name__ + '.pkl'\n",
    "\n",
    "        params = [p.astype(np.float16) for p in self.params]\n",
    "        if GPU:\n",
    "            params = [to_cpu(p) for p in params]\n",
    "\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=None):\n",
    "        if file_name is None:\n",
    "            file_name = self.__class__.__name__ + '.pkl'\n",
    "\n",
    "        if '/' in file_name:\n",
    "            file_name = file_name.replace('/', os.sep)\n",
    "\n",
    "        if not os.path.exists(file_name):\n",
    "            raise IOError('No file: ' + file_name)\n",
    "\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "\n",
    "        params = [p.astype('f') for p in params]\n",
    "        if GPU:\n",
    "            params = [to_gpu(p) for p in params]\n",
    "\n",
    "        for i, param in enumerate(self.params):\n",
    "            param[...] = params[i]\n",
    "\n",
    "\n",
    "#================================================================================#\n",
    "#config.py\n",
    "#================================================================================#\n",
    "# coding: utf-8\n",
    "\n",
    "GPU = False\n",
    "\n",
    "#================================================================================#\n",
    "#functions.py\n",
    "#================================================================================#\n",
    "# coding: utf-8\n",
    "#from common.np import *\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 정답 데이터가 원핫 벡터일 경우 정답 레이블 인덱스로 변환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "#================================================================================#\n",
    "#layers.py\n",
    "#================================================================================#\n",
    "\n",
    "# coding: utf-8\n",
    "#from common.np import *  # import numpy as np\n",
    "#from common.config import GPU\n",
    "#from common.functions import softmax, cross_entropy_error\n",
    "\n",
    "\n",
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.out = softmax(x)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = self.out * dout\n",
    "        sumdx = np.sum(dx, axis=1, keepdims=True)\n",
    "        dx -= self.out * sumdx\n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None  # softmax의 출력\n",
    "        self.t = None  # 정답 레이블\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "\n",
    "        # 정답 레이블이 원핫 벡터일 경우 정답의 인덱스로 변환\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "\n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = self.y.copy()\n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx = dx / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx\n",
    "\n",
    "#================================================================================#\n",
    "#layers.py\n",
    "#================================================================================#\n",
    "\n",
    "class SigmoidWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.loss = None\n",
    "        self.y = None  # sigmoid의 출력\n",
    "        self.t = None  # 정답 데이터\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "        self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = (self.y - self.t) * dout / batch_size\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Dropout:\n",
    "    '''\n",
    "    http://arxiv.org/abs/1207.0580\n",
    "    '''\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.params, self.grads = [], []\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "        return None\n",
    "\n",
    "\n",
    "#================================================================================#\n",
    "#np.py\n",
    "#================================================================================#\n",
    "\n",
    "# coding: utf-8\n",
    "#from common.config import GPU\n",
    "\n",
    "\n",
    "if GPU:\n",
    "    import cupy as np\n",
    "    np.cuda.set_allocator(np.cuda.MemoryPool().malloc)\n",
    "    np.add.at = np.scatter_add\n",
    "\n",
    "    print('\\033[92m' + '-' * 60 + '\\033[0m')\n",
    "    print(' ' * 23 + '\\033[92mGPU Mode (cupy)\\033[0m')\n",
    "    print('\\033[92m' + '-' * 60 + '\\033[0m\\n')\n",
    "else:\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "#================================================================================#\n",
    "#optimizer.py\n",
    "#================================================================================#\n",
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "#from common.np import *\n",
    "\n",
    "\n",
    "class SGD:\n",
    "    '''\n",
    "    확률적 경사하강법(Stochastic Gradient Descent)\n",
    "    '''\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]\n",
    "\n",
    "\n",
    "class Momentum:\n",
    "    '''\n",
    "    모멘텀 SGG(Momentum SGD)\n",
    "    '''\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = []\n",
    "            for param in params:\n",
    "                self.v.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.v[i] = self.momentum * self.v[i] - self.lr * grads[i]\n",
    "            params[i] += self.v[i]\n",
    "\n",
    "\n",
    "class Nesterov:\n",
    "    '''\n",
    "    네스테로프 가속 경사(NAG; Nesterov's Accelerated Gradient) (http://arxiv.org/abs/1212.0901)\n",
    "    '네스테로프 모멘텀 최적화'라고도 한다.\n",
    "    '''\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = []\n",
    "            for param in params:\n",
    "                self.v.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.v[i] *= self.momentum\n",
    "            self.v[i] -= self.lr * grads[i]\n",
    "            params[i] += self.momentum * self.momentum * self.v[i]\n",
    "            params[i] -= (1 + self.momentum) * self.lr * grads[i]\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "    '''\n",
    "    AdaGrad\n",
    "    '''\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = []\n",
    "            for param in params:\n",
    "                self.h.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.h[i] += grads[i] * grads[i]\n",
    "            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)\n",
    "\n",
    "\n",
    "class RMSprop:\n",
    "    '''\n",
    "    RMSprop\n",
    "    '''\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = []\n",
    "            for param in params:\n",
    "                self.h.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.h[i] *= self.decay_rate\n",
    "            self.h[i] += (1 - self.decay_rate) * grads[i] * grads[i]\n",
    "            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    '''\n",
    "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
    "    '''\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "            \n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)\n",
    "\n",
    "#================================================================================#\n",
    "#time_layers.py\n",
    "#================================================================================#\n",
    "# coding: utf-8\n",
    "#from common.np import *  # import numpy as np (or import cupy as np)\n",
    "#from common.layers import *\n",
    "#from common.functions import sigmoid\n",
    "\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
    "        h_next = np.tanh(t)\n",
    "\n",
    "        self.cache = (x, h_prev, h_next)\n",
    "        return h_next\n",
    "\n",
    "    def backward(self, dh_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, h_next = self.cache\n",
    "\n",
    "        dt = dh_next * (1 - h_next ** 2)\n",
    "        db = np.sum(dt, axis=0)\n",
    "        dWh = np.dot(h_prev.T, dt)\n",
    "        dh_prev = np.dot(dt, Wh.T)\n",
    "        dWx = np.dot(x.T, dt)\n",
    "        dx = np.dot(dt, Wx.T)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        return dx, dh_prev\n",
    "\n",
    "\n",
    "class TimeRNN:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.dh = None, None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        D, H = Wx.shape\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = RNN(*self.params)\n",
    "            self.h = layer.forward(xs[:, t, :], self.h)\n",
    "            hs[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D, H = Wx.shape\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh = 0\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
    "            dxs[:, t, :] = dx\n",
    "\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        '''\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Wx: 입력 x에 대한 가중치 매개변수(4개분의 가중치가 담겨 있음)\n",
    "        Wh: 은닉 상태 h에 대한 가장추 매개변수(4개분의 가중치가 담겨 있음)\n",
    "        b: 편향（4개분의 편향이 담겨 있음）\n",
    "        '''\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, H = h_prev.shape\n",
    "\n",
    "        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n",
    "\n",
    "        f = A[:, :H]\n",
    "        g = A[:, H:2*H]\n",
    "        i = A[:, 2*H:3*H]\n",
    "        o = A[:, 3*H:]\n",
    "\n",
    "        f = sigmoid(f)\n",
    "        g = np.tanh(g)\n",
    "        i = sigmoid(i)\n",
    "        o = sigmoid(o)\n",
    "\n",
    "        c_next = f * c_prev + g * i\n",
    "        h_next = o * np.tanh(c_next)\n",
    "\n",
    "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "    def backward(self, dh_next, dc_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
    "\n",
    "        tanh_c_next = np.tanh(c_next)\n",
    "\n",
    "        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "\n",
    "        dc_prev = ds * f\n",
    "\n",
    "        di = ds * g\n",
    "        df = ds * c_prev\n",
    "        do = dh_next * tanh_c_next\n",
    "        dg = ds * i\n",
    "\n",
    "        di *= i * (1 - i)\n",
    "        df *= f * (1 - f)\n",
    "        do *= o * (1 - o)\n",
    "        dg *= (1 - g ** 2)\n",
    "\n",
    "        dA = np.hstack((df, dg, di, do))\n",
    "\n",
    "        dWh = np.dot(h_prev.T, dA)\n",
    "        dWx = np.dot(x.T, dA)\n",
    "        db = dA.sum(axis=0)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        dx = np.dot(dA, Wx.T)\n",
    "        dh_prev = np.dot(dA, Wh.T)\n",
    "\n",
    "        return dx, dh_prev, dc_prev\n",
    "\n",
    "\n",
    "class TimeLSTM:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.c = None, None\n",
    "        self.dh = None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        H = Wh.shape[0]\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "        if not self.stateful or self.c is None:\n",
    "            self.c = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = LSTM(*self.params)\n",
    "            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n",
    "            hs[:, t, :] = self.h\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D = Wx.shape[0]\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh, dc = 0, 0\n",
    "\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n",
    "            dxs[:, t, :] = dx\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h, c=None):\n",
    "        self.h, self.c = h, c\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h, self.c = None, None\n",
    "\n",
    "\n",
    "class TimeEmbedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.layers = None\n",
    "        self.W = W\n",
    "\n",
    "    def forward(self, xs):\n",
    "        N, T = xs.shape\n",
    "        V, D = self.W.shape\n",
    "\n",
    "        out = np.empty((N, T, D), dtype='f')\n",
    "        self.layers = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Embedding(self.W)\n",
    "            out[:, t, :] = layer.forward(xs[:, t])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, D = dout.shape\n",
    "\n",
    "        grad = 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            layer.backward(dout[:, t, :])\n",
    "            grad += layer.grads[0]\n",
    "\n",
    "        self.grads[0][...] = grad\n",
    "        return None\n",
    "\n",
    "\n",
    "class TimeAffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        rx = x.reshape(N*T, -1)\n",
    "        out = np.dot(rx, W) + b\n",
    "        self.x = x\n",
    "        return out.reshape(N, T, -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        x = self.x\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        dout = dout.reshape(N*T, -1)\n",
    "        rx = x.reshape(N*T, -1)\n",
    "\n",
    "        db = np.sum(dout, axis=0)\n",
    "        dW = np.dot(rx.T, dout)\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dx = dx.reshape(*x.shape)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class TimeSoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        self.ignore_label = -1\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T, V = xs.shape\n",
    "\n",
    "        if ts.ndim == 3:  # 정답 레이블이 원핫 벡터인 경우\n",
    "            ts = ts.argmax(axis=2)\n",
    "\n",
    "        mask = (ts != self.ignore_label)\n",
    "\n",
    "        # 배치용과 시계열용을 정리(reshape)\n",
    "        xs = xs.reshape(N * T, V)\n",
    "        ts = ts.reshape(N * T)\n",
    "        mask = mask.reshape(N * T)\n",
    "\n",
    "        ys = softmax(xs)\n",
    "        ls = np.log(ys[np.arange(N * T), ts])\n",
    "        ls *= mask  # ignore_label에 해당하는 데이터는 손실을 0으로 설정\n",
    "        loss = -np.sum(ls)\n",
    "        loss /= mask.sum()\n",
    "\n",
    "        self.cache = (ts, ys, mask, (N, T, V))\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ts, ys, mask, (N, T, V) = self.cache\n",
    "\n",
    "        dx = ys\n",
    "        dx[np.arange(N * T), ts] -= 1\n",
    "        dx *= dout\n",
    "        dx /= mask.sum()\n",
    "        dx *= mask[:, np.newaxis]  # ignore_labelㅇㅔ 해당하는 데이터는 기울기를 0으로 설정\n",
    "\n",
    "        dx = dx.reshape((N, T, V))\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class TimeDropout:\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.params, self.grads = [], []\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "        self.train_flg = True\n",
    "\n",
    "    def forward(self, xs):\n",
    "        if self.train_flg:\n",
    "            flg = np.random.rand(*xs.shape) > self.dropout_ratio\n",
    "            scale = 1 / (1.0 - self.dropout_ratio)\n",
    "            self.mask = flg.astype(np.float32) * scale\n",
    "\n",
    "            return xs * self.mask\n",
    "        else:\n",
    "            return xs\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "\n",
    "class TimeBiLSTM:\n",
    "    def __init__(self, Wx1, Wh1, b1,\n",
    "                 Wx2, Wh2, b2, stateful=False):\n",
    "        self.forward_lstm = TimeLSTM(Wx1, Wh1, b1, stateful)\n",
    "        self.backward_lstm = TimeLSTM(Wx2, Wh2, b2, stateful)\n",
    "        self.params = self.forward_lstm.params + self.backward_lstm.params\n",
    "        self.grads = self.forward_lstm.grads + self.backward_lstm.grads\n",
    "\n",
    "    def forward(self, xs):\n",
    "        o1 = self.forward_lstm.forward(xs)\n",
    "        o2 = self.backward_lstm.forward(xs[:, ::-1])\n",
    "        o2 = o2[:, ::-1]\n",
    "\n",
    "        out = np.concatenate((o1, o2), axis=2)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        H = dhs.shape[2] // 2\n",
    "        do1 = dhs[:, :, :H]\n",
    "        do2 = dhs[:, :, H:]\n",
    "\n",
    "        dxs1 = self.forward_lstm.backward(do1)\n",
    "        do2 = do2[:, ::-1]\n",
    "        dxs2 = self.backward_lstm.backward(do2)\n",
    "        dxs2 = dxs2[:, ::-1]\n",
    "        dxs = dxs1 + dxs2\n",
    "        return dxs\n",
    "\n",
    "# ====================================================================== #\n",
    "# 이 아래의 계층들은 책에서 설명하지 않았거나\n",
    "# 처리 속도보다는 쉽게 이해할 수 있도록 구현했습니다.\n",
    "#\n",
    "# TimeSigmoidWithLoss: 시계열 데이터용 시그모이드 + 손실 계층\n",
    "# GRU: GRU 계층\n",
    "# TimeGRU: 시계열 데이터용 GRU 계층\n",
    "# BiTimeLSTM: 양방향 LSTM 계층\n",
    "# Simple_TimeSoftmaxWithLoss：간단한 TimeSoftmaxWithLoss 계층의 구현\n",
    "# Simple_TimeAffine: 간단한 TimeAffine 계층의 구현\n",
    "# ====================================================================== #\n",
    "\n",
    "\n",
    "class TimeSigmoidWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.xs_shape = None\n",
    "        self.layers = None\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T = xs.shape\n",
    "        self.xs_shape = xs.shape\n",
    "\n",
    "        self.layers = []\n",
    "        loss = 0\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = SigmoidWithLoss()\n",
    "            loss += layer.forward(xs[:, t], ts[:, t])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return loss / T\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        N, T = self.xs_shape\n",
    "        dxs = np.empty(self.xs_shape, dtype='f')\n",
    "\n",
    "        dout *= 1/T\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dxs[:, t] = layer.backward(dout)\n",
    "\n",
    "        return dxs\n",
    "\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, Wx, Wh):\n",
    "        '''\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Wx: 입력 x에 대한 가중치 매개변수(3개 분의 가중치가 담겨 있음)\n",
    "        Wh: 은닉 상태 h에 대한 가중치 매개변수(3개 분의 가중치가 담겨 있음)\n",
    "        '''\n",
    "        self.Wx, self.Wh = Wx, Wh\n",
    "        self.dWx, self.dWh = None, None\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        H, H3 = self.Wh.shape\n",
    "        Wxz, Wxr, Wx = self.Wx[:, :H], self.Wx[:, H:2 * H], self.Wx[:, 2 * H:]\n",
    "        Whz, Whr, Wh = self.Wh[:, :H], self.Wh[:, H:2 * H], self.Wh[:, 2 * H:]\n",
    "\n",
    "        z = sigmoid(np.dot(x, Wxz) + np.dot(h_prev, Whz))\n",
    "        r = sigmoid(np.dot(x, Wxr) + np.dot(h_prev, Whr))\n",
    "        h_hat = np.tanh(np.dot(x, Wx) + np.dot(r*h_prev, Wh))\n",
    "        h_next = (1-z) * h_prev + z * h_hat\n",
    "\n",
    "        self.cache = (x, h_prev, z, r, h_hat)\n",
    "\n",
    "        return h_next\n",
    "\n",
    "    def backward(self, dh_next):\n",
    "        H, H3 = self.Wh.shape\n",
    "        Wxz, Wxr, Wx = self.Wx[:, :H], self.Wx[:, H:2 * H], self.Wx[:, 2 * H:]\n",
    "        Whz, Whr, Wh = self.Wh[:, :H], self.Wh[:, H:2 * H], self.Wh[:, 2 * H:]\n",
    "        x, h_prev, z, r, h_hat = self.cache\n",
    "\n",
    "        dh_hat =dh_next * z\n",
    "        dh_prev = dh_next * (1-z)\n",
    "\n",
    "        # tanh\n",
    "        dt = dh_hat * (1 - h_hat ** 2)\n",
    "        dWh = np.dot((r * h_prev).T, dt)\n",
    "        dhr = np.dot(dt, Wh.T)\n",
    "        dWx = np.dot(x.T, dt)\n",
    "        dx = np.dot(dt, Wx.T)\n",
    "        dh_prev += r * dhr\n",
    "\n",
    "        # update gate(z)\n",
    "        dz = dh_next * h_hat - dh_next * h_prev\n",
    "        dt = dz * z * (1-z)\n",
    "        dWhz = np.dot(h_prev.T, dt)\n",
    "        dh_prev += np.dot(dt, Whz.T)\n",
    "        dWxz = np.dot(x.T, dt)\n",
    "        dx += np.dot(dt, Wxz.T)\n",
    "\n",
    "        # rest gate(r)\n",
    "        dr = dhr * h_prev\n",
    "        dt = dr * r * (1-r)\n",
    "        dWhr = np.dot(h_prev.T, dt)\n",
    "        dh_prev += np.dot(dt, Whr.T)\n",
    "        dWxr = np.dot(x.T, dt)\n",
    "        dx += np.dot(dt, Wxr.T)\n",
    "\n",
    "        self.dWx = np.hstack((dWxz, dWxr, dWx))\n",
    "        self.dWh = np.hstack((dWhz, dWhr, dWh))\n",
    "\n",
    "        return dx, dh_prev\n",
    "\n",
    "\n",
    "class TimeGRU:\n",
    "    def __init__(self, Wx, Wh, stateful=False):\n",
    "        self.Wx, self.Wh = Wx, Wh\n",
    "        selfdWx, self.dWh = None, None\n",
    "        self.layers = None\n",
    "        self.h, self.dh = None, None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        N, T, D = xs.shape\n",
    "        H, H3 = self.Wh.shape\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = GRU(self.Wx, self.Wh)\n",
    "            self.h = layer.forward(xs[:, t, :], self.h)\n",
    "            hs[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        N, T, H = dhs.shape\n",
    "        D = self.Wx.shape[0]\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        self.dWx, self.dWh = 0, 0\n",
    "\n",
    "        dh = 0\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
    "\n",
    "            dxs[:, t, :] = dx\n",
    "            self.dWx += layer.dWx\n",
    "            self.dWh += layer.dWh\n",
    "\n",
    "        self.dh = dh\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "\n",
    "\n",
    "class Simple_TimeSoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T, V = xs.shape\n",
    "        layers = []\n",
    "        loss = 0\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = SoftmaxWithLoss()\n",
    "            loss += layer.forward(xs[:, t, :], ts[:, t])\n",
    "            layers.append(layer)\n",
    "        loss /= T\n",
    "\n",
    "        self.cache = (layers, xs)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        layers, xs = self.cache\n",
    "        N, T, V = xs.shape\n",
    "        dxs = np.empty(xs.shape, dtype='f')\n",
    "\n",
    "        dout *= 1/T\n",
    "        for t in range(T):\n",
    "            layer = layers[t]\n",
    "            dxs[:, t, :] = layer.backward(dout)\n",
    "\n",
    "        return dxs\n",
    "\n",
    "\n",
    "class Simple_TimeAffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W, self.b = W, b\n",
    "        self.dW, self.db = None, None\n",
    "        self.layers = None\n",
    "\n",
    "    def forward(self, xs):\n",
    "        N, T, D = xs.shape\n",
    "        D, M = self.W.shape\n",
    "\n",
    "        self.layers = []\n",
    "        out = np.empty((N, T, M), dtype='f')\n",
    "        for t in range(T):\n",
    "            layer = Affine(self.W, self.b)\n",
    "            out[:, t, :] = layer.forward(xs[:, t, :])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, M = dout.shape\n",
    "        D, M = self.W.shape\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        self.dW, self.db = 0, 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dxs[:, t, :] = layer.backward(dout[:, t, :])\n",
    "\n",
    "            self.dW += layer.dW\n",
    "            self.db += layer.db\n",
    "\n",
    "        return dxs\n",
    "\n",
    "#================================================================================#\n",
    "#trainer.py\n",
    "#================================================================================#\n",
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "#from common.np import *  # import numpy as np\n",
    "#from common.util import clip_grads\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            # 뒤섞기\n",
    "            idx = numpy.random.permutation(numpy.arange(data_size))\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "                # 기울기 구해 매개변수 갱신\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.params, model.grads)  # 공유된 가중치를 하나로 모음\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # 평가\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    avg_loss = total_loss / loss_count\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print('| 에폭 %d |  반복 %d / %d | 시간 %d[s] | 손실 %.2f'\n",
    "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
    "                    self.loss_list.append(float(avg_loss))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = numpy.arange(len(self.loss_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.loss_list, label='train')\n",
    "        plt.xlabel('반복 (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('손실')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class RnnlmTrainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.time_idx = None\n",
    "        self.ppl_list = None\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def get_batch(self, x, t, batch_size, time_size):\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "\n",
    "        data_size = len(x)\n",
    "        jump = data_size // batch_size\n",
    "        offsets = [i * jump for i in range(batch_size)]  # 배치에서 각 샘플을 읽기 시작하는 위치\n",
    "\n",
    "        for time in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, time] = x[(offset + self.time_idx) % data_size]\n",
    "                batch_t[i, time] = t[(offset + self.time_idx) % data_size]\n",
    "            self.time_idx += 1\n",
    "        return batch_x, batch_t\n",
    "\n",
    "    def fit(self, xs, ts, max_epoch=10, batch_size=20, time_size=35,\n",
    "            max_grad=None, eval_interval=20):\n",
    "        data_size = len(xs)\n",
    "        max_iters = data_size // (batch_size * time_size)\n",
    "        self.time_idx = 0\n",
    "        self.ppl_list = []\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            for iters in range(max_iters):\n",
    "                batch_x, batch_t = self.get_batch(xs, ts, batch_size, time_size)\n",
    "\n",
    "                # 기울기를 구해 매개변수 갱신\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.params, model.grads)  # 공유된 가중치를 하나로 모음\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # 퍼플렉서티 평가\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    ppl = np.exp(total_loss / loss_count)\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print('| 에폭 %d |  반복 %d / %d | 시간 %d[s] | 퍼플렉서티 %.2f'\n",
    "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, ppl))\n",
    "                    self.ppl_list.append(float(ppl))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = numpy.arange(len(self.ppl_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.ppl_list, label='train')\n",
    "        plt.xlabel('반복 (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('퍼플렉서티')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def remove_duplicate(params, grads):\n",
    "    '''\n",
    "    매개변수 배열 중 중복되는 가중치를 하나로 모아\n",
    "    그 가중치에 대응하는 기울기를 더한다.\n",
    "    '''\n",
    "    params, grads = params[:], grads[:]  # copy list\n",
    "\n",
    "    while True:\n",
    "        find_flg = False\n",
    "        L = len(params)\n",
    "\n",
    "        for i in range(0, L - 1):\n",
    "            for j in range(i + 1, L):\n",
    "                # 가중치 공유 시\n",
    "                if params[i] is params[j]:\n",
    "                    grads[i] += grads[j]  # 경사를 더함\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                # 가중치를 전치행렬로 공유하는 경우(weight tying)\n",
    "                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
    "                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
    "                    grads[i] += grads[j].T\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "\n",
    "                if find_flg: break\n",
    "            if find_flg: break\n",
    "\n",
    "        if not find_flg: break\n",
    "\n",
    "    return params, grads\n",
    "\n",
    "\n",
    "#================================================================================#\n",
    "#util.py\n",
    "#================================================================================#\n",
    "\n",
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "#from common.np import *\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "\n",
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    '''코사인 유사도 산출\n",
    "\n",
    "    :param x: 벡터\n",
    "    :param y: 벡터\n",
    "    :param eps: '0으로 나누기'를 방지하기 위한 작은 값\n",
    "    :return:\n",
    "    '''\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
    "    return np.dot(nx, ny)\n",
    "\n",
    "\n",
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    '''유사 단어 검색\n",
    "\n",
    "    :param query: 쿼리(텍스트)\n",
    "    :param word_to_id: 단어에서 단어 ID로 변환하는 딕셔너리\n",
    "    :param id_to_word: 단어 ID에서 단어로 변환하는 딕셔너리\n",
    "    :param word_matrix: 단어 벡터를 정리한 행렬. 각 행에 해당 단어 벡터가 저장되어 있다고 가정한다.\n",
    "    :param top: 상위 몇 개까지 출력할 지 지정\n",
    "    '''\n",
    "    if query not in word_to_id:\n",
    "        print('%s(을)를 찾을 수 없습니다.' % query)\n",
    "        return\n",
    "\n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    vocab_size = len(id_to_word)\n",
    "\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "\n",
    "    # 코사인 유사도를 기준으로 내림차순으로 출력\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "\n",
    "\n",
    "def convert_one_hot(corpus, vocab_size):\n",
    "    '''원핫 표현으로 변환\n",
    "\n",
    "    :param corpus: 단어 ID 목록(1차원 또는 2차원 넘파이 배열)\n",
    "    :param vocab_size: 어휘 수\n",
    "    :return: 원핫 표현(2차원 또는 3차원 넘파이 배열)\n",
    "    '''\n",
    "    N = corpus.shape[0]\n",
    "\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    '''동시발생 행렬 생성\n",
    "\n",
    "    :param corpus: 말뭉치(단어 ID 목록)\n",
    "    :param vocab_size: 어휘 수\n",
    "    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n",
    "    :return: 동시발생 행렬\n",
    "    '''\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "\n",
    "    return co_matrix\n",
    "\n",
    "\n",
    "def ppmi(C, verbose=False, eps = 1e-8):\n",
    "    '''PPMI(점별 상호정보량) 생성\n",
    "\n",
    "    :param C: 동시발생 행렬\n",
    "    :param verbose: 진행 상황을 출력할지 여부\n",
    "    :return:\n",
    "    '''\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis=0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
    "            M[i, j] = max(0, pmi)\n",
    "\n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total//100) == 0:\n",
    "                    print('%.1f%% 완료' % (100*cnt/total))\n",
    "    return M\n",
    "\n",
    "\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    '''맥락과 타깃 생성\n",
    "\n",
    "    :param corpus: 말뭉치(단어 ID 목록)\n",
    "    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n",
    "    :return:\n",
    "    '''\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "\n",
    "    return np.array(contexts), np.array(target)\n",
    "\n",
    "\n",
    "def to_cpu(x):\n",
    "    import numpy\n",
    "    if type(x) == numpy.ndarray:\n",
    "        return x\n",
    "    return np.asnumpy(x)\n",
    "\n",
    "\n",
    "def to_gpu(x):\n",
    "    import cupy\n",
    "    if type(x) == cupy.ndarray:\n",
    "        return x\n",
    "    return cupy.asarray(x)\n",
    "\n",
    "\n",
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate\n",
    "\n",
    "\n",
    "def eval_perplexity(model, corpus, batch_size=10, time_size=35):\n",
    "    print('퍼플렉서티 평가 중 ...')\n",
    "    corpus_size = len(corpus)\n",
    "    total_loss, loss_cnt = 0, 0\n",
    "    max_iters = (corpus_size - 1) // (batch_size * time_size)\n",
    "    jump = (corpus_size - 1) // batch_size\n",
    "\n",
    "    for iters in range(max_iters):\n",
    "        xs = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "        ts = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "        time_offset = iters * time_size\n",
    "        offsets = [time_offset + (i * jump) for i in range(batch_size)]\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                xs[i, t] = corpus[(offset + t) % corpus_size]\n",
    "                ts[i, t] = corpus[(offset + t + 1) % corpus_size]\n",
    "\n",
    "        try:\n",
    "            loss = model.forward(xs, ts, train_flg=False)\n",
    "        except TypeError:\n",
    "            loss = model.forward(xs, ts)\n",
    "        total_loss += loss\n",
    "\n",
    "        sys.stdout.write('\\r%d / %d' % (iters, max_iters))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    print('')\n",
    "    ppl = np.exp(total_loss / max_iters)\n",
    "    return ppl\n",
    "\n",
    "\n",
    "def eval_seq2seq(model, question, correct, id_to_char,\n",
    "                 verbos=False, is_reverse=False):\n",
    "    correct = correct.flatten()\n",
    "    # 머릿글자\n",
    "    start_id = correct[0]\n",
    "    correct = correct[1:]\n",
    "    guess = model.generate(question, start_id, len(correct))\n",
    "\n",
    "    # 문자열로 변환\n",
    "    question = ''.join([id_to_char[int(c)] for c in question.flatten()])\n",
    "    correct = ''.join([id_to_char[int(c)] for c in correct])\n",
    "    guess = ''.join([id_to_char[int(c)] for c in guess])\n",
    "\n",
    "    if verbos:\n",
    "        if is_reverse:\n",
    "            question = question[::-1]\n",
    "\n",
    "        colors = {'ok': '\\033[92m', 'fail': '\\033[91m', 'close': '\\033[0m'}\n",
    "        print('Q', question)\n",
    "        print('T', correct)\n",
    "\n",
    "        is_windows = os.name == 'nt'\n",
    "\n",
    "        if correct == guess:\n",
    "            mark = colors['ok'] + '☑' + colors['close']\n",
    "            if is_windows:\n",
    "                mark = 'O'\n",
    "            print(mark + ' ' + guess)\n",
    "        else:\n",
    "            mark = colors['fail'] + '☒' + colors['close']\n",
    "            if is_windows:\n",
    "                mark = 'X'\n",
    "            print(mark + ' ' + guess)\n",
    "        print('---')\n",
    "\n",
    "    return 1 if guess == correct else 0\n",
    "\n",
    "\n",
    "def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):\n",
    "    for word in (a, b, c):\n",
    "        if word not in word_to_id:\n",
    "            print('%s(을)를 찾을 수 없습니다.' % word)\n",
    "            return\n",
    "\n",
    "    print('\\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')\n",
    "    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n",
    "    query_vec = b_vec - a_vec + c_vec\n",
    "    query_vec = normalize(query_vec)\n",
    "\n",
    "    similarity = np.dot(word_matrix, query_vec)\n",
    "\n",
    "    if answer is not None:\n",
    "        print(\"==>\" + answer + \":\" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))\n",
    "\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if np.isnan(similarity[i]):\n",
    "            continue\n",
    "        if id_to_word[i] in (a, b, c):\n",
    "            continue\n",
    "        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    if x.ndim == 2:\n",
    "        s = np.sqrt((x * x).sum(1))\n",
    "        x /= s.reshape((s.shape[0], 1))\n",
    "    elif x.ndim == 1:\n",
    "        s = np.sqrt((x * x).sum())\n",
    "        x /= s\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
