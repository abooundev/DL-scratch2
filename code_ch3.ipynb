{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cbow_predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.4406188   0.04038909 -0.66497779  1.3811278  -0.39000136  0.2046645\n",
      "   0.89314203]]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "#from common.layers import MatMul\n",
    "\n",
    "\n",
    "# 샘플 맥락 데이터\n",
    "c0 = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
    "c1 = np.array([[0, 0, 1, 0, 0, 0, 0]])\n",
    "\n",
    "# 가중치 초기화\n",
    "W_in = np.random.randn(7, 3)\n",
    "W_out = np.random.randn(3, 7)\n",
    "\n",
    "# 계층 생성\n",
    "in_layer0 = MatMul(W_in)\n",
    "in_layer1 = MatMul(W_in)\n",
    "out_layer = MatMul(W_out)\n",
    "\n",
    "# 순전파\n",
    "h0 = in_layer0.forward(c0)\n",
    "h1 = in_layer1.forward(c1)\n",
    "h = 0.5 * (h0 + h1)\n",
    "s = out_layer.forward(h)\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple_cbow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "#from common.layers import MatMul, SoftmaxWithLoss\n",
    "\n",
    "\n",
    "class SimpleCBOW:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.in_layer0 = MatMul(W_in)\n",
    "        self.in_layer1 = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h0 = self.in_layer0.forward(contexts[:, 0])\n",
    "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
    "        h = (h0 + h1) * 0.5\n",
    "        score = self.out_layer.forward(h)\n",
    "        loss = self.loss_layer.forward(score, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da *= 0.5\n",
    "        self.in_layer1.backward(da)\n",
    "        self.in_layer0.backward(da)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple_skip_gram.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "#from common.layers import MatMul, SoftmaxWithLoss\n",
    "\n",
    "class SimpleSkipGram:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 가중치 초기화\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.in_layer = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer1 = SoftmaxWithLoss()\n",
    "        self.loss_layer2 = SoftmaxWithLoss()\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        layers = [self.in_layer, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = self.in_layer.forward(target)\n",
    "        s = self.out_layer.forward(h)\n",
    "        l1 = self.loss_layer1.forward(s, contexts[:, 0])\n",
    "        l2 = self.loss_layer2.forward(s, contexts[:, 1])\n",
    "        loss = l1 + l2\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dl1 = self.loss_layer1.backward(dout)\n",
    "        dl2 = self.loss_layer2.backward(dout)\n",
    "        ds = dl1 + dl2\n",
    "        dh = self.out_layer.backward(ds)\n",
    "        self.in_layer.backward(dh)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "#from common.trainer import Trainer\n",
    "#from common.optimizer import Adam\n",
    "#from simple_cbow import SimpleCBOW\n",
    "#from common.util import preprocess, create_contexts_target, convert_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus: [0 1 2 3 4 1 5 6]\n",
      "word_to_id: {'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "id_to_word: {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
      "vocab_size: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ncorpus: [0 1 2 3 4 1 5 6]\\nword_to_id: {'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\\nid_to_word: {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\\nvocab_size: 7\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "print(\"corpus:\", corpus)\n",
    "print(\"word_to_id:\", word_to_id)\n",
    "print(\"id_to_word:\", id_to_word)\n",
    "print(\"vocab_size:\", vocab_size)\n",
    "\"\"\"\n",
    "corpus: [0 1 2 3 4 1 5 6]\n",
    "word_to_id: {'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
    "id_to_word: {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
    "vocab_size: 7\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contexts: [[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n",
      "target: [1 2 3 4 1 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ncontexts: [[0 2]\\n [1 3]\\n [2 4]\\n [3 1]\\n [4 5]\\n [1 6]]\\ntarget: [1 2 3 4 1 5]\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "print(\"contexts:\", contexts)\n",
    "print(\"target:\", target)\n",
    "\"\"\"\n",
    "contexts: [[0 2]\n",
    " [1 3]\n",
    " [2 4]\n",
    " [3 1]\n",
    " [4 5]\n",
    " [1 6]]\n",
    "target: [1 2 3 4 1 5]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contexts: [[[1 0 0 0 0 0 0]\n",
      "  [0 0 1 0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0 0 0 0]\n",
      "  [0 0 0 1 0 0 0]]\n",
      "\n",
      " [[0 0 1 0 0 0 0]\n",
      "  [0 0 0 0 1 0 0]]\n",
      "\n",
      " [[0 0 0 1 0 0 0]\n",
      "  [0 1 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 1 0 0]\n",
      "  [0 0 0 0 0 1 0]]\n",
      "\n",
      " [[0 1 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 1]]]\n",
      "target: [[0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "print(\"contexts:\", contexts)\n",
    "print(\"target:\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 2 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 3 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 4 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 5 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 6 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 7 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 8 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 9 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
      "| 에폭 10 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 11 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 12 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 13 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 14 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 15 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 16 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 17 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 18 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 19 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 20 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 21 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 22 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 23 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 24 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 25 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 26 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 27 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 28 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 29 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
      "| 에폭 30 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 31 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 32 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 33 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 34 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 35 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 36 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 37 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 38 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 39 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
      "| 에폭 40 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 41 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 42 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 43 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
      "| 에폭 44 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 45 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
      "| 에폭 46 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
      "| 에폭 47 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
      "| 에폭 48 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
      "| 에폭 49 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
      "| 에폭 50 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
      "| 에폭 51 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
      "| 에폭 52 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
      "| 에폭 53 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
      "| 에폭 54 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
      "| 에폭 55 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
      "| 에폭 56 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
      "| 에폭 57 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
      "| 에폭 58 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
      "| 에폭 59 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
      "| 에폭 60 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
      "| 에폭 61 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
      "| 에폭 62 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
      "| 에폭 63 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
      "| 에폭 64 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
      "| 에폭 65 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
      "| 에폭 66 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
      "| 에폭 67 |  반복 1 / 2 | 시간 0[s] | 손실 1.85\n",
      "| 에폭 68 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
      "| 에폭 69 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
      "| 에폭 70 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
      "| 에폭 71 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
      "| 에폭 72 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
      "| 에폭 73 |  반복 1 / 2 | 시간 0[s] | 손실 1.85\n",
      "| 에폭 74 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
      "| 에폭 75 |  반복 1 / 2 | 시간 0[s] | 손실 1.83\n",
      "| 에폭 76 |  반복 1 / 2 | 시간 0[s] | 손실 1.85\n",
      "| 에폭 77 |  반복 1 / 2 | 시간 0[s] | 손실 1.82\n",
      "| 에폭 78 |  반복 1 / 2 | 시간 0[s] | 손실 1.83\n",
      "| 에폭 79 |  반복 1 / 2 | 시간 0[s] | 손실 1.82\n",
      "| 에폭 80 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
      "| 에폭 81 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
      "| 에폭 82 |  반복 1 / 2 | 시간 0[s] | 손실 1.83\n",
      "| 에폭 83 |  반복 1 / 2 | 시간 0[s] | 손실 1.80\n",
      "| 에폭 84 |  반복 1 / 2 | 시간 0[s] | 손실 1.83\n",
      "| 에폭 85 |  반복 1 / 2 | 시간 0[s] | 손실 1.79\n",
      "| 에폭 86 |  반복 1 / 2 | 시간 0[s] | 손실 1.79\n",
      "| 에폭 87 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
      "| 에폭 88 |  반복 1 / 2 | 시간 0[s] | 손실 1.78\n",
      "| 에폭 89 |  반복 1 / 2 | 시간 0[s] | 손실 1.78\n",
      "| 에폭 90 |  반복 1 / 2 | 시간 0[s] | 손실 1.79\n",
      "| 에폭 91 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
      "| 에폭 92 |  반복 1 / 2 | 시간 0[s] | 손실 1.77\n",
      "| 에폭 93 |  반복 1 / 2 | 시간 0[s] | 손실 1.76\n",
      "| 에폭 94 |  반복 1 / 2 | 시간 0[s] | 손실 1.78\n",
      "| 에폭 95 |  반복 1 / 2 | 시간 0[s] | 손실 1.75\n",
      "| 에폭 96 |  반복 1 / 2 | 시간 0[s] | 손실 1.77\n",
      "| 에폭 97 |  반복 1 / 2 | 시간 0[s] | 손실 1.74\n",
      "| 에폭 98 |  반복 1 / 2 | 시간 0[s] | 손실 1.74\n",
      "| 에폭 99 |  반복 1 / 2 | 시간 0[s] | 손실 1.77\n",
      "| 에폭 100 |  반복 1 / 2 | 시간 0[s] | 손실 1.69\n",
      "| 에폭 101 |  반복 1 / 2 | 시간 0[s] | 손실 1.77\n",
      "| 에폭 102 |  반복 1 / 2 | 시간 0[s] | 손실 1.72\n",
      "| 에폭 103 |  반복 1 / 2 | 시간 0[s] | 손실 1.72\n",
      "| 에폭 104 |  반복 1 / 2 | 시간 0[s] | 손실 1.73\n",
      "| 에폭 105 |  반복 1 / 2 | 시간 0[s] | 손실 1.75\n",
      "| 에폭 106 |  반복 1 / 2 | 시간 0[s] | 손실 1.68\n",
      "| 에폭 107 |  반복 1 / 2 | 시간 0[s] | 손실 1.71\n",
      "| 에폭 108 |  반복 1 / 2 | 시간 0[s] | 손실 1.72\n",
      "| 에폭 109 |  반복 1 / 2 | 시간 0[s] | 손실 1.70\n",
      "| 에폭 110 |  반복 1 / 2 | 시간 0[s] | 손실 1.69\n",
      "| 에폭 111 |  반복 1 / 2 | 시간 0[s] | 손실 1.66\n",
      "| 에폭 112 |  반복 1 / 2 | 시간 0[s] | 손실 1.72\n",
      "| 에폭 113 |  반복 1 / 2 | 시간 0[s] | 손실 1.66\n",
      "| 에폭 114 |  반복 1 / 2 | 시간 0[s] | 손실 1.71\n",
      "| 에폭 115 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
      "| 에폭 116 |  반복 1 / 2 | 시간 0[s] | 손실 1.66\n",
      "| 에폭 117 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
      "| 에폭 118 |  반복 1 / 2 | 시간 0[s] | 손실 1.64\n",
      "| 에폭 119 |  반복 1 / 2 | 시간 0[s] | 손실 1.69\n",
      "| 에폭 120 |  반복 1 / 2 | 시간 0[s] | 손실 1.60\n",
      "| 에폭 121 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
      "| 에폭 122 |  반복 1 / 2 | 시간 0[s] | 손실 1.67\n",
      "| 에폭 123 |  반복 1 / 2 | 시간 0[s] | 손실 1.60\n",
      "| 에폭 124 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
      "| 에폭 125 |  반복 1 / 2 | 시간 0[s] | 손실 1.63\n",
      "| 에폭 126 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
      "| 에폭 127 |  반복 1 / 2 | 시간 0[s] | 손실 1.56\n",
      "| 에폭 128 |  반복 1 / 2 | 시간 0[s] | 손실 1.57\n",
      "| 에폭 129 |  반복 1 / 2 | 시간 0[s] | 손실 1.64\n",
      "| 에폭 130 |  반복 1 / 2 | 시간 0[s] | 손실 1.61\n",
      "| 에폭 131 |  반복 1 / 2 | 시간 0[s] | 손실 1.57\n",
      "| 에폭 132 |  반복 1 / 2 | 시간 0[s] | 손실 1.57\n",
      "| 에폭 133 |  반복 1 / 2 | 시간 0[s] | 손실 1.60\n",
      "| 에폭 134 |  반복 1 / 2 | 시간 0[s] | 손실 1.60\n",
      "| 에폭 135 |  반복 1 / 2 | 시간 0[s] | 손실 1.56\n",
      "| 에폭 136 |  반복 1 / 2 | 시간 0[s] | 손실 1.52\n",
      "| 에폭 137 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
      "| 에폭 138 |  반복 1 / 2 | 시간 0[s] | 손실 1.51\n",
      "| 에폭 139 |  반복 1 / 2 | 시간 0[s] | 손실 1.55\n",
      "| 에폭 140 |  반복 1 / 2 | 시간 0[s] | 손실 1.54\n",
      "| 에폭 141 |  반복 1 / 2 | 시간 0[s] | 손실 1.55\n",
      "| 에폭 142 |  반복 1 / 2 | 시간 0[s] | 손실 1.50\n",
      "| 에폭 143 |  반복 1 / 2 | 시간 0[s] | 손실 1.62\n",
      "| 에폭 144 |  반복 1 / 2 | 시간 0[s] | 손실 1.46\n",
      "| 에폭 145 |  반복 1 / 2 | 시간 0[s] | 손실 1.54\n",
      "| 에폭 146 |  반복 1 / 2 | 시간 0[s] | 손실 1.54\n",
      "| 에폭 147 |  반복 1 / 2 | 시간 0[s] | 손실 1.48\n",
      "| 에폭 148 |  반복 1 / 2 | 시간 0[s] | 손실 1.46\n",
      "| 에폭 149 |  반복 1 / 2 | 시간 0[s] | 손실 1.53\n",
      "| 에폭 150 |  반복 1 / 2 | 시간 0[s] | 손실 1.51\n",
      "| 에폭 151 |  반복 1 / 2 | 시간 0[s] | 손실 1.49\n",
      "| 에폭 152 |  반복 1 / 2 | 시간 0[s] | 손실 1.48\n",
      "| 에폭 153 |  반복 1 / 2 | 시간 0[s] | 손실 1.48\n",
      "| 에폭 154 |  반복 1 / 2 | 시간 0[s] | 손실 1.49\n",
      "| 에폭 155 |  반복 1 / 2 | 시간 0[s] | 손실 1.41\n",
      "| 에폭 156 |  반복 1 / 2 | 시간 0[s] | 손실 1.49\n",
      "| 에폭 157 |  반복 1 / 2 | 시간 0[s] | 손실 1.41\n",
      "| 에폭 158 |  반복 1 / 2 | 시간 0[s] | 손실 1.51\n",
      "| 에폭 159 |  반복 1 / 2 | 시간 0[s] | 손실 1.50\n",
      "| 에폭 160 |  반복 1 / 2 | 시간 0[s] | 손실 1.41\n",
      "| 에폭 161 |  반복 1 / 2 | 시간 0[s] | 손실 1.42\n",
      "| 에폭 162 |  반복 1 / 2 | 시간 0[s] | 손실 1.40\n",
      "| 에폭 163 |  반복 1 / 2 | 시간 0[s] | 손실 1.44\n",
      "| 에폭 164 |  반복 1 / 2 | 시간 0[s] | 손실 1.51\n",
      "| 에폭 165 |  반복 1 / 2 | 시간 0[s] | 손실 1.37\n",
      "| 에폭 166 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
      "| 에폭 167 |  반복 1 / 2 | 시간 0[s] | 손실 1.45\n",
      "| 에폭 168 |  반복 1 / 2 | 시간 0[s] | 손실 1.37\n",
      "| 에폭 169 |  반복 1 / 2 | 시간 0[s] | 손실 1.44\n",
      "| 에폭 170 |  반복 1 / 2 | 시간 0[s] | 손실 1.39\n",
      "| 에폭 171 |  반복 1 / 2 | 시간 0[s] | 손실 1.44\n",
      "| 에폭 172 |  반복 1 / 2 | 시간 0[s] | 손실 1.38\n",
      "| 에폭 173 |  반복 1 / 2 | 시간 0[s] | 손실 1.38\n",
      "| 에폭 174 |  반복 1 / 2 | 시간 0[s] | 손실 1.39\n",
      "| 에폭 175 |  반복 1 / 2 | 시간 0[s] | 손실 1.31\n",
      "| 에폭 176 |  반복 1 / 2 | 시간 0[s] | 손실 1.32\n",
      "| 에폭 177 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
      "| 에폭 178 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
      "| 에폭 179 |  반복 1 / 2 | 시간 0[s] | 손실 1.41\n",
      "| 에폭 180 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
      "| 에폭 181 |  반복 1 / 2 | 시간 0[s] | 손실 1.32\n",
      "| 에폭 182 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
      "| 에폭 183 |  반복 1 / 2 | 시간 0[s] | 손실 1.31\n",
      "| 에폭 184 |  반복 1 / 2 | 시간 0[s] | 손실 1.38\n",
      "| 에폭 185 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
      "| 에폭 186 |  반복 1 / 2 | 시간 0[s] | 손실 1.31\n",
      "| 에폭 187 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
      "| 에폭 188 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
      "| 에폭 189 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
      "| 에폭 190 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
      "| 에폭 191 |  반복 1 / 2 | 시간 0[s] | 손실 1.32\n",
      "| 에폭 192 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
      "| 에폭 193 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
      "| 에폭 194 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
      "| 에폭 195 |  반복 1 / 2 | 시간 0[s] | 손실 1.26\n",
      "| 에폭 196 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
      "| 에폭 197 |  반복 1 / 2 | 시간 0[s] | 손실 1.21\n",
      "| 에폭 198 |  반복 1 / 2 | 시간 0[s] | 손실 1.25\n",
      "| 에폭 199 |  반복 1 / 2 | 시간 0[s] | 손실 1.34\n",
      "| 에폭 200 |  반복 1 / 2 | 시간 0[s] | 손실 1.30\n",
      "| 에폭 201 |  반복 1 / 2 | 시간 0[s] | 손실 1.20\n",
      "| 에폭 202 |  반복 1 / 2 | 시간 0[s] | 손실 1.19\n",
      "| 에폭 203 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
      "| 에폭 204 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
      "| 에폭 205 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
      "| 에폭 206 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
      "| 에폭 207 |  반복 1 / 2 | 시간 0[s] | 손실 1.25\n",
      "| 에폭 208 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
      "| 에폭 209 |  반복 1 / 2 | 시간 0[s] | 손실 1.28\n",
      "| 에폭 210 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
      "| 에폭 211 |  반복 1 / 2 | 시간 0[s] | 손실 1.34\n",
      "| 에폭 212 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
      "| 에폭 213 |  반복 1 / 2 | 시간 0[s] | 손실 1.28\n",
      "| 에폭 214 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
      "| 에폭 215 |  반복 1 / 2 | 시간 0[s] | 손실 1.19\n",
      "| 에폭 216 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
      "| 에폭 217 |  반복 1 / 2 | 시간 0[s] | 손실 1.26\n",
      "| 에폭 218 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 219 |  반복 1 / 2 | 시간 0[s] | 손실 1.26\n",
      "| 에폭 220 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
      "| 에폭 221 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
      "| 에폭 222 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 223 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
      "| 에폭 224 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
      "| 에폭 225 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
      "| 에폭 226 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
      "| 에폭 227 |  반복 1 / 2 | 시간 0[s] | 손실 1.21\n",
      "| 에폭 228 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
      "| 에폭 229 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
      "| 에폭 230 |  반복 1 / 2 | 시간 0[s] | 손실 1.21\n",
      "| 에폭 231 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
      "| 에폭 232 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
      "| 에폭 233 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
      "| 에폭 234 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 235 |  반복 1 / 2 | 시간 0[s] | 손실 1.19\n",
      "| 에폭 236 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
      "| 에폭 237 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 238 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
      "| 에폭 239 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 240 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 241 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
      "| 에폭 242 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
      "| 에폭 243 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
      "| 에폭 244 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
      "| 에폭 245 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 246 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
      "| 에폭 247 |  반복 1 / 2 | 시간 0[s] | 손실 1.07\n",
      "| 에폭 248 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
      "| 에폭 249 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
      "| 에폭 250 |  반복 1 / 2 | 시간 0[s] | 손실 1.07\n",
      "| 에폭 251 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 252 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
      "| 에폭 253 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
      "| 에폭 254 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
      "| 에폭 255 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
      "| 에폭 256 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
      "| 에폭 257 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 258 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
      "| 에폭 259 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 260 |  반복 1 / 2 | 시간 0[s] | 손실 1.19\n",
      "| 에폭 261 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
      "| 에폭 262 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
      "| 에폭 263 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 264 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
      "| 에폭 265 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
      "| 에폭 266 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 267 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
      "| 에폭 268 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
      "| 에폭 269 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
      "| 에폭 270 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
      "| 에폭 271 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
      "| 에폭 272 |  반복 1 / 2 | 시간 1[s] | 손실 0.99\n",
      "| 에폭 273 |  반복 1 / 2 | 시간 1[s] | 손실 1.00\n",
      "| 에폭 274 |  반복 1 / 2 | 시간 1[s] | 손실 1.07\n",
      "| 에폭 275 |  반복 1 / 2 | 시간 1[s] | 손실 0.91\n",
      "| 에폭 276 |  반복 1 / 2 | 시간 1[s] | 손실 0.98\n",
      "| 에폭 277 |  반복 1 / 2 | 시간 1[s] | 손실 0.99\n",
      "| 에폭 278 |  반복 1 / 2 | 시간 1[s] | 손실 0.98\n",
      "| 에폭 279 |  반복 1 / 2 | 시간 1[s] | 손실 1.06\n",
      "| 에폭 280 |  반복 1 / 2 | 시간 1[s] | 손실 0.81\n",
      "| 에폭 281 |  반복 1 / 2 | 시간 1[s] | 손실 1.06\n",
      "| 에폭 282 |  반복 1 / 2 | 시간 1[s] | 손실 1.05\n",
      "| 에폭 283 |  반복 1 / 2 | 시간 1[s] | 손실 0.96\n",
      "| 에폭 284 |  반복 1 / 2 | 시간 1[s] | 손실 0.97\n",
      "| 에폭 285 |  반복 1 / 2 | 시간 1[s] | 손실 0.78\n",
      "| 에폭 286 |  반복 1 / 2 | 시간 1[s] | 손실 1.05\n",
      "| 에폭 287 |  반복 1 / 2 | 시간 1[s] | 손실 0.87\n",
      "| 에폭 288 |  반복 1 / 2 | 시간 1[s] | 손실 1.13\n",
      "| 에폭 289 |  반복 1 / 2 | 시간 1[s] | 손실 0.94\n",
      "| 에폭 290 |  반복 1 / 2 | 시간 1[s] | 손실 0.87\n",
      "| 에폭 291 |  반복 1 / 2 | 시간 1[s] | 손실 0.95\n",
      "| 에폭 292 |  반복 1 / 2 | 시간 1[s] | 손실 0.93\n",
      "| 에폭 293 |  반복 1 / 2 | 시간 1[s] | 손실 0.86\n",
      "| 에폭 294 |  반복 1 / 2 | 시간 1[s] | 손실 1.03\n",
      "| 에폭 295 |  반복 1 / 2 | 시간 1[s] | 손실 0.93\n",
      "| 에폭 296 |  반복 1 / 2 | 시간 1[s] | 손실 1.01\n",
      "| 에폭 297 |  반복 1 / 2 | 시간 1[s] | 손실 0.75\n",
      "| 에폭 298 |  반복 1 / 2 | 시간 1[s] | 손실 1.10\n",
      "| 에폭 299 |  반복 1 / 2 | 시간 1[s] | 손실 0.83\n",
      "| 에폭 300 |  반복 1 / 2 | 시간 1[s] | 손실 0.85\n",
      "| 에폭 301 |  반복 1 / 2 | 시간 1[s] | 손실 1.01\n",
      "| 에폭 302 |  반복 1 / 2 | 시간 1[s] | 손실 0.99\n",
      "| 에폭 303 |  반복 1 / 2 | 시간 1[s] | 손실 0.84\n",
      "| 에폭 304 |  반복 1 / 2 | 시간 1[s] | 손실 0.99\n",
      "| 에폭 305 |  반복 1 / 2 | 시간 1[s] | 손실 0.75\n",
      "| 에폭 306 |  반복 1 / 2 | 시간 1[s] | 손실 0.98\n",
      "| 에폭 307 |  반복 1 / 2 | 시간 1[s] | 손실 0.92\n",
      "| 에폭 308 |  반복 1 / 2 | 시간 1[s] | 손실 0.81\n",
      "| 에폭 309 |  반복 1 / 2 | 시간 1[s] | 손실 0.99\n",
      "| 에폭 310 |  반복 1 / 2 | 시간 1[s] | 손실 0.97\n",
      "| 에폭 311 |  반복 1 / 2 | 시간 1[s] | 손실 0.73\n",
      "| 에폭 312 |  반복 1 / 2 | 시간 1[s] | 손실 0.96\n",
      "| 에폭 313 |  반복 1 / 2 | 시간 1[s] | 손실 0.89\n",
      "| 에폭 314 |  반복 1 / 2 | 시간 1[s] | 손실 0.81\n",
      "| 에폭 315 |  반복 1 / 2 | 시간 1[s] | 손실 0.97\n",
      "| 에폭 316 |  반복 1 / 2 | 시간 1[s] | 손실 0.78\n",
      "| 에폭 317 |  반복 1 / 2 | 시간 1[s] | 손실 0.99\n",
      "| 에폭 318 |  반복 1 / 2 | 시간 1[s] | 손실 0.79\n",
      "| 에폭 319 |  반복 1 / 2 | 시간 1[s] | 손실 0.85\n",
      "| 에폭 320 |  반복 1 / 2 | 시간 1[s] | 손실 0.97\n",
      "| 에폭 321 |  반복 1 / 2 | 시간 1[s] | 손실 0.87\n",
      "| 에폭 322 |  반복 1 / 2 | 시간 1[s] | 손실 0.96\n",
      "| 에폭 323 |  반복 1 / 2 | 시간 1[s] | 손실 0.77\n",
      "| 에폭 324 |  반복 1 / 2 | 시간 1[s] | 손실 0.80\n",
      "| 에폭 325 |  반복 1 / 2 | 시간 1[s] | 손실 0.95\n",
      "| 에폭 326 |  반복 1 / 2 | 시간 1[s] | 손실 0.86\n",
      "| 에폭 327 |  반복 1 / 2 | 시간 1[s] | 손실 0.86\n",
      "| 에폭 328 |  반복 1 / 2 | 시간 1[s] | 손실 0.84\n",
      "| 에폭 329 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
      "| 에폭 330 |  반복 1 / 2 | 시간 1[s] | 손실 0.94\n",
      "| 에폭 331 |  반복 1 / 2 | 시간 1[s] | 손실 0.85\n",
      "| 에폭 332 |  반복 1 / 2 | 시간 1[s] | 손실 0.87\n",
      "| 에폭 333 |  반복 1 / 2 | 시간 1[s] | 손실 0.85\n",
      "| 에폭 334 |  반복 1 / 2 | 시간 1[s] | 손실 0.85\n",
      "| 에폭 335 |  반복 1 / 2 | 시간 1[s] | 손실 0.84\n",
      "| 에폭 336 |  반복 1 / 2 | 시간 1[s] | 손실 0.84\n",
      "| 에폭 337 |  반복 1 / 2 | 시간 1[s] | 손실 0.93\n",
      "| 에폭 338 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
      "| 에폭 339 |  반복 1 / 2 | 시간 1[s] | 손실 0.81\n",
      "| 에폭 340 |  반복 1 / 2 | 시간 1[s] | 손실 0.92\n",
      "| 에폭 341 |  반복 1 / 2 | 시간 1[s] | 손실 0.86\n",
      "| 에폭 342 |  반복 1 / 2 | 시간 1[s] | 손실 0.82\n",
      "| 에폭 343 |  반복 1 / 2 | 시간 1[s] | 손실 0.80\n",
      "| 에폭 344 |  반복 1 / 2 | 시간 1[s] | 손실 0.82\n",
      "| 에폭 345 |  반복 1 / 2 | 시간 1[s] | 손실 0.82\n",
      "| 에폭 346 |  반복 1 / 2 | 시간 1[s] | 손실 0.82\n",
      "| 에폭 347 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
      "| 에폭 348 |  반복 1 / 2 | 시간 1[s] | 손실 0.97\n",
      "| 에폭 349 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
      "| 에폭 350 |  반복 1 / 2 | 시간 1[s] | 손실 0.90\n",
      "| 에폭 351 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 352 |  반복 1 / 2 | 시간 1[s] | 손실 0.84\n",
      "| 에폭 353 |  반복 1 / 2 | 시간 1[s] | 손실 0.77\n",
      "| 에폭 354 |  반복 1 / 2 | 시간 1[s] | 손실 0.80\n",
      "| 에폭 355 |  반복 1 / 2 | 시간 1[s] | 손실 0.80\n",
      "| 에폭 356 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
      "| 에폭 357 |  반복 1 / 2 | 시간 1[s] | 손실 0.89\n",
      "| 에폭 358 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
      "| 에폭 359 |  반복 1 / 2 | 시간 1[s] | 손실 0.79\n",
      "| 에폭 360 |  반복 1 / 2 | 시간 1[s] | 손실 0.79\n",
      "| 에폭 361 |  반복 1 / 2 | 시간 1[s] | 손실 0.79\n",
      "| 에폭 362 |  반복 1 / 2 | 시간 1[s] | 손실 0.88\n",
      "| 에폭 363 |  반복 1 / 2 | 시간 1[s] | 손실 0.82\n",
      "| 에폭 364 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
      "| 에폭 365 |  반복 1 / 2 | 시간 1[s] | 손실 0.78\n",
      "| 에폭 366 |  반복 1 / 2 | 시간 1[s] | 손실 0.78\n",
      "| 에폭 367 |  반복 1 / 2 | 시간 1[s] | 손실 0.78\n",
      "| 에폭 368 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
      "| 에폭 369 |  반복 1 / 2 | 시간 1[s] | 손실 0.90\n",
      "| 에폭 370 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
      "| 에폭 371 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
      "| 에폭 372 |  반복 1 / 2 | 시간 1[s] | 손실 0.82\n",
      "| 에폭 373 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
      "| 에폭 374 |  반복 1 / 2 | 시간 1[s] | 손실 0.90\n",
      "| 에폭 375 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
      "| 에폭 376 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 377 |  반복 1 / 2 | 시간 1[s] | 손실 0.81\n",
      "| 에폭 378 |  반복 1 / 2 | 시간 1[s] | 손실 0.80\n",
      "| 에폭 379 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 380 |  반복 1 / 2 | 시간 1[s] | 손실 0.89\n",
      "| 에폭 381 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 382 |  반복 1 / 2 | 시간 1[s] | 손실 0.80\n",
      "| 에폭 383 |  반복 1 / 2 | 시간 1[s] | 손실 0.79\n",
      "| 에폭 384 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 385 |  반복 1 / 2 | 시간 1[s] | 손실 0.84\n",
      "| 에폭 386 |  반복 1 / 2 | 시간 1[s] | 손실 0.79\n",
      "| 에폭 387 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
      "| 에폭 388 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 389 |  반복 1 / 2 | 시간 1[s] | 손실 0.87\n",
      "| 에폭 390 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
      "| 에폭 391 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
      "| 에폭 392 |  반복 1 / 2 | 시간 1[s] | 손실 0.92\n",
      "| 에폭 393 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
      "| 에폭 394 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 395 |  반복 1 / 2 | 시간 1[s] | 손실 0.86\n",
      "| 에폭 396 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 397 |  반복 1 / 2 | 시간 1[s] | 손실 0.86\n",
      "| 에폭 398 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
      "| 에폭 399 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 400 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
      "| 에폭 401 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
      "| 에폭 402 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
      "| 에폭 403 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 404 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 405 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
      "| 에폭 406 |  반복 1 / 2 | 시간 1[s] | 손실 0.81\n",
      "| 에폭 407 |  반복 1 / 2 | 시간 1[s] | 손실 0.80\n",
      "| 에폭 408 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 409 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 410 |  반복 1 / 2 | 시간 1[s] | 손실 0.84\n",
      "| 에폭 411 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 412 |  반복 1 / 2 | 시간 1[s] | 손실 0.79\n",
      "| 에폭 413 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 414 |  반복 1 / 2 | 시간 1[s] | 손실 0.80\n",
      "| 에폭 415 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 416 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
      "| 에폭 417 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 418 |  반복 1 / 2 | 시간 1[s] | 손실 0.78\n",
      "| 에폭 419 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 420 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
      "| 에폭 421 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
      "| 에폭 422 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 423 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
      "| 에폭 424 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
      "| 에폭 425 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 426 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
      "| 에폭 427 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 428 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
      "| 에폭 429 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 430 |  반복 1 / 2 | 시간 1[s] | 손실 0.73\n",
      "| 에폭 431 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 432 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
      "| 에폭 433 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 434 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 435 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 436 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
      "| 에폭 437 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 438 |  반복 1 / 2 | 시간 1[s] | 손실 0.73\n",
      "| 에폭 439 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 440 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 441 |  반복 1 / 2 | 시간 1[s] | 손실 0.75\n",
      "| 에폭 442 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
      "| 에폭 443 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
      "| 에폭 444 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 445 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 446 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 447 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 448 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 449 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 450 |  반복 1 / 2 | 시간 1[s] | 손실 0.80\n",
      "| 에폭 451 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
      "| 에폭 452 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
      "| 에폭 453 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 454 |  반복 1 / 2 | 시간 1[s] | 손실 0.73\n",
      "| 에폭 455 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
      "| 에폭 456 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 457 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 458 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 459 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 460 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 461 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 462 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
      "| 에폭 463 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
      "| 에폭 464 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 465 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 466 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 467 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 468 |  반복 1 / 2 | 시간 1[s] | 손실 0.81\n",
      "| 에폭 469 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 470 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 471 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 472 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
      "| 에폭 473 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
      "| 에폭 474 |  반복 1 / 2 | 시간 1[s] | 손실 0.80\n",
      "| 에폭 475 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 476 |  반복 1 / 2 | 시간 1[s] | 손실 0.43\n",
      "| 에폭 477 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
      "| 에폭 478 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
      "| 에폭 479 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 480 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
      "| 에폭 481 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 482 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
      "| 에폭 483 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 484 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 485 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
      "| 에폭 486 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 487 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 488 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 489 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 490 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 491 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 492 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 493 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
      "| 에폭 494 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 495 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 496 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 497 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 498 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 499 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 500 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 501 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
      "| 에폭 502 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
      "| 에폭 503 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
      "| 에폭 504 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 505 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 506 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 507 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 508 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 509 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 510 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 511 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 512 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 513 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 514 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 515 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 516 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
      "| 에폭 517 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
      "| 에폭 518 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
      "| 에폭 519 |  반복 1 / 2 | 시간 2[s] | 손실 0.46\n",
      "| 에폭 520 |  반복 1 / 2 | 시간 2[s] | 손실 0.60\n",
      "| 에폭 521 |  반복 1 / 2 | 시간 2[s] | 손실 0.55\n",
      "| 에폭 522 |  반복 1 / 2 | 시간 2[s] | 손실 0.60\n",
      "| 에폭 523 |  반복 1 / 2 | 시간 2[s] | 손실 0.48\n",
      "| 에폭 524 |  반복 1 / 2 | 시간 2[s] | 손실 0.55\n",
      "| 에폭 525 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 526 |  반복 1 / 2 | 시간 2[s] | 손실 0.56\n",
      "| 에폭 527 |  반복 1 / 2 | 시간 2[s] | 손실 0.66\n",
      "| 에폭 528 |  반복 1 / 2 | 시간 2[s] | 손실 0.54\n",
      "| 에폭 529 |  반복 1 / 2 | 시간 2[s] | 손실 0.46\n",
      "| 에폭 530 |  반복 1 / 2 | 시간 2[s] | 손실 0.69\n",
      "| 에폭 531 |  반복 1 / 2 | 시간 2[s] | 손실 0.35\n",
      "| 에폭 532 |  반복 1 / 2 | 시간 2[s] | 손실 0.57\n",
      "| 에폭 533 |  반복 1 / 2 | 시간 2[s] | 손실 0.61\n",
      "| 에폭 534 |  반복 1 / 2 | 시간 2[s] | 손실 0.53\n",
      "| 에폭 535 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 536 |  반복 1 / 2 | 시간 2[s] | 손실 0.57\n",
      "| 에폭 537 |  반복 1 / 2 | 시간 2[s] | 손실 0.53\n",
      "| 에폭 538 |  반복 1 / 2 | 시간 2[s] | 손실 0.58\n",
      "| 에폭 539 |  반복 1 / 2 | 시간 2[s] | 손실 0.57\n",
      "| 에폭 540 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 541 |  반복 1 / 2 | 시간 2[s] | 손실 0.52\n",
      "| 에폭 542 |  반복 1 / 2 | 시간 2[s] | 손실 0.54\n",
      "| 에폭 543 |  반복 1 / 2 | 시간 2[s] | 손실 0.53\n",
      "| 에폭 544 |  반복 1 / 2 | 시간 2[s] | 손실 0.53\n",
      "| 에폭 545 |  반복 1 / 2 | 시간 2[s] | 손실 0.53\n",
      "| 에폭 546 |  반복 1 / 2 | 시간 2[s] | 손실 0.52\n",
      "| 에폭 547 |  반복 1 / 2 | 시간 2[s] | 손실 0.48\n",
      "| 에폭 548 |  반복 1 / 2 | 시간 2[s] | 손실 0.56\n",
      "| 에폭 549 |  반복 1 / 2 | 시간 2[s] | 손실 0.68\n",
      "| 에폭 550 |  반복 1 / 2 | 시간 2[s] | 손실 0.43\n",
      "| 에폭 551 |  반복 1 / 2 | 시간 2[s] | 손실 0.52\n",
      "| 에폭 552 |  반복 1 / 2 | 시간 2[s] | 손실 0.52\n",
      "| 에폭 553 |  반복 1 / 2 | 시간 2[s] | 손실 0.41\n",
      "| 에폭 554 |  반복 1 / 2 | 시간 2[s] | 손실 0.47\n",
      "| 에폭 555 |  반복 1 / 2 | 시간 2[s] | 손실 0.67\n",
      "| 에폭 556 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 557 |  반복 1 / 2 | 시간 2[s] | 손실 0.52\n",
      "| 에폭 558 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 559 |  반복 1 / 2 | 시간 2[s] | 손실 0.55\n",
      "| 에폭 560 |  반복 1 / 2 | 시간 2[s] | 손실 0.42\n",
      "| 에폭 561 |  반복 1 / 2 | 시간 2[s] | 손실 0.50\n",
      "| 에폭 562 |  반복 1 / 2 | 시간 2[s] | 손실 0.61\n",
      "| 에폭 563 |  반복 1 / 2 | 시간 2[s] | 손실 0.47\n",
      "| 에폭 564 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 565 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 566 |  반복 1 / 2 | 시간 2[s] | 손실 0.41\n",
      "| 에폭 567 |  반복 1 / 2 | 시간 2[s] | 손실 0.70\n",
      "| 에폭 568 |  반복 1 / 2 | 시간 2[s] | 손실 0.50\n",
      "| 에폭 569 |  반복 1 / 2 | 시간 2[s] | 손실 0.41\n",
      "| 에폭 570 |  반복 1 / 2 | 시간 2[s] | 손실 0.40\n",
      "| 에폭 571 |  반복 1 / 2 | 시간 2[s] | 손실 0.65\n",
      "| 에폭 572 |  반복 1 / 2 | 시간 2[s] | 손실 0.41\n",
      "| 에폭 573 |  반복 1 / 2 | 시간 2[s] | 손실 0.59\n",
      "| 에폭 574 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 575 |  반복 1 / 2 | 시간 2[s] | 손실 0.55\n",
      "| 에폭 576 |  반복 1 / 2 | 시간 2[s] | 손실 0.50\n",
      "| 에폭 577 |  반복 1 / 2 | 시간 2[s] | 손실 0.49\n",
      "| 에폭 578 |  반복 1 / 2 | 시간 2[s] | 손실 0.40\n",
      "| 에폭 579 |  반복 1 / 2 | 시간 2[s] | 손실 0.54\n",
      "| 에폭 580 |  반복 1 / 2 | 시간 2[s] | 손실 0.39\n",
      "| 에폭 581 |  반복 1 / 2 | 시간 2[s] | 손실 0.59\n",
      "| 에폭 582 |  반복 1 / 2 | 시간 2[s] | 손실 0.49\n",
      "| 에폭 583 |  반복 1 / 2 | 시간 2[s] | 손실 0.49\n",
      "| 에폭 584 |  반복 1 / 2 | 시간 2[s] | 손실 0.59\n",
      "| 에폭 585 |  반복 1 / 2 | 시간 2[s] | 손실 0.34\n",
      "| 에폭 586 |  반복 1 / 2 | 시간 2[s] | 손실 0.49\n",
      "| 에폭 587 |  반복 1 / 2 | 시간 2[s] | 손실 0.58\n",
      "| 에폭 588 |  반복 1 / 2 | 시간 2[s] | 손실 0.53\n",
      "| 에폭 589 |  반복 1 / 2 | 시간 2[s] | 손실 0.34\n",
      "| 에폭 590 |  반복 1 / 2 | 시간 2[s] | 손실 0.58\n",
      "| 에폭 591 |  반복 1 / 2 | 시간 2[s] | 손실 0.53\n",
      "| 에폭 592 |  반복 1 / 2 | 시간 2[s] | 손실 0.48\n",
      "| 에폭 593 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 594 |  반복 1 / 2 | 시간 2[s] | 손실 0.58\n",
      "| 에폭 595 |  반복 1 / 2 | 시간 2[s] | 손실 0.43\n",
      "| 에폭 596 |  반복 1 / 2 | 시간 2[s] | 손실 0.43\n",
      "| 에폭 597 |  반복 1 / 2 | 시간 2[s] | 손실 0.43\n",
      "| 에폭 598 |  반복 1 / 2 | 시간 2[s] | 손실 0.57\n",
      "| 에폭 599 |  반복 1 / 2 | 시간 2[s] | 손실 0.48\n",
      "| 에폭 600 |  반복 1 / 2 | 시간 2[s] | 손실 0.57\n",
      "| 에폭 601 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 602 |  반복 1 / 2 | 시간 2[s] | 손실 0.52\n",
      "| 에폭 603 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 604 |  반복 1 / 2 | 시간 2[s] | 손실 0.42\n",
      "| 에폭 605 |  반복 1 / 2 | 시간 2[s] | 손실 0.67\n",
      "| 에폭 606 |  반복 1 / 2 | 시간 2[s] | 손실 0.27\n",
      "| 에폭 607 |  반복 1 / 2 | 시간 2[s] | 손실 0.62\n",
      "| 에폭 608 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 609 |  반복 1 / 2 | 시간 2[s] | 손실 0.47\n",
      "| 에폭 610 |  반복 1 / 2 | 시간 2[s] | 손실 0.61\n",
      "| 에폭 611 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 612 |  반복 1 / 2 | 시간 2[s] | 손실 0.47\n",
      "| 에폭 613 |  반복 1 / 2 | 시간 2[s] | 손실 0.42\n",
      "| 에폭 614 |  반복 1 / 2 | 시간 2[s] | 손실 0.56\n",
      "| 에폭 615 |  반복 1 / 2 | 시간 2[s] | 손실 0.41\n",
      "| 에폭 616 |  반복 1 / 2 | 시간 2[s] | 손실 0.42\n",
      "| 에폭 617 |  반복 1 / 2 | 시간 2[s] | 손실 0.46\n",
      "| 에폭 618 |  반복 1 / 2 | 시간 2[s] | 손실 0.36\n",
      "| 에폭 619 |  반복 1 / 2 | 시간 2[s] | 손실 0.50\n",
      "| 에폭 620 |  반복 1 / 2 | 시간 2[s] | 손실 0.56\n",
      "| 에폭 621 |  반복 1 / 2 | 시간 2[s] | 손실 0.41\n",
      "| 에폭 622 |  반복 1 / 2 | 시간 2[s] | 손실 0.50\n",
      "| 에폭 623 |  반복 1 / 2 | 시간 2[s] | 손실 0.36\n",
      "| 에폭 624 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 625 |  반복 1 / 2 | 시간 2[s] | 손실 0.40\n",
      "| 에폭 626 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 627 |  반복 1 / 2 | 시간 2[s] | 손실 0.55\n",
      "| 에폭 628 |  반복 1 / 2 | 시간 2[s] | 손실 0.31\n",
      "| 에폭 629 |  반복 1 / 2 | 시간 2[s] | 손실 0.55\n",
      "| 에폭 630 |  반복 1 / 2 | 시간 2[s] | 손실 0.40\n",
      "| 에폭 631 |  반복 1 / 2 | 시간 2[s] | 손실 0.55\n",
      "| 에폭 632 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 633 |  반복 1 / 2 | 시간 2[s] | 손실 0.35\n",
      "| 에폭 634 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 635 |  반복 1 / 2 | 시간 2[s] | 손실 0.59\n",
      "| 에폭 636 |  반복 1 / 2 | 시간 2[s] | 손실 0.41\n",
      "| 에폭 637 |  반복 1 / 2 | 시간 2[s] | 손실 0.29\n",
      "| 에폭 638 |  반복 1 / 2 | 시간 2[s] | 손실 0.50\n",
      "| 에폭 639 |  반복 1 / 2 | 시간 2[s] | 손실 0.55\n",
      "| 에폭 640 |  반복 1 / 2 | 시간 2[s] | 손실 0.29\n",
      "| 에폭 641 |  반복 1 / 2 | 시간 2[s] | 손실 0.54\n",
      "| 에폭 642 |  반복 1 / 2 | 시간 2[s] | 손실 0.34\n",
      "| 에폭 643 |  반복 1 / 2 | 시간 2[s] | 손실 0.60\n",
      "| 에폭 644 |  반복 1 / 2 | 시간 2[s] | 손실 0.24\n",
      "| 에폭 645 |  반복 1 / 2 | 시간 2[s] | 손실 0.54\n",
      "| 에폭 646 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 647 |  반복 1 / 2 | 시간 2[s] | 손실 0.64\n",
      "| 에폭 648 |  반복 1 / 2 | 시간 2[s] | 손실 0.24\n",
      "| 에폭 649 |  반복 1 / 2 | 시간 2[s] | 손실 0.50\n",
      "| 에폭 650 |  반복 1 / 2 | 시간 2[s] | 손실 0.48\n",
      "| 에폭 651 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 652 |  반복 1 / 2 | 시간 2[s] | 손실 0.50\n",
      "| 에폭 653 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 654 |  반복 1 / 2 | 시간 2[s] | 손실 0.40\n",
      "| 에폭 655 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 656 |  반복 1 / 2 | 시간 2[s] | 손실 0.53\n",
      "| 에폭 657 |  반복 1 / 2 | 시간 2[s] | 손실 0.33\n",
      "| 에폭 658 |  반복 1 / 2 | 시간 2[s] | 손실 0.49\n",
      "| 에폭 659 |  반복 1 / 2 | 시간 2[s] | 손실 0.47\n",
      "| 에폭 660 |  반복 1 / 2 | 시간 2[s] | 손실 0.53\n",
      "| 에폭 661 |  반복 1 / 2 | 시간 2[s] | 손실 0.33\n",
      "| 에폭 662 |  반복 1 / 2 | 시간 2[s] | 손실 0.39\n",
      "| 에폭 663 |  반복 1 / 2 | 시간 2[s] | 손실 0.43\n",
      "| 에폭 664 |  반복 1 / 2 | 시간 2[s] | 손실 0.53\n",
      "| 에폭 665 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 666 |  반복 1 / 2 | 시간 2[s] | 손실 0.43\n",
      "| 에폭 667 |  반복 1 / 2 | 시간 2[s] | 손실 0.49\n",
      "| 에폭 668 |  반복 1 / 2 | 시간 2[s] | 손실 0.33\n",
      "| 에폭 669 |  반복 1 / 2 | 시간 2[s] | 손실 0.36\n",
      "| 에폭 670 |  반복 1 / 2 | 시간 2[s] | 손실 0.49\n",
      "| 에폭 671 |  반복 1 / 2 | 시간 2[s] | 손실 0.42\n",
      "| 에폭 672 |  반복 1 / 2 | 시간 2[s] | 손실 0.56\n",
      "| 에폭 673 |  반복 1 / 2 | 시간 2[s] | 손실 0.29\n",
      "| 에폭 674 |  반복 1 / 2 | 시간 2[s] | 손실 0.52\n",
      "| 에폭 675 |  반복 1 / 2 | 시간 2[s] | 손실 0.42\n",
      "| 에폭 676 |  반복 1 / 2 | 시간 2[s] | 손실 0.32\n",
      "| 에폭 677 |  반복 1 / 2 | 시간 2[s] | 손실 0.52\n",
      "| 에폭 678 |  반복 1 / 2 | 시간 2[s] | 손실 0.36\n",
      "| 에폭 679 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 680 |  반복 1 / 2 | 시간 2[s] | 손실 0.31\n",
      "| 에폭 681 |  반복 1 / 2 | 시간 2[s] | 손실 0.55\n",
      "| 에폭 682 |  반복 1 / 2 | 시간 2[s] | 손실 0.48\n",
      "| 에폭 683 |  반복 1 / 2 | 시간 2[s] | 손실 0.32\n",
      "| 에폭 684 |  반복 1 / 2 | 시간 2[s] | 손실 0.42\n",
      "| 에폭 685 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 686 |  반복 1 / 2 | 시간 2[s] | 손실 0.41\n",
      "| 에폭 687 |  반복 1 / 2 | 시간 2[s] | 손실 0.41\n",
      "| 에폭 688 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 689 |  반복 1 / 2 | 시간 2[s] | 손실 0.52\n",
      "| 에폭 690 |  반복 1 / 2 | 시간 2[s] | 손실 0.31\n",
      "| 에폭 691 |  반복 1 / 2 | 시간 2[s] | 손실 0.41\n",
      "| 에폭 692 |  반복 1 / 2 | 시간 2[s] | 손실 0.31\n",
      "| 에폭 693 |  반복 1 / 2 | 시간 2[s] | 손실 0.55\n",
      "| 에폭 694 |  반복 1 / 2 | 시간 2[s] | 손실 0.41\n",
      "| 에폭 695 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 696 |  반복 1 / 2 | 시간 2[s] | 손실 0.34\n",
      "| 에폭 697 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 698 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 699 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 700 |  반복 1 / 2 | 시간 2[s] | 손실 0.31\n",
      "| 에폭 701 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 702 |  반복 1 / 2 | 시간 2[s] | 손실 0.30\n",
      "| 에폭 703 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 704 |  반복 1 / 2 | 시간 2[s] | 손실 0.47\n",
      "| 에폭 705 |  반복 1 / 2 | 시간 2[s] | 손실 0.33\n",
      "| 에폭 706 |  반복 1 / 2 | 시간 2[s] | 손실 0.37\n",
      "| 에폭 707 |  반복 1 / 2 | 시간 2[s] | 손실 0.50\n",
      "| 에폭 708 |  반복 1 / 2 | 시간 2[s] | 손실 0.30\n",
      "| 에폭 709 |  반복 1 / 2 | 시간 2[s] | 손실 0.40\n",
      "| 에폭 710 |  반복 1 / 2 | 시간 2[s] | 손실 0.40\n",
      "| 에폭 711 |  반복 1 / 2 | 시간 2[s] | 손실 0.40\n",
      "| 에폭 712 |  반복 1 / 2 | 시간 2[s] | 손실 0.43\n",
      "| 에폭 713 |  반복 1 / 2 | 시간 2[s] | 손실 0.47\n",
      "| 에폭 714 |  반복 1 / 2 | 시간 2[s] | 손실 0.40\n",
      "| 에폭 715 |  반복 1 / 2 | 시간 2[s] | 손실 0.43\n",
      "| 에폭 716 |  반복 1 / 2 | 시간 2[s] | 손실 0.29\n",
      "| 에폭 717 |  반복 1 / 2 | 시간 2[s] | 손실 0.40\n",
      "| 에폭 718 |  반복 1 / 2 | 시간 2[s] | 손실 0.36\n",
      "| 에폭 719 |  반복 1 / 2 | 시간 2[s] | 손실 0.50\n",
      "| 에폭 720 |  반복 1 / 2 | 시간 2[s] | 손실 0.32\n",
      "| 에폭 721 |  반복 1 / 2 | 시간 2[s] | 손실 0.39\n",
      "| 에폭 722 |  반복 1 / 2 | 시간 2[s] | 손실 0.40\n",
      "| 에폭 723 |  반복 1 / 2 | 시간 2[s] | 손실 0.39\n",
      "| 에폭 724 |  반복 1 / 2 | 시간 2[s] | 손실 0.40\n",
      "| 에폭 725 |  반복 1 / 2 | 시간 2[s] | 손실 0.36\n",
      "| 에폭 726 |  반복 1 / 2 | 시간 2[s] | 손실 0.39\n",
      "| 에폭 727 |  반복 1 / 2 | 시간 2[s] | 손실 0.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 728 |  반복 1 / 2 | 시간 2[s] | 손실 0.32\n",
      "| 에폭 729 |  반복 1 / 2 | 시간 2[s] | 손실 0.46\n",
      "| 에폭 730 |  반복 1 / 2 | 시간 2[s] | 손실 0.42\n",
      "| 에폭 731 |  반복 1 / 2 | 시간 2[s] | 손실 0.39\n",
      "| 에폭 732 |  반복 1 / 2 | 시간 2[s] | 손실 0.28\n",
      "| 에폭 733 |  반복 1 / 2 | 시간 2[s] | 손실 0.49\n",
      "| 에폭 734 |  반복 1 / 2 | 시간 2[s] | 손실 0.39\n",
      "| 에폭 735 |  반복 1 / 2 | 시간 2[s] | 손실 0.36\n",
      "| 에폭 736 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 737 |  반복 1 / 2 | 시간 3[s] | 손실 0.39\n",
      "| 에폭 738 |  반복 1 / 2 | 시간 3[s] | 손실 0.32\n",
      "| 에폭 739 |  반복 1 / 2 | 시간 3[s] | 손실 0.49\n",
      "| 에폭 740 |  반복 1 / 2 | 시간 3[s] | 손실 0.39\n",
      "| 에폭 741 |  반복 1 / 2 | 시간 3[s] | 손실 0.46\n",
      "| 에폭 742 |  반복 1 / 2 | 시간 3[s] | 손실 0.28\n",
      "| 에폭 743 |  반복 1 / 2 | 시간 3[s] | 손실 0.38\n",
      "| 에폭 744 |  반복 1 / 2 | 시간 3[s] | 손실 0.38\n",
      "| 에폭 745 |  반복 1 / 2 | 시간 3[s] | 손실 0.39\n",
      "| 에폭 746 |  반복 1 / 2 | 시간 3[s] | 손실 0.38\n",
      "| 에폭 747 |  반복 1 / 2 | 시간 3[s] | 손실 0.41\n",
      "| 에폭 748 |  반복 1 / 2 | 시간 3[s] | 손실 0.27\n",
      "| 에폭 749 |  반복 1 / 2 | 시간 3[s] | 손실 0.49\n",
      "| 에폭 750 |  반복 1 / 2 | 시간 3[s] | 손실 0.35\n",
      "| 에폭 751 |  반복 1 / 2 | 시간 3[s] | 손실 0.30\n",
      "| 에폭 752 |  반복 1 / 2 | 시간 3[s] | 손실 0.38\n",
      "| 에폭 753 |  반복 1 / 2 | 시간 3[s] | 손실 0.48\n",
      "| 에폭 754 |  반복 1 / 2 | 시간 3[s] | 손실 0.28\n",
      "| 에폭 755 |  반복 1 / 2 | 시간 3[s] | 손실 0.56\n",
      "| 에폭 756 |  반복 1 / 2 | 시간 3[s] | 손실 0.27\n",
      "| 에폭 757 |  반복 1 / 2 | 시간 3[s] | 손실 0.51\n",
      "| 에폭 758 |  반복 1 / 2 | 시간 3[s] | 손실 0.17\n",
      "| 에폭 759 |  반복 1 / 2 | 시간 3[s] | 손실 0.48\n",
      "| 에폭 760 |  반복 1 / 2 | 시간 3[s] | 손실 0.38\n",
      "| 에폭 761 |  반복 1 / 2 | 시간 3[s] | 손실 0.37\n",
      "| 에폭 762 |  반복 1 / 2 | 시간 3[s] | 손실 0.25\n",
      "| 에폭 763 |  반복 1 / 2 | 시간 3[s] | 손실 0.58\n",
      "| 에폭 764 |  반복 1 / 2 | 시간 3[s] | 손실 0.30\n",
      "| 에폭 765 |  반복 1 / 2 | 시간 3[s] | 손실 0.27\n",
      "| 에폭 766 |  반복 1 / 2 | 시간 3[s] | 손실 0.35\n",
      "| 에폭 767 |  반복 1 / 2 | 시간 3[s] | 손실 0.58\n",
      "| 에폭 768 |  반복 1 / 2 | 시간 3[s] | 손실 0.17\n",
      "| 에폭 769 |  반복 1 / 2 | 시간 3[s] | 손실 0.50\n",
      "| 에폭 770 |  반복 1 / 2 | 시간 3[s] | 손실 0.45\n",
      "| 에폭 771 |  반복 1 / 2 | 시간 3[s] | 손실 0.19\n",
      "| 에폭 772 |  반복 1 / 2 | 시간 3[s] | 손실 0.58\n",
      "| 에폭 773 |  반복 1 / 2 | 시간 3[s] | 손실 0.34\n",
      "| 에폭 774 |  반복 1 / 2 | 시간 3[s] | 손실 0.29\n",
      "| 에폭 775 |  반복 1 / 2 | 시간 3[s] | 손실 0.34\n",
      "| 에폭 776 |  반복 1 / 2 | 시간 3[s] | 손실 0.39\n",
      "| 에폭 777 |  반복 1 / 2 | 시간 3[s] | 손실 0.24\n",
      "| 에폭 778 |  반복 1 / 2 | 시간 3[s] | 손실 0.48\n",
      "| 에폭 779 |  반복 1 / 2 | 시간 3[s] | 손실 0.39\n",
      "| 에폭 780 |  반복 1 / 2 | 시간 3[s] | 손실 0.37\n",
      "| 에폭 781 |  반복 1 / 2 | 시간 3[s] | 손실 0.23\n",
      "| 에폭 782 |  반복 1 / 2 | 시간 3[s] | 손실 0.40\n",
      "| 에폭 783 |  반복 1 / 2 | 시간 3[s] | 손실 0.57\n",
      "| 에폭 784 |  반복 1 / 2 | 시간 3[s] | 손실 0.26\n",
      "| 에폭 785 |  반복 1 / 2 | 시간 3[s] | 손실 0.45\n",
      "| 에폭 786 |  반복 1 / 2 | 시간 3[s] | 손실 0.37\n",
      "| 에폭 787 |  반복 1 / 2 | 시간 3[s] | 손실 0.28\n",
      "| 에폭 788 |  반복 1 / 2 | 시간 3[s] | 손실 0.36\n",
      "| 에폭 789 |  반복 1 / 2 | 시간 3[s] | 손실 0.34\n",
      "| 에폭 790 |  반복 1 / 2 | 시간 3[s] | 손실 0.36\n",
      "| 에폭 791 |  반복 1 / 2 | 시간 3[s] | 손실 0.29\n",
      "| 에폭 792 |  반복 1 / 2 | 시간 3[s] | 손실 0.57\n",
      "| 에폭 793 |  반복 1 / 2 | 시간 3[s] | 손실 0.25\n",
      "| 에폭 794 |  반복 1 / 2 | 시간 3[s] | 손실 0.37\n",
      "| 에폭 795 |  반복 1 / 2 | 시간 3[s] | 손실 0.36\n",
      "| 에폭 796 |  반복 1 / 2 | 시간 3[s] | 손실 0.36\n",
      "| 에폭 797 |  반복 1 / 2 | 시간 3[s] | 손실 0.36\n",
      "| 에폭 798 |  반복 1 / 2 | 시간 3[s] | 손실 0.33\n",
      "| 에폭 799 |  반복 1 / 2 | 시간 3[s] | 손실 0.47\n",
      "| 에폭 800 |  반복 1 / 2 | 시간 3[s] | 손실 0.28\n",
      "| 에폭 801 |  반복 1 / 2 | 시간 3[s] | 손실 0.36\n",
      "| 에폭 802 |  반복 1 / 2 | 시간 3[s] | 손실 0.36\n",
      "| 에폭 803 |  반복 1 / 2 | 시간 3[s] | 손실 0.44\n",
      "| 에폭 804 |  반복 1 / 2 | 시간 3[s] | 손실 0.36\n",
      "| 에폭 805 |  반복 1 / 2 | 시간 3[s] | 손실 0.28\n",
      "| 에폭 806 |  반복 1 / 2 | 시간 3[s] | 손실 0.25\n",
      "| 에폭 807 |  반복 1 / 2 | 시간 3[s] | 손실 0.44\n",
      "| 에폭 808 |  반복 1 / 2 | 시간 3[s] | 손실 0.49\n",
      "| 에폭 809 |  반복 1 / 2 | 시간 3[s] | 손실 0.33\n",
      "| 에폭 810 |  반복 1 / 2 | 시간 3[s] | 손실 0.16\n",
      "| 에폭 811 |  반복 1 / 2 | 시간 3[s] | 손실 0.57\n",
      "| 에폭 812 |  반복 1 / 2 | 시간 3[s] | 손실 0.14\n",
      "| 에폭 813 |  반복 1 / 2 | 시간 3[s] | 손실 0.33\n",
      "| 에폭 814 |  반복 1 / 2 | 시간 3[s] | 손실 0.49\n",
      "| 에폭 815 |  반복 1 / 2 | 시간 3[s] | 손실 0.25\n",
      "| 에폭 816 |  반복 1 / 2 | 시간 3[s] | 손실 0.46\n",
      "| 에폭 817 |  반복 1 / 2 | 시간 3[s] | 손실 0.35\n",
      "| 에폭 818 |  반복 1 / 2 | 시간 3[s] | 손실 0.33\n",
      "| 에폭 819 |  반복 1 / 2 | 시간 3[s] | 손실 0.27\n",
      "| 에폭 820 |  반복 1 / 2 | 시간 3[s] | 손실 0.55\n",
      "| 에폭 821 |  반복 1 / 2 | 시간 3[s] | 손실 0.24\n",
      "| 에폭 822 |  반복 1 / 2 | 시간 3[s] | 손실 0.35\n",
      "| 에폭 823 |  반복 1 / 2 | 시간 3[s] | 손실 0.38\n",
      "| 에폭 824 |  반복 1 / 2 | 시간 3[s] | 손실 0.35\n",
      "| 에폭 825 |  반복 1 / 2 | 시간 3[s] | 손실 0.32\n",
      "| 에폭 826 |  반복 1 / 2 | 시간 3[s] | 손실 0.38\n",
      "| 에폭 827 |  반복 1 / 2 | 시간 3[s] | 손실 0.35\n",
      "| 에폭 828 |  반복 1 / 2 | 시간 3[s] | 손실 0.43\n",
      "| 에폭 829 |  반복 1 / 2 | 시간 3[s] | 손실 0.16\n",
      "| 에폭 830 |  반복 1 / 2 | 시간 3[s] | 손실 0.34\n",
      "| 에폭 831 |  반복 1 / 2 | 시간 3[s] | 손실 0.43\n",
      "| 에폭 832 |  반복 1 / 2 | 시간 3[s] | 손실 0.48\n",
      "| 에폭 833 |  반복 1 / 2 | 시간 3[s] | 손실 0.13\n",
      "| 에폭 834 |  반복 1 / 2 | 시간 3[s] | 손실 0.46\n",
      "| 에폭 835 |  반복 1 / 2 | 시간 3[s] | 손실 0.35\n",
      "| 에폭 836 |  반복 1 / 2 | 시간 3[s] | 손실 0.34\n",
      "| 에폭 837 |  반복 1 / 2 | 시간 3[s] | 손실 0.35\n",
      "| 에폭 838 |  반복 1 / 2 | 시간 3[s] | 손실 0.35\n",
      "| 에폭 839 |  반복 1 / 2 | 시간 3[s] | 손실 0.34\n",
      "| 에폭 840 |  반복 1 / 2 | 시간 3[s] | 손실 0.32\n",
      "| 에폭 841 |  반복 1 / 2 | 시간 3[s] | 손실 0.37\n",
      "| 에폭 842 |  반복 1 / 2 | 시간 3[s] | 손실 0.35\n",
      "| 에폭 843 |  반복 1 / 2 | 시간 3[s] | 손실 0.43\n",
      "| 에폭 844 |  반복 1 / 2 | 시간 3[s] | 손실 0.23\n",
      "| 에폭 845 |  반복 1 / 2 | 시간 3[s] | 손실 0.26\n",
      "| 에폭 846 |  반복 1 / 2 | 시간 3[s] | 손실 0.43\n",
      "| 에폭 847 |  반복 1 / 2 | 시간 3[s] | 손실 0.45\n",
      "| 에폭 848 |  반복 1 / 2 | 시간 3[s] | 손실 0.24\n",
      "| 에폭 849 |  반복 1 / 2 | 시간 3[s] | 손실 0.36\n",
      "| 에폭 850 |  반복 1 / 2 | 시간 3[s] | 손실 0.32\n",
      "| 에폭 851 |  반복 1 / 2 | 시간 3[s] | 손실 0.35\n",
      "| 에폭 852 |  반복 1 / 2 | 시간 3[s] | 손실 0.36\n",
      "| 에폭 853 |  반복 1 / 2 | 시간 3[s] | 손실 0.45\n",
      "| 에폭 854 |  반복 1 / 2 | 시간 3[s] | 손실 0.13\n",
      "| 에폭 855 |  반복 1 / 2 | 시간 3[s] | 손실 0.44\n",
      "| 에폭 856 |  반복 1 / 2 | 시간 3[s] | 손실 0.34\n",
      "| 에폭 857 |  반복 1 / 2 | 시간 3[s] | 손실 0.34\n",
      "| 에폭 858 |  반복 1 / 2 | 시간 3[s] | 손실 0.21\n",
      "| 에폭 859 |  반복 1 / 2 | 시간 3[s] | 손실 0.47\n",
      "| 에폭 860 |  반복 1 / 2 | 시간 3[s] | 손실 0.32\n",
      "| 에폭 861 |  반복 1 / 2 | 시간 3[s] | 손실 0.34\n",
      "| 에폭 862 |  반복 1 / 2 | 시간 3[s] | 손실 0.36\n",
      "| 에폭 863 |  반복 1 / 2 | 시간 3[s] | 손실 0.31\n",
      "| 에폭 864 |  반복 1 / 2 | 시간 3[s] | 손실 0.34\n",
      "| 에폭 865 |  반복 1 / 2 | 시간 3[s] | 손실 0.36\n",
      "| 에폭 866 |  반복 1 / 2 | 시간 3[s] | 손실 0.42\n",
      "| 에폭 867 |  반복 1 / 2 | 시간 3[s] | 손실 0.25\n",
      "| 에폭 868 |  반복 1 / 2 | 시간 3[s] | 손실 0.34\n",
      "| 에폭 869 |  반복 1 / 2 | 시간 3[s] | 손실 0.42\n",
      "| 에폭 870 |  반복 1 / 2 | 시간 3[s] | 손실 0.12\n",
      "| 에폭 871 |  반복 1 / 2 | 시간 3[s] | 손실 0.55\n",
      "| 에폭 872 |  반복 1 / 2 | 시간 3[s] | 손실 0.24\n",
      "| 에폭 873 |  반복 1 / 2 | 시간 3[s] | 손실 0.43\n",
      "| 에폭 874 |  반복 1 / 2 | 시간 3[s] | 손실 0.33\n",
      "| 에폭 875 |  반복 1 / 2 | 시간 3[s] | 손실 0.22\n",
      "| 에폭 876 |  반복 1 / 2 | 시간 3[s] | 손실 0.36\n",
      "| 에폭 877 |  반복 1 / 2 | 시간 3[s] | 손실 0.31\n",
      "| 에폭 878 |  반복 1 / 2 | 시간 3[s] | 손실 0.46\n",
      "| 에폭 879 |  반복 1 / 2 | 시간 3[s] | 손실 0.21\n",
      "| 에폭 880 |  반복 1 / 2 | 시간 3[s] | 손실 0.35\n",
      "| 에폭 881 |  반복 1 / 2 | 시간 3[s] | 손실 0.33\n",
      "| 에폭 882 |  반복 1 / 2 | 시간 3[s] | 손실 0.33\n",
      "| 에폭 883 |  반복 1 / 2 | 시간 3[s] | 손실 0.31\n",
      "| 에폭 884 |  반복 1 / 2 | 시간 3[s] | 손실 0.22\n",
      "| 에폭 885 |  반복 1 / 2 | 시간 3[s] | 손실 0.55\n",
      "| 에폭 886 |  반복 1 / 2 | 시간 3[s] | 손실 0.22\n",
      "| 에폭 887 |  반복 1 / 2 | 시간 3[s] | 손실 0.25\n",
      "| 에폭 888 |  반복 1 / 2 | 시간 3[s] | 손실 0.33\n",
      "| 에폭 889 |  반복 1 / 2 | 시간 3[s] | 손실 0.42\n",
      "| 에폭 890 |  반복 1 / 2 | 시간 3[s] | 손실 0.44\n",
      "| 에폭 891 |  반복 1 / 2 | 시간 3[s] | 손실 0.24\n",
      "| 에폭 892 |  반복 1 / 2 | 시간 3[s] | 손실 0.31\n",
      "| 에폭 893 |  반복 1 / 2 | 시간 3[s] | 손실 0.32\n",
      "| 에폭 894 |  반복 1 / 2 | 시간 3[s] | 손실 0.35\n",
      "| 에폭 895 |  반복 1 / 2 | 시간 3[s] | 손실 0.23\n",
      "| 에폭 896 |  반복 1 / 2 | 시간 3[s] | 손실 0.43\n",
      "| 에폭 897 |  반복 1 / 2 | 시간 3[s] | 손실 0.33\n",
      "| 에폭 898 |  반복 1 / 2 | 시간 3[s] | 손실 0.31\n",
      "| 에폭 899 |  반복 1 / 2 | 시간 3[s] | 손실 0.44\n",
      "| 에폭 900 |  반복 1 / 2 | 시간 3[s] | 손실 0.24\n",
      "| 에폭 901 |  반복 1 / 2 | 시간 3[s] | 손실 0.32\n",
      "| 에폭 902 |  반복 1 / 2 | 시간 3[s] | 손실 0.31\n",
      "| 에폭 903 |  반복 1 / 2 | 시간 3[s] | 손실 0.24\n",
      "| 에폭 904 |  반복 1 / 2 | 시간 3[s] | 손실 0.32\n",
      "| 에폭 905 |  반복 1 / 2 | 시간 3[s] | 손실 0.44\n",
      "| 에폭 906 |  반복 1 / 2 | 시간 3[s] | 손실 0.33\n",
      "| 에폭 907 |  반복 1 / 2 | 시간 3[s] | 손실 0.30\n",
      "| 에폭 908 |  반복 1 / 2 | 시간 3[s] | 손실 0.35\n",
      "| 에폭 909 |  반복 1 / 2 | 시간 3[s] | 손실 0.41\n",
      "| 에폭 910 |  반복 1 / 2 | 시간 3[s] | 손실 0.33\n",
      "| 에폭 911 |  반복 1 / 2 | 시간 3[s] | 손실 0.23\n",
      "| 에폭 912 |  반복 1 / 2 | 시간 3[s] | 손실 0.31\n",
      "| 에폭 913 |  반복 1 / 2 | 시간 3[s] | 손실 0.32\n",
      "| 에폭 914 |  반복 1 / 2 | 시간 3[s] | 손실 0.32\n",
      "| 에폭 915 |  반복 1 / 2 | 시간 3[s] | 손실 0.24\n",
      "| 에폭 916 |  반복 1 / 2 | 시간 3[s] | 손실 0.32\n",
      "| 에폭 917 |  반복 1 / 2 | 시간 3[s] | 손실 0.43\n",
      "| 에폭 918 |  반복 1 / 2 | 시간 3[s] | 손실 0.31\n",
      "| 에폭 919 |  반복 1 / 2 | 시간 3[s] | 손실 0.34\n",
      "| 에폭 920 |  반복 1 / 2 | 시간 3[s] | 손실 0.21\n",
      "| 에폭 921 |  반복 1 / 2 | 시간 3[s] | 손실 0.33\n",
      "| 에폭 922 |  반복 1 / 2 | 시간 3[s] | 손실 0.32\n",
      "| 에폭 923 |  반복 1 / 2 | 시간 3[s] | 손실 0.52\n",
      "| 에폭 924 |  반복 1 / 2 | 시간 3[s] | 손실 0.23\n",
      "| 에폭 925 |  반복 1 / 2 | 시간 4[s] | 손실 0.41\n",
      "| 에폭 926 |  반복 1 / 2 | 시간 4[s] | 손실 0.10\n",
      "| 에폭 927 |  반복 1 / 2 | 시간 4[s] | 손실 0.34\n",
      "| 에폭 928 |  반복 1 / 2 | 시간 4[s] | 손실 0.43\n",
      "| 에폭 929 |  반복 1 / 2 | 시간 4[s] | 손실 0.32\n",
      "| 에폭 930 |  반복 1 / 2 | 시간 4[s] | 손실 0.32\n",
      "| 에폭 931 |  반복 1 / 2 | 시간 4[s] | 손실 0.30\n",
      "| 에폭 932 |  반복 1 / 2 | 시간 4[s] | 손실 0.33\n",
      "| 에폭 933 |  반복 1 / 2 | 시간 4[s] | 손실 0.42\n",
      "| 에폭 934 |  반복 1 / 2 | 시간 4[s] | 손실 0.32\n",
      "| 에폭 935 |  반복 1 / 2 | 시간 4[s] | 손실 0.21\n",
      "| 에폭 936 |  반복 1 / 2 | 시간 4[s] | 손실 0.32\n",
      "| 에폭 937 |  반복 1 / 2 | 시간 4[s] | 손실 0.43\n",
      "| 에폭 938 |  반복 1 / 2 | 시간 4[s] | 손실 0.23\n",
      "| 에폭 939 |  반복 1 / 2 | 시간 4[s] | 손실 0.32\n",
      "| 에폭 940 |  반복 1 / 2 | 시간 4[s] | 손실 0.30\n",
      "| 에폭 941 |  반복 1 / 2 | 시간 4[s] | 손실 0.22\n",
      "| 에폭 942 |  반복 1 / 2 | 시간 4[s] | 손실 0.30\n",
      "| 에폭 943 |  반복 1 / 2 | 시간 4[s] | 손실 0.44\n",
      "| 에폭 944 |  반복 1 / 2 | 시간 4[s] | 손실 0.32\n",
      "| 에폭 945 |  반복 1 / 2 | 시간 4[s] | 손실 0.30\n",
      "| 에폭 946 |  반복 1 / 2 | 시간 4[s] | 손실 0.23\n",
      "| 에폭 947 |  반복 1 / 2 | 시간 4[s] | 손실 0.53\n",
      "| 에폭 948 |  반복 1 / 2 | 시간 4[s] | 손실 0.19\n",
      "| 에폭 949 |  반복 1 / 2 | 시간 4[s] | 손실 0.43\n",
      "| 에폭 950 |  반복 1 / 2 | 시간 4[s] | 손실 0.22\n",
      "| 에폭 951 |  반복 1 / 2 | 시간 4[s] | 손실 0.30\n",
      "| 에폭 952 |  반복 1 / 2 | 시간 4[s] | 손실 0.22\n",
      "| 에폭 953 |  반복 1 / 2 | 시간 4[s] | 손실 0.41\n",
      "| 에폭 954 |  반복 1 / 2 | 시간 4[s] | 손실 0.33\n",
      "| 에폭 955 |  반복 1 / 2 | 시간 4[s] | 손실 0.43\n",
      "| 에폭 956 |  반복 1 / 2 | 시간 4[s] | 손실 0.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 957 |  반복 1 / 2 | 시간 4[s] | 손실 0.31\n",
      "| 에폭 958 |  반복 1 / 2 | 시간 4[s] | 손실 0.32\n",
      "| 에폭 959 |  반복 1 / 2 | 시간 4[s] | 손실 0.42\n",
      "| 에폭 960 |  반복 1 / 2 | 시간 4[s] | 손실 0.11\n",
      "| 에폭 961 |  반복 1 / 2 | 시간 4[s] | 손실 0.29\n",
      "| 에폭 962 |  반복 1 / 2 | 시간 4[s] | 손실 0.53\n",
      "| 에폭 963 |  반복 1 / 2 | 시간 4[s] | 손실 0.22\n",
      "| 에폭 964 |  반복 1 / 2 | 시간 4[s] | 손실 0.29\n",
      "| 에폭 965 |  반복 1 / 2 | 시간 4[s] | 손실 0.33\n",
      "| 에폭 966 |  반복 1 / 2 | 시간 4[s] | 손실 0.31\n",
      "| 에폭 967 |  반복 1 / 2 | 시간 4[s] | 손실 0.31\n",
      "| 에폭 968 |  반복 1 / 2 | 시간 4[s] | 손실 0.31\n",
      "| 에폭 969 |  반복 1 / 2 | 시간 4[s] | 손실 0.30\n",
      "| 에폭 970 |  반복 1 / 2 | 시간 4[s] | 손실 0.31\n",
      "| 에폭 971 |  반복 1 / 2 | 시간 4[s] | 손실 0.42\n",
      "| 에폭 972 |  반복 1 / 2 | 시간 4[s] | 손실 0.21\n",
      "| 에폭 973 |  반복 1 / 2 | 시간 4[s] | 손실 0.40\n",
      "| 에폭 974 |  반복 1 / 2 | 시간 4[s] | 손실 0.20\n",
      "| 에폭 975 |  반복 1 / 2 | 시간 4[s] | 손실 0.33\n",
      "| 에폭 976 |  반복 1 / 2 | 시간 4[s] | 손실 0.40\n",
      "| 에폭 977 |  반복 1 / 2 | 시간 4[s] | 손실 0.20\n",
      "| 에폭 978 |  반복 1 / 2 | 시간 4[s] | 손실 0.21\n",
      "| 에폭 979 |  반복 1 / 2 | 시간 4[s] | 손실 0.31\n",
      "| 에폭 980 |  반복 1 / 2 | 시간 4[s] | 손실 0.42\n",
      "| 에폭 981 |  반복 1 / 2 | 시간 4[s] | 손실 0.20\n",
      "| 에폭 982 |  반복 1 / 2 | 시간 4[s] | 손실 0.40\n",
      "| 에폭 983 |  반복 1 / 2 | 시간 4[s] | 손실 0.31\n",
      "| 에폭 984 |  반복 1 / 2 | 시간 4[s] | 손실 0.32\n",
      "| 에폭 985 |  반복 1 / 2 | 시간 4[s] | 손실 0.31\n",
      "| 에폭 986 |  반복 1 / 2 | 시간 4[s] | 손실 0.31\n",
      "| 에폭 987 |  반복 1 / 2 | 시간 4[s] | 손실 0.18\n",
      "| 에폭 988 |  반복 1 / 2 | 시간 4[s] | 손실 0.53\n",
      "| 에폭 989 |  반복 1 / 2 | 시간 4[s] | 손실 0.21\n",
      "| 에폭 990 |  반복 1 / 2 | 시간 4[s] | 손실 0.41\n",
      "| 에폭 991 |  반복 1 / 2 | 시간 4[s] | 손실 0.18\n",
      "| 에폭 992 |  반복 1 / 2 | 시간 4[s] | 손실 0.31\n",
      "| 에폭 993 |  반복 1 / 2 | 시간 4[s] | 손실 0.30\n",
      "| 에폭 994 |  반복 1 / 2 | 시간 4[s] | 손실 0.43\n",
      "| 에폭 995 |  반복 1 / 2 | 시간 4[s] | 손실 0.20\n",
      "| 에폭 996 |  반복 1 / 2 | 시간 4[s] | 손실 0.20\n",
      "| 에폭 997 |  반복 1 / 2 | 시간 4[s] | 손실 0.51\n",
      "| 에폭 998 |  반복 1 / 2 | 시간 4[s] | 손실 0.21\n",
      "| 에폭 999 |  반복 1 / 2 | 시간 4[s] | 손실 0.30\n",
      "| 에폭 1000 |  반복 1 / 2 | 시간 4[s] | 손실 0.20\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(contexts, target, max_epoch, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 48152 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 48373 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 49552 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 49892 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 48152 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 48373 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 49552 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 49892 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_vecs shape: (7, 5)\n",
      "word_vecs:\n",
      " [[ 1.0051681   1.0606265  -0.89968085  1.0096691  -1.6717455 ]\n",
      " [-1.305112   -0.01289637  0.75665206 -1.2728289  -1.2093349 ]\n",
      " [ 1.0537305   0.57333666 -1.0446119   1.1022193   0.722222  ]\n",
      " [-0.8566321  -1.6239289   1.4915109  -0.6251232  -1.1760442 ]\n",
      " [ 1.0874134   0.56969786 -1.0374919   1.1244438   0.7134779 ]\n",
      " [ 1.0069185   1.0571725  -0.8893863   1.0217401  -1.660727  ]\n",
      " [-1.2847663   1.3940749  -1.591743   -1.2779802  -0.8214862 ]]\n"
     ]
    }
   ],
   "source": [
    "word_vecs = model.word_vecs\n",
    "print(\"word_vecs shape:\", word_vecs.shape)\n",
    "print(\"word_vecs:\\n\",word_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you [ 1.0051681   1.0606265  -0.89968085  1.0096691  -1.6717455 ]\n",
      "say [-1.305112   -0.01289637  0.75665206 -1.2728289  -1.2093349 ]\n",
      "goodbye [ 1.0537305   0.57333666 -1.0446119   1.1022193   0.722222  ]\n",
      "and [-0.8566321 -1.6239289  1.4915109 -0.6251232 -1.1760442]\n",
      "i [ 1.0874134   0.56969786 -1.0374919   1.1244438   0.7134779 ]\n",
      "hello [ 1.0069185  1.0571725 -0.8893863  1.0217401 -1.660727 ]\n",
      ". [-1.2847663  1.3940749 -1.591743  -1.2779802 -0.8214862]\n"
     ]
    }
   ],
   "source": [
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SimpleCBOW' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-66acf8bb8b73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mcontexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleCBOW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SimpleCBOW' is not defined"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "#from common.trainer import Trainer\n",
    "#from common.optimizer import Adam\n",
    "#from simple_cbow import SimpleCBOW\n",
    "#from common.util import preprocess, create_contexts_target, convert_one_hot\n",
    "\n",
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 1000\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "\n",
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "\n",
    "word_vecs = model.word_vecs\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# common_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================#\n",
    "#base_model.py\n",
    "#================================================================================#\n",
    "\n",
    "\n",
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import pickle\n",
    "#from common.np import *\n",
    "#from common.util import to_gpu, to_cpu\n",
    "\n",
    "\n",
    "class BaseModel:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = None, None\n",
    "\n",
    "    def forward(self, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save_params(self, file_name=None):\n",
    "        if file_name is None:\n",
    "            file_name = self.__class__.__name__ + '.pkl'\n",
    "\n",
    "        params = [p.astype(np.float16) for p in self.params]\n",
    "        if GPU:\n",
    "            params = [to_cpu(p) for p in params]\n",
    "\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=None):\n",
    "        if file_name is None:\n",
    "            file_name = self.__class__.__name__ + '.pkl'\n",
    "\n",
    "        if '/' in file_name:\n",
    "            file_name = file_name.replace('/', os.sep)\n",
    "\n",
    "        if not os.path.exists(file_name):\n",
    "            raise IOError('No file: ' + file_name)\n",
    "\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "\n",
    "        params = [p.astype('f') for p in params]\n",
    "        if GPU:\n",
    "            params = [to_gpu(p) for p in params]\n",
    "\n",
    "        for i, param in enumerate(self.params):\n",
    "            param[...] = params[i]\n",
    "\n",
    "\n",
    "#================================================================================#\n",
    "#config.py\n",
    "#================================================================================#\n",
    "# coding: utf-8\n",
    "\n",
    "GPU = False\n",
    "\n",
    "#================================================================================#\n",
    "#functions.py\n",
    "#================================================================================#\n",
    "# coding: utf-8\n",
    "#from common.np import *\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 정답 데이터가 원핫 벡터일 경우 정답 레이블 인덱스로 변환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "#================================================================================#\n",
    "#layers.py\n",
    "#================================================================================#\n",
    "\n",
    "# coding: utf-8\n",
    "#from common.np import *  # import numpy as np\n",
    "#from common.config import GPU\n",
    "#from common.functions import softmax, cross_entropy_error\n",
    "\n",
    "\n",
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.out = softmax(x)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = self.out * dout\n",
    "        sumdx = np.sum(dx, axis=1, keepdims=True)\n",
    "        dx -= self.out * sumdx\n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None  # softmax의 출력\n",
    "        self.t = None  # 정답 레이블\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "\n",
    "        # 정답 레이블이 원핫 벡터일 경우 정답의 인덱스로 변환\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "\n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = self.y.copy()\n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx = dx / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx\n",
    "\n",
    "#================================================================================#\n",
    "#layers.py\n",
    "#================================================================================#\n",
    "\n",
    "class SigmoidWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.loss = None\n",
    "        self.y = None  # sigmoid의 출력\n",
    "        self.t = None  # 정답 데이터\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "        self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = (self.y - self.t) * dout / batch_size\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Dropout:\n",
    "    '''\n",
    "    http://arxiv.org/abs/1207.0580\n",
    "    '''\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.params, self.grads = [], []\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "        return None\n",
    "\n",
    "\n",
    "#================================================================================#\n",
    "#np.py\n",
    "#================================================================================#\n",
    "\n",
    "# coding: utf-8\n",
    "#from common.config import GPU\n",
    "\n",
    "\n",
    "if GPU:\n",
    "    import cupy as np\n",
    "    np.cuda.set_allocator(np.cuda.MemoryPool().malloc)\n",
    "    np.add.at = np.scatter_add\n",
    "\n",
    "    print('\\033[92m' + '-' * 60 + '\\033[0m')\n",
    "    print(' ' * 23 + '\\033[92mGPU Mode (cupy)\\033[0m')\n",
    "    print('\\033[92m' + '-' * 60 + '\\033[0m\\n')\n",
    "else:\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "#================================================================================#\n",
    "#optimizer.py\n",
    "#================================================================================#\n",
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "#from common.np import *\n",
    "\n",
    "\n",
    "class SGD:\n",
    "    '''\n",
    "    확률적 경사하강법(Stochastic Gradient Descent)\n",
    "    '''\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]\n",
    "\n",
    "\n",
    "class Momentum:\n",
    "    '''\n",
    "    모멘텀 SGG(Momentum SGD)\n",
    "    '''\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = []\n",
    "            for param in params:\n",
    "                self.v.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.v[i] = self.momentum * self.v[i] - self.lr * grads[i]\n",
    "            params[i] += self.v[i]\n",
    "\n",
    "\n",
    "class Nesterov:\n",
    "    '''\n",
    "    네스테로프 가속 경사(NAG; Nesterov's Accelerated Gradient) (http://arxiv.org/abs/1212.0901)\n",
    "    '네스테로프 모멘텀 최적화'라고도 한다.\n",
    "    '''\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = []\n",
    "            for param in params:\n",
    "                self.v.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.v[i] *= self.momentum\n",
    "            self.v[i] -= self.lr * grads[i]\n",
    "            params[i] += self.momentum * self.momentum * self.v[i]\n",
    "            params[i] -= (1 + self.momentum) * self.lr * grads[i]\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "    '''\n",
    "    AdaGrad\n",
    "    '''\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = []\n",
    "            for param in params:\n",
    "                self.h.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.h[i] += grads[i] * grads[i]\n",
    "            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)\n",
    "\n",
    "\n",
    "class RMSprop:\n",
    "    '''\n",
    "    RMSprop\n",
    "    '''\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = []\n",
    "            for param in params:\n",
    "                self.h.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.h[i] *= self.decay_rate\n",
    "            self.h[i] += (1 - self.decay_rate) * grads[i] * grads[i]\n",
    "            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    '''\n",
    "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
    "    '''\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "            \n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)\n",
    "\n",
    "#================================================================================#\n",
    "#time_layers.py\n",
    "#================================================================================#\n",
    "# coding: utf-8\n",
    "#from common.np import *  # import numpy as np (or import cupy as np)\n",
    "#from common.layers import *\n",
    "#from common.functions import sigmoid\n",
    "\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
    "        h_next = np.tanh(t)\n",
    "\n",
    "        self.cache = (x, h_prev, h_next)\n",
    "        return h_next\n",
    "\n",
    "    def backward(self, dh_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, h_next = self.cache\n",
    "\n",
    "        dt = dh_next * (1 - h_next ** 2)\n",
    "        db = np.sum(dt, axis=0)\n",
    "        dWh = np.dot(h_prev.T, dt)\n",
    "        dh_prev = np.dot(dt, Wh.T)\n",
    "        dWx = np.dot(x.T, dt)\n",
    "        dx = np.dot(dt, Wx.T)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        return dx, dh_prev\n",
    "\n",
    "\n",
    "class TimeRNN:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.dh = None, None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        D, H = Wx.shape\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = RNN(*self.params)\n",
    "            self.h = layer.forward(xs[:, t, :], self.h)\n",
    "            hs[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D, H = Wx.shape\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh = 0\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
    "            dxs[:, t, :] = dx\n",
    "\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        '''\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Wx: 입력 x에 대한 가중치 매개변수(4개분의 가중치가 담겨 있음)\n",
    "        Wh: 은닉 상태 h에 대한 가장추 매개변수(4개분의 가중치가 담겨 있음)\n",
    "        b: 편향（4개분의 편향이 담겨 있음）\n",
    "        '''\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, H = h_prev.shape\n",
    "\n",
    "        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n",
    "\n",
    "        f = A[:, :H]\n",
    "        g = A[:, H:2*H]\n",
    "        i = A[:, 2*H:3*H]\n",
    "        o = A[:, 3*H:]\n",
    "\n",
    "        f = sigmoid(f)\n",
    "        g = np.tanh(g)\n",
    "        i = sigmoid(i)\n",
    "        o = sigmoid(o)\n",
    "\n",
    "        c_next = f * c_prev + g * i\n",
    "        h_next = o * np.tanh(c_next)\n",
    "\n",
    "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "    def backward(self, dh_next, dc_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
    "\n",
    "        tanh_c_next = np.tanh(c_next)\n",
    "\n",
    "        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "\n",
    "        dc_prev = ds * f\n",
    "\n",
    "        di = ds * g\n",
    "        df = ds * c_prev\n",
    "        do = dh_next * tanh_c_next\n",
    "        dg = ds * i\n",
    "\n",
    "        di *= i * (1 - i)\n",
    "        df *= f * (1 - f)\n",
    "        do *= o * (1 - o)\n",
    "        dg *= (1 - g ** 2)\n",
    "\n",
    "        dA = np.hstack((df, dg, di, do))\n",
    "\n",
    "        dWh = np.dot(h_prev.T, dA)\n",
    "        dWx = np.dot(x.T, dA)\n",
    "        db = dA.sum(axis=0)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        dx = np.dot(dA, Wx.T)\n",
    "        dh_prev = np.dot(dA, Wh.T)\n",
    "\n",
    "        return dx, dh_prev, dc_prev\n",
    "\n",
    "\n",
    "class TimeLSTM:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.c = None, None\n",
    "        self.dh = None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        H = Wh.shape[0]\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "        if not self.stateful or self.c is None:\n",
    "            self.c = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = LSTM(*self.params)\n",
    "            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n",
    "            hs[:, t, :] = self.h\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D = Wx.shape[0]\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh, dc = 0, 0\n",
    "\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n",
    "            dxs[:, t, :] = dx\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h, c=None):\n",
    "        self.h, self.c = h, c\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h, self.c = None, None\n",
    "\n",
    "\n",
    "class TimeEmbedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.layers = None\n",
    "        self.W = W\n",
    "\n",
    "    def forward(self, xs):\n",
    "        N, T = xs.shape\n",
    "        V, D = self.W.shape\n",
    "\n",
    "        out = np.empty((N, T, D), dtype='f')\n",
    "        self.layers = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Embedding(self.W)\n",
    "            out[:, t, :] = layer.forward(xs[:, t])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, D = dout.shape\n",
    "\n",
    "        grad = 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            layer.backward(dout[:, t, :])\n",
    "            grad += layer.grads[0]\n",
    "\n",
    "        self.grads[0][...] = grad\n",
    "        return None\n",
    "\n",
    "\n",
    "class TimeAffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        rx = x.reshape(N*T, -1)\n",
    "        out = np.dot(rx, W) + b\n",
    "        self.x = x\n",
    "        return out.reshape(N, T, -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        x = self.x\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        dout = dout.reshape(N*T, -1)\n",
    "        rx = x.reshape(N*T, -1)\n",
    "\n",
    "        db = np.sum(dout, axis=0)\n",
    "        dW = np.dot(rx.T, dout)\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dx = dx.reshape(*x.shape)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class TimeSoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        self.ignore_label = -1\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T, V = xs.shape\n",
    "\n",
    "        if ts.ndim == 3:  # 정답 레이블이 원핫 벡터인 경우\n",
    "            ts = ts.argmax(axis=2)\n",
    "\n",
    "        mask = (ts != self.ignore_label)\n",
    "\n",
    "        # 배치용과 시계열용을 정리(reshape)\n",
    "        xs = xs.reshape(N * T, V)\n",
    "        ts = ts.reshape(N * T)\n",
    "        mask = mask.reshape(N * T)\n",
    "\n",
    "        ys = softmax(xs)\n",
    "        ls = np.log(ys[np.arange(N * T), ts])\n",
    "        ls *= mask  # ignore_label에 해당하는 데이터는 손실을 0으로 설정\n",
    "        loss = -np.sum(ls)\n",
    "        loss /= mask.sum()\n",
    "\n",
    "        self.cache = (ts, ys, mask, (N, T, V))\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ts, ys, mask, (N, T, V) = self.cache\n",
    "\n",
    "        dx = ys\n",
    "        dx[np.arange(N * T), ts] -= 1\n",
    "        dx *= dout\n",
    "        dx /= mask.sum()\n",
    "        dx *= mask[:, np.newaxis]  # ignore_labelㅇㅔ 해당하는 데이터는 기울기를 0으로 설정\n",
    "\n",
    "        dx = dx.reshape((N, T, V))\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class TimeDropout:\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.params, self.grads = [], []\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "        self.train_flg = True\n",
    "\n",
    "    def forward(self, xs):\n",
    "        if self.train_flg:\n",
    "            flg = np.random.rand(*xs.shape) > self.dropout_ratio\n",
    "            scale = 1 / (1.0 - self.dropout_ratio)\n",
    "            self.mask = flg.astype(np.float32) * scale\n",
    "\n",
    "            return xs * self.mask\n",
    "        else:\n",
    "            return xs\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "\n",
    "class TimeBiLSTM:\n",
    "    def __init__(self, Wx1, Wh1, b1,\n",
    "                 Wx2, Wh2, b2, stateful=False):\n",
    "        self.forward_lstm = TimeLSTM(Wx1, Wh1, b1, stateful)\n",
    "        self.backward_lstm = TimeLSTM(Wx2, Wh2, b2, stateful)\n",
    "        self.params = self.forward_lstm.params + self.backward_lstm.params\n",
    "        self.grads = self.forward_lstm.grads + self.backward_lstm.grads\n",
    "\n",
    "    def forward(self, xs):\n",
    "        o1 = self.forward_lstm.forward(xs)\n",
    "        o2 = self.backward_lstm.forward(xs[:, ::-1])\n",
    "        o2 = o2[:, ::-1]\n",
    "\n",
    "        out = np.concatenate((o1, o2), axis=2)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        H = dhs.shape[2] // 2\n",
    "        do1 = dhs[:, :, :H]\n",
    "        do2 = dhs[:, :, H:]\n",
    "\n",
    "        dxs1 = self.forward_lstm.backward(do1)\n",
    "        do2 = do2[:, ::-1]\n",
    "        dxs2 = self.backward_lstm.backward(do2)\n",
    "        dxs2 = dxs2[:, ::-1]\n",
    "        dxs = dxs1 + dxs2\n",
    "        return dxs\n",
    "\n",
    "# ====================================================================== #\n",
    "# 이 아래의 계층들은 책에서 설명하지 않았거나\n",
    "# 처리 속도보다는 쉽게 이해할 수 있도록 구현했습니다.\n",
    "#\n",
    "# TimeSigmoidWithLoss: 시계열 데이터용 시그모이드 + 손실 계층\n",
    "# GRU: GRU 계층\n",
    "# TimeGRU: 시계열 데이터용 GRU 계층\n",
    "# BiTimeLSTM: 양방향 LSTM 계층\n",
    "# Simple_TimeSoftmaxWithLoss：간단한 TimeSoftmaxWithLoss 계층의 구현\n",
    "# Simple_TimeAffine: 간단한 TimeAffine 계층의 구현\n",
    "# ====================================================================== #\n",
    "\n",
    "\n",
    "class TimeSigmoidWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.xs_shape = None\n",
    "        self.layers = None\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T = xs.shape\n",
    "        self.xs_shape = xs.shape\n",
    "\n",
    "        self.layers = []\n",
    "        loss = 0\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = SigmoidWithLoss()\n",
    "            loss += layer.forward(xs[:, t], ts[:, t])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return loss / T\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        N, T = self.xs_shape\n",
    "        dxs = np.empty(self.xs_shape, dtype='f')\n",
    "\n",
    "        dout *= 1/T\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dxs[:, t] = layer.backward(dout)\n",
    "\n",
    "        return dxs\n",
    "\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, Wx, Wh):\n",
    "        '''\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Wx: 입력 x에 대한 가중치 매개변수(3개 분의 가중치가 담겨 있음)\n",
    "        Wh: 은닉 상태 h에 대한 가중치 매개변수(3개 분의 가중치가 담겨 있음)\n",
    "        '''\n",
    "        self.Wx, self.Wh = Wx, Wh\n",
    "        self.dWx, self.dWh = None, None\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        H, H3 = self.Wh.shape\n",
    "        Wxz, Wxr, Wx = self.Wx[:, :H], self.Wx[:, H:2 * H], self.Wx[:, 2 * H:]\n",
    "        Whz, Whr, Wh = self.Wh[:, :H], self.Wh[:, H:2 * H], self.Wh[:, 2 * H:]\n",
    "\n",
    "        z = sigmoid(np.dot(x, Wxz) + np.dot(h_prev, Whz))\n",
    "        r = sigmoid(np.dot(x, Wxr) + np.dot(h_prev, Whr))\n",
    "        h_hat = np.tanh(np.dot(x, Wx) + np.dot(r*h_prev, Wh))\n",
    "        h_next = (1-z) * h_prev + z * h_hat\n",
    "\n",
    "        self.cache = (x, h_prev, z, r, h_hat)\n",
    "\n",
    "        return h_next\n",
    "\n",
    "    def backward(self, dh_next):\n",
    "        H, H3 = self.Wh.shape\n",
    "        Wxz, Wxr, Wx = self.Wx[:, :H], self.Wx[:, H:2 * H], self.Wx[:, 2 * H:]\n",
    "        Whz, Whr, Wh = self.Wh[:, :H], self.Wh[:, H:2 * H], self.Wh[:, 2 * H:]\n",
    "        x, h_prev, z, r, h_hat = self.cache\n",
    "\n",
    "        dh_hat =dh_next * z\n",
    "        dh_prev = dh_next * (1-z)\n",
    "\n",
    "        # tanh\n",
    "        dt = dh_hat * (1 - h_hat ** 2)\n",
    "        dWh = np.dot((r * h_prev).T, dt)\n",
    "        dhr = np.dot(dt, Wh.T)\n",
    "        dWx = np.dot(x.T, dt)\n",
    "        dx = np.dot(dt, Wx.T)\n",
    "        dh_prev += r * dhr\n",
    "\n",
    "        # update gate(z)\n",
    "        dz = dh_next * h_hat - dh_next * h_prev\n",
    "        dt = dz * z * (1-z)\n",
    "        dWhz = np.dot(h_prev.T, dt)\n",
    "        dh_prev += np.dot(dt, Whz.T)\n",
    "        dWxz = np.dot(x.T, dt)\n",
    "        dx += np.dot(dt, Wxz.T)\n",
    "\n",
    "        # rest gate(r)\n",
    "        dr = dhr * h_prev\n",
    "        dt = dr * r * (1-r)\n",
    "        dWhr = np.dot(h_prev.T, dt)\n",
    "        dh_prev += np.dot(dt, Whr.T)\n",
    "        dWxr = np.dot(x.T, dt)\n",
    "        dx += np.dot(dt, Wxr.T)\n",
    "\n",
    "        self.dWx = np.hstack((dWxz, dWxr, dWx))\n",
    "        self.dWh = np.hstack((dWhz, dWhr, dWh))\n",
    "\n",
    "        return dx, dh_prev\n",
    "\n",
    "\n",
    "class TimeGRU:\n",
    "    def __init__(self, Wx, Wh, stateful=False):\n",
    "        self.Wx, self.Wh = Wx, Wh\n",
    "        selfdWx, self.dWh = None, None\n",
    "        self.layers = None\n",
    "        self.h, self.dh = None, None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        N, T, D = xs.shape\n",
    "        H, H3 = self.Wh.shape\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = GRU(self.Wx, self.Wh)\n",
    "            self.h = layer.forward(xs[:, t, :], self.h)\n",
    "            hs[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        N, T, H = dhs.shape\n",
    "        D = self.Wx.shape[0]\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        self.dWx, self.dWh = 0, 0\n",
    "\n",
    "        dh = 0\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
    "\n",
    "            dxs[:, t, :] = dx\n",
    "            self.dWx += layer.dWx\n",
    "            self.dWh += layer.dWh\n",
    "\n",
    "        self.dh = dh\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "\n",
    "\n",
    "class Simple_TimeSoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T, V = xs.shape\n",
    "        layers = []\n",
    "        loss = 0\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = SoftmaxWithLoss()\n",
    "            loss += layer.forward(xs[:, t, :], ts[:, t])\n",
    "            layers.append(layer)\n",
    "        loss /= T\n",
    "\n",
    "        self.cache = (layers, xs)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        layers, xs = self.cache\n",
    "        N, T, V = xs.shape\n",
    "        dxs = np.empty(xs.shape, dtype='f')\n",
    "\n",
    "        dout *= 1/T\n",
    "        for t in range(T):\n",
    "            layer = layers[t]\n",
    "            dxs[:, t, :] = layer.backward(dout)\n",
    "\n",
    "        return dxs\n",
    "\n",
    "\n",
    "class Simple_TimeAffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W, self.b = W, b\n",
    "        self.dW, self.db = None, None\n",
    "        self.layers = None\n",
    "\n",
    "    def forward(self, xs):\n",
    "        N, T, D = xs.shape\n",
    "        D, M = self.W.shape\n",
    "\n",
    "        self.layers = []\n",
    "        out = np.empty((N, T, M), dtype='f')\n",
    "        for t in range(T):\n",
    "            layer = Affine(self.W, self.b)\n",
    "            out[:, t, :] = layer.forward(xs[:, t, :])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, M = dout.shape\n",
    "        D, M = self.W.shape\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        self.dW, self.db = 0, 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dxs[:, t, :] = layer.backward(dout[:, t, :])\n",
    "\n",
    "            self.dW += layer.dW\n",
    "            self.db += layer.db\n",
    "\n",
    "        return dxs\n",
    "\n",
    "#================================================================================#\n",
    "#trainer.py\n",
    "#================================================================================#\n",
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "#from common.np import *  # import numpy as np\n",
    "#from common.util import clip_grads\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            # 뒤섞기\n",
    "            idx = numpy.random.permutation(numpy.arange(data_size))\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "                # 기울기 구해 매개변수 갱신\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.params, model.grads)  # 공유된 가중치를 하나로 모음\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # 평가\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    avg_loss = total_loss / loss_count\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print('| 에폭 %d |  반복 %d / %d | 시간 %d[s] | 손실 %.2f'\n",
    "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
    "                    self.loss_list.append(float(avg_loss))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = numpy.arange(len(self.loss_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.loss_list, label='train')\n",
    "        plt.xlabel('반복 (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('손실')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class RnnlmTrainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.time_idx = None\n",
    "        self.ppl_list = None\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def get_batch(self, x, t, batch_size, time_size):\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "\n",
    "        data_size = len(x)\n",
    "        jump = data_size // batch_size\n",
    "        offsets = [i * jump for i in range(batch_size)]  # 배치에서 각 샘플을 읽기 시작하는 위치\n",
    "\n",
    "        for time in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, time] = x[(offset + self.time_idx) % data_size]\n",
    "                batch_t[i, time] = t[(offset + self.time_idx) % data_size]\n",
    "            self.time_idx += 1\n",
    "        return batch_x, batch_t\n",
    "\n",
    "    def fit(self, xs, ts, max_epoch=10, batch_size=20, time_size=35,\n",
    "            max_grad=None, eval_interval=20):\n",
    "        data_size = len(xs)\n",
    "        max_iters = data_size // (batch_size * time_size)\n",
    "        self.time_idx = 0\n",
    "        self.ppl_list = []\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            for iters in range(max_iters):\n",
    "                batch_x, batch_t = self.get_batch(xs, ts, batch_size, time_size)\n",
    "\n",
    "                # 기울기를 구해 매개변수 갱신\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.params, model.grads)  # 공유된 가중치를 하나로 모음\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # 퍼플렉서티 평가\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    ppl = np.exp(total_loss / loss_count)\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print('| 에폭 %d |  반복 %d / %d | 시간 %d[s] | 퍼플렉서티 %.2f'\n",
    "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, ppl))\n",
    "                    self.ppl_list.append(float(ppl))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = numpy.arange(len(self.ppl_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.ppl_list, label='train')\n",
    "        plt.xlabel('반복 (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('퍼플렉서티')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def remove_duplicate(params, grads):\n",
    "    '''\n",
    "    매개변수 배열 중 중복되는 가중치를 하나로 모아\n",
    "    그 가중치에 대응하는 기울기를 더한다.\n",
    "    '''\n",
    "    params, grads = params[:], grads[:]  # copy list\n",
    "\n",
    "    while True:\n",
    "        find_flg = False\n",
    "        L = len(params)\n",
    "\n",
    "        for i in range(0, L - 1):\n",
    "            for j in range(i + 1, L):\n",
    "                # 가중치 공유 시\n",
    "                if params[i] is params[j]:\n",
    "                    grads[i] += grads[j]  # 경사를 더함\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                # 가중치를 전치행렬로 공유하는 경우(weight tying)\n",
    "                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
    "                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
    "                    grads[i] += grads[j].T\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "\n",
    "                if find_flg: break\n",
    "            if find_flg: break\n",
    "\n",
    "        if not find_flg: break\n",
    "\n",
    "    return params, grads\n",
    "\n",
    "\n",
    "#================================================================================#\n",
    "#util.py\n",
    "#================================================================================#\n",
    "\n",
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "#from common.np import *\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "\n",
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    '''코사인 유사도 산출\n",
    "\n",
    "    :param x: 벡터\n",
    "    :param y: 벡터\n",
    "    :param eps: '0으로 나누기'를 방지하기 위한 작은 값\n",
    "    :return:\n",
    "    '''\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
    "    return np.dot(nx, ny)\n",
    "\n",
    "\n",
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    '''유사 단어 검색\n",
    "\n",
    "    :param query: 쿼리(텍스트)\n",
    "    :param word_to_id: 단어에서 단어 ID로 변환하는 딕셔너리\n",
    "    :param id_to_word: 단어 ID에서 단어로 변환하는 딕셔너리\n",
    "    :param word_matrix: 단어 벡터를 정리한 행렬. 각 행에 해당 단어 벡터가 저장되어 있다고 가정한다.\n",
    "    :param top: 상위 몇 개까지 출력할 지 지정\n",
    "    '''\n",
    "    if query not in word_to_id:\n",
    "        print('%s(을)를 찾을 수 없습니다.' % query)\n",
    "        return\n",
    "\n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    vocab_size = len(id_to_word)\n",
    "\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "\n",
    "    # 코사인 유사도를 기준으로 내림차순으로 출력\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "\n",
    "\n",
    "def convert_one_hot(corpus, vocab_size):\n",
    "    '''원핫 표현으로 변환\n",
    "\n",
    "    :param corpus: 단어 ID 목록(1차원 또는 2차원 넘파이 배열)\n",
    "    :param vocab_size: 어휘 수\n",
    "    :return: 원핫 표현(2차원 또는 3차원 넘파이 배열)\n",
    "    '''\n",
    "    N = corpus.shape[0]\n",
    "\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    '''동시발생 행렬 생성\n",
    "\n",
    "    :param corpus: 말뭉치(단어 ID 목록)\n",
    "    :param vocab_size: 어휘 수\n",
    "    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n",
    "    :return: 동시발생 행렬\n",
    "    '''\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "\n",
    "    return co_matrix\n",
    "\n",
    "\n",
    "def ppmi(C, verbose=False, eps = 1e-8):\n",
    "    '''PPMI(점별 상호정보량) 생성\n",
    "\n",
    "    :param C: 동시발생 행렬\n",
    "    :param verbose: 진행 상황을 출력할지 여부\n",
    "    :return:\n",
    "    '''\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis=0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
    "            M[i, j] = max(0, pmi)\n",
    "\n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total//100) == 0:\n",
    "                    print('%.1f%% 완료' % (100*cnt/total))\n",
    "    return M\n",
    "\n",
    "\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    '''맥락과 타깃 생성\n",
    "\n",
    "    :param corpus: 말뭉치(단어 ID 목록)\n",
    "    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n",
    "    :return:\n",
    "    '''\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "\n",
    "    return np.array(contexts), np.array(target)\n",
    "\n",
    "\n",
    "def to_cpu(x):\n",
    "    import numpy\n",
    "    if type(x) == numpy.ndarray:\n",
    "        return x\n",
    "    return np.asnumpy(x)\n",
    "\n",
    "\n",
    "def to_gpu(x):\n",
    "    import cupy\n",
    "    if type(x) == cupy.ndarray:\n",
    "        return x\n",
    "    return cupy.asarray(x)\n",
    "\n",
    "\n",
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate\n",
    "\n",
    "\n",
    "def eval_perplexity(model, corpus, batch_size=10, time_size=35):\n",
    "    print('퍼플렉서티 평가 중 ...')\n",
    "    corpus_size = len(corpus)\n",
    "    total_loss, loss_cnt = 0, 0\n",
    "    max_iters = (corpus_size - 1) // (batch_size * time_size)\n",
    "    jump = (corpus_size - 1) // batch_size\n",
    "\n",
    "    for iters in range(max_iters):\n",
    "        xs = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "        ts = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "        time_offset = iters * time_size\n",
    "        offsets = [time_offset + (i * jump) for i in range(batch_size)]\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                xs[i, t] = corpus[(offset + t) % corpus_size]\n",
    "                ts[i, t] = corpus[(offset + t + 1) % corpus_size]\n",
    "\n",
    "        try:\n",
    "            loss = model.forward(xs, ts, train_flg=False)\n",
    "        except TypeError:\n",
    "            loss = model.forward(xs, ts)\n",
    "        total_loss += loss\n",
    "\n",
    "        sys.stdout.write('\\r%d / %d' % (iters, max_iters))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    print('')\n",
    "    ppl = np.exp(total_loss / max_iters)\n",
    "    return ppl\n",
    "\n",
    "\n",
    "def eval_seq2seq(model, question, correct, id_to_char,\n",
    "                 verbos=False, is_reverse=False):\n",
    "    correct = correct.flatten()\n",
    "    # 머릿글자\n",
    "    start_id = correct[0]\n",
    "    correct = correct[1:]\n",
    "    guess = model.generate(question, start_id, len(correct))\n",
    "\n",
    "    # 문자열로 변환\n",
    "    question = ''.join([id_to_char[int(c)] for c in question.flatten()])\n",
    "    correct = ''.join([id_to_char[int(c)] for c in correct])\n",
    "    guess = ''.join([id_to_char[int(c)] for c in guess])\n",
    "\n",
    "    if verbos:\n",
    "        if is_reverse:\n",
    "            question = question[::-1]\n",
    "\n",
    "        colors = {'ok': '\\033[92m', 'fail': '\\033[91m', 'close': '\\033[0m'}\n",
    "        print('Q', question)\n",
    "        print('T', correct)\n",
    "\n",
    "        is_windows = os.name == 'nt'\n",
    "\n",
    "        if correct == guess:\n",
    "            mark = colors['ok'] + '☑' + colors['close']\n",
    "            if is_windows:\n",
    "                mark = 'O'\n",
    "            print(mark + ' ' + guess)\n",
    "        else:\n",
    "            mark = colors['fail'] + '☒' + colors['close']\n",
    "            if is_windows:\n",
    "                mark = 'X'\n",
    "            print(mark + ' ' + guess)\n",
    "        print('---')\n",
    "\n",
    "    return 1 if guess == correct else 0\n",
    "\n",
    "\n",
    "def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):\n",
    "    for word in (a, b, c):\n",
    "        if word not in word_to_id:\n",
    "            print('%s(을)를 찾을 수 없습니다.' % word)\n",
    "            return\n",
    "\n",
    "    print('\\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')\n",
    "    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n",
    "    query_vec = b_vec - a_vec + c_vec\n",
    "    query_vec = normalize(query_vec)\n",
    "\n",
    "    similarity = np.dot(word_matrix, query_vec)\n",
    "\n",
    "    if answer is not None:\n",
    "        print(\"==>\" + answer + \":\" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))\n",
    "\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if np.isnan(similarity[i]):\n",
    "            continue\n",
    "        if id_to_word[i] in (a, b, c):\n",
    "            continue\n",
    "        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    if x.ndim == 2:\n",
    "        s = np.sqrt((x * x).sum(1))\n",
    "        x /= s.reshape((s.shape[0], 1))\n",
    "    elif x.ndim == 1:\n",
    "        s = np.sqrt((x * x).sum())\n",
    "        x /= s\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
