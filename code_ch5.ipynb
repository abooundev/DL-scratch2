{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple_rnnlm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "#from common.time_layers import *\n",
    "\n",
    "\n",
    "class SimpleRnnlm:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        # 가중치 초기화\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')\n",
    "        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
    "        rnn_b = np.zeros(H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.rnn_layer = self.layers[1]\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        loss = self.loss_layer.forward(xs, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.rnn_layer.reset_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 417.40\n",
      "| 에폭 2 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 388.67\n",
      "| 에폭 3 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 269.74\n",
      "| 에폭 4 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 221.38\n",
      "| 에폭 5 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 211.50\n",
      "| 에폭 6 |  반복 1 / 19 | 시간 1[s] | 퍼플렉서티 210.37\n",
      "| 에폭 7 |  반복 1 / 19 | 시간 1[s] | 퍼플렉서티 201.75\n",
      "| 에폭 8 |  반복 1 / 19 | 시간 1[s] | 퍼플렉서티 201.22\n",
      "| 에폭 9 |  반복 1 / 19 | 시간 1[s] | 퍼플렉서티 195.44\n",
      "| 에폭 10 |  반복 1 / 19 | 시간 2[s] | 퍼플렉서티 191.58\n",
      "| 에폭 11 |  반복 1 / 19 | 시간 3[s] | 퍼플렉서티 193.02\n",
      "| 에폭 12 |  반복 1 / 19 | 시간 3[s] | 퍼플렉서티 190.21\n",
      "| 에폭 13 |  반복 1 / 19 | 시간 3[s] | 퍼플렉서티 193.58\n",
      "| 에폭 14 |  반복 1 / 19 | 시간 4[s] | 퍼플렉서티 187.42\n",
      "| 에폭 15 |  반복 1 / 19 | 시간 4[s] | 퍼플렉서티 187.22\n",
      "| 에폭 16 |  반복 1 / 19 | 시간 5[s] | 퍼플렉서티 191.49\n",
      "| 에폭 17 |  반복 1 / 19 | 시간 6[s] | 퍼플렉서티 189.51\n",
      "| 에폭 18 |  반복 1 / 19 | 시간 6[s] | 퍼플렉서티 185.56\n",
      "| 에폭 19 |  반복 1 / 19 | 시간 6[s] | 퍼플렉서티 181.39\n",
      "| 에폭 20 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 182.44\n",
      "| 에폭 21 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 179.45\n",
      "| 에폭 22 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 178.03\n",
      "| 에폭 23 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 179.69\n",
      "| 에폭 24 |  반복 1 / 19 | 시간 8[s] | 퍼플렉서티 177.88\n",
      "| 에폭 25 |  반복 1 / 19 | 시간 8[s] | 퍼플렉서티 172.77\n",
      "| 에폭 26 |  반복 1 / 19 | 시간 8[s] | 퍼플렉서티 174.31\n",
      "| 에폭 27 |  반복 1 / 19 | 시간 8[s] | 퍼플렉서티 172.37\n",
      "| 에폭 28 |  반복 1 / 19 | 시간 9[s] | 퍼플렉서티 171.74\n",
      "| 에폭 29 |  반복 1 / 19 | 시간 9[s] | 퍼플렉서티 165.65\n",
      "| 에폭 30 |  반복 1 / 19 | 시간 9[s] | 퍼플렉서티 162.83\n",
      "| 에폭 31 |  반복 1 / 19 | 시간 10[s] | 퍼플렉서티 162.01\n",
      "| 에폭 32 |  반복 1 / 19 | 시간 10[s] | 퍼플렉서티 157.29\n",
      "| 에폭 33 |  반복 1 / 19 | 시간 10[s] | 퍼플렉서티 156.05\n",
      "| 에폭 34 |  반복 1 / 19 | 시간 10[s] | 퍼플렉서티 155.94\n",
      "| 에폭 35 |  반복 1 / 19 | 시간 11[s] | 퍼플렉서티 148.33\n",
      "| 에폭 36 |  반복 1 / 19 | 시간 11[s] | 퍼플렉서티 145.93\n",
      "| 에폭 37 |  반복 1 / 19 | 시간 11[s] | 퍼플렉서티 147.12\n",
      "| 에폭 38 |  반복 1 / 19 | 시간 12[s] | 퍼플렉서티 137.58\n",
      "| 에폭 39 |  반복 1 / 19 | 시간 12[s] | 퍼플렉서티 133.29\n",
      "| 에폭 40 |  반복 1 / 19 | 시간 12[s] | 퍼플렉서티 129.35\n",
      "| 에폭 41 |  반복 1 / 19 | 시간 12[s] | 퍼플렉서티 124.51\n",
      "| 에폭 42 |  반복 1 / 19 | 시간 13[s] | 퍼플렉서티 124.36\n",
      "| 에폭 43 |  반복 1 / 19 | 시간 13[s] | 퍼플렉서티 118.16\n",
      "| 에폭 44 |  반복 1 / 19 | 시간 13[s] | 퍼플렉서티 113.97\n",
      "| 에폭 45 |  반복 1 / 19 | 시간 14[s] | 퍼플렉서티 106.71\n",
      "| 에폭 46 |  반복 1 / 19 | 시간 14[s] | 퍼플렉서티 103.94\n",
      "| 에폭 47 |  반복 1 / 19 | 시간 14[s] | 퍼플렉서티 103.66\n",
      "| 에폭 48 |  반복 1 / 19 | 시간 14[s] | 퍼플렉서티 97.71\n",
      "| 에폭 49 |  반복 1 / 19 | 시간 15[s] | 퍼플렉서티 92.86\n",
      "| 에폭 50 |  반복 1 / 19 | 시간 15[s] | 퍼플렉서티 88.03\n",
      "| 에폭 51 |  반복 1 / 19 | 시간 15[s] | 퍼플렉서티 85.11\n",
      "| 에폭 52 |  반복 1 / 19 | 시간 15[s] | 퍼플렉서티 81.13\n",
      "| 에폭 53 |  반복 1 / 19 | 시간 15[s] | 퍼플렉서티 78.45\n",
      "| 에폭 54 |  반복 1 / 19 | 시간 16[s] | 퍼플렉서티 74.04\n",
      "| 에폭 55 |  반복 1 / 19 | 시간 16[s] | 퍼플렉서티 71.39\n",
      "| 에폭 56 |  반복 1 / 19 | 시간 16[s] | 퍼플렉서티 67.06\n",
      "| 에폭 57 |  반복 1 / 19 | 시간 16[s] | 퍼플렉서티 67.58\n",
      "| 에폭 58 |  반복 1 / 19 | 시간 17[s] | 퍼플렉서티 60.83\n",
      "| 에폭 59 |  반복 1 / 19 | 시간 17[s] | 퍼플렉서티 60.12\n",
      "| 에폭 60 |  반복 1 / 19 | 시간 17[s] | 퍼플렉서티 55.34\n",
      "| 에폭 61 |  반복 1 / 19 | 시간 17[s] | 퍼플렉서티 51.87\n",
      "| 에폭 62 |  반복 1 / 19 | 시간 17[s] | 퍼플렉서티 49.43\n",
      "| 에폭 63 |  반복 1 / 19 | 시간 17[s] | 퍼플렉서티 49.02\n",
      "| 에폭 64 |  반복 1 / 19 | 시간 18[s] | 퍼플렉서티 45.92\n",
      "| 에폭 65 |  반복 1 / 19 | 시간 18[s] | 퍼플렉서티 41.99\n",
      "| 에폭 66 |  반복 1 / 19 | 시간 18[s] | 퍼플렉서티 39.18\n",
      "| 에폭 67 |  반복 1 / 19 | 시간 19[s] | 퍼플렉서티 37.11\n",
      "| 에폭 68 |  반복 1 / 19 | 시간 19[s] | 퍼플렉서티 37.12\n",
      "| 에폭 69 |  반복 1 / 19 | 시간 19[s] | 퍼플렉서티 34.00\n",
      "| 에폭 70 |  반복 1 / 19 | 시간 19[s] | 퍼플렉서티 31.95\n",
      "| 에폭 71 |  반복 1 / 19 | 시간 20[s] | 퍼플렉서티 29.78\n",
      "| 에폭 72 |  반복 1 / 19 | 시간 21[s] | 퍼플렉서티 28.03\n",
      "| 에폭 73 |  반복 1 / 19 | 시간 22[s] | 퍼플렉서티 27.81\n",
      "| 에폭 74 |  반복 1 / 19 | 시간 23[s] | 퍼플렉서티 25.21\n",
      "| 에폭 75 |  반복 1 / 19 | 시간 24[s] | 퍼플렉서티 24.42\n",
      "| 에폭 76 |  반복 1 / 19 | 시간 25[s] | 퍼플렉서티 22.28\n",
      "| 에폭 77 |  반복 1 / 19 | 시간 25[s] | 퍼플렉서티 21.84\n",
      "| 에폭 78 |  반복 1 / 19 | 시간 27[s] | 퍼플렉서티 20.99\n",
      "| 에폭 79 |  반복 1 / 19 | 시간 27[s] | 퍼플렉서티 18.94\n",
      "| 에폭 80 |  반복 1 / 19 | 시간 27[s] | 퍼플렉서티 17.64\n",
      "| 에폭 81 |  반복 1 / 19 | 시간 27[s] | 퍼플렉서티 16.60\n",
      "| 에폭 82 |  반복 1 / 19 | 시간 28[s] | 퍼플렉서티 17.15\n",
      "| 에폭 83 |  반복 1 / 19 | 시간 29[s] | 퍼플렉서티 16.31\n",
      "| 에폭 84 |  반복 1 / 19 | 시간 29[s] | 퍼플렉서티 15.28\n",
      "| 에폭 85 |  반복 1 / 19 | 시간 29[s] | 퍼플렉서티 13.64\n",
      "| 에폭 86 |  반복 1 / 19 | 시간 29[s] | 퍼플렉서티 13.32\n",
      "| 에폭 87 |  반복 1 / 19 | 시간 30[s] | 퍼플렉서티 12.73\n",
      "| 에폭 88 |  반복 1 / 19 | 시간 30[s] | 퍼플렉서티 11.60\n",
      "| 에폭 89 |  반복 1 / 19 | 시간 30[s] | 퍼플렉서티 11.32\n",
      "| 에폭 90 |  반복 1 / 19 | 시간 31[s] | 퍼플렉서티 10.35\n",
      "| 에폭 91 |  반복 1 / 19 | 시간 31[s] | 퍼플렉서티 10.10\n",
      "| 에폭 92 |  반복 1 / 19 | 시간 32[s] | 퍼플렉서티 9.75\n",
      "| 에폭 93 |  반복 1 / 19 | 시간 32[s] | 퍼플렉서티 9.44\n",
      "| 에폭 94 |  반복 1 / 19 | 시간 32[s] | 퍼플렉서티 8.23\n",
      "| 에폭 95 |  반복 1 / 19 | 시간 32[s] | 퍼플렉서티 7.90\n",
      "| 에폭 96 |  반복 1 / 19 | 시간 33[s] | 퍼플렉서티 7.98\n",
      "| 에폭 97 |  반복 1 / 19 | 시간 33[s] | 퍼플렉서티 7.10\n",
      "| 에폭 98 |  반복 1 / 19 | 시간 33[s] | 퍼플렉서티 6.94\n",
      "| 에폭 99 |  반복 1 / 19 | 시간 33[s] | 퍼플렉서티 6.79\n",
      "| 에폭 100 |  반복 1 / 19 | 시간 34[s] | 퍼플렉서티 6.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 48152 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 48373 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 54140 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 54540 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 47113 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 49436 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 54000 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 48152 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 48373 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 54140 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 54540 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 47113 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 49436 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/Users/csg/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 54000 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "#from common.optimizer import SGD\n",
    "#from common.trainer import RnnlmTrainer\n",
    "from dataset import ptb\n",
    "#from simple_rnnlm import SimpleRnnlm\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 10\n",
    "wordvec_size = 100\n",
    "hidden_size = 100  # RNN의 은닉 상태 벡터의 원소 수\n",
    "time_size = 5  # RNN을 펼치는 크기\n",
    "lr = 0.1\n",
    "max_epoch = 100\n",
    "\n",
    "# 학습 데이터 읽기\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_size = 1000  # 테스트 데이터셋을 작게 설정\n",
    "corpus = corpus[:corpus_size]\n",
    "vocab_size = int(max(corpus) + 1)\n",
    "xs = corpus[:-1]  # 입력\n",
    "ts = corpus[1:]  # 출력（정답 레이블）\n",
    "\n",
    "# 모델 생성\n",
    "model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)\n",
    "trainer = RnnlmTrainer(model, optimizer)\n",
    "\n",
    "trainer.fit(xs, ts, max_epoch, batch_size, time_size)\n",
    "trainer.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n| 에폭 1 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 417.40\\n| 에폭 2 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 388.67\\n| 에폭 3 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 269.74\\n| 에폭 4 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 221.38\\n| 에폭 5 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 211.50\\n| 에폭 6 |  반복 1 / 19 | 시간 1[s] | 퍼플렉서티 210.37\\n| 에폭 7 |  반복 1 / 19 | 시간 1[s] | 퍼플렉서티 201.75\\n| 에폭 8 |  반복 1 / 19 | 시간 1[s] | 퍼플렉서티 201.22\\n| 에폭 9 |  반복 1 / 19 | 시간 1[s] | 퍼플렉서티 195.44\\n| 에폭 10 |  반복 1 / 19 | 시간 2[s] | 퍼플렉서티 191.58\\n| 에폭 11 |  반복 1 / 19 | 시간 3[s] | 퍼플렉서티 193.02\\n| 에폭 12 |  반복 1 / 19 | 시간 3[s] | 퍼플렉서티 190.21\\n| 에폭 13 |  반복 1 / 19 | 시간 3[s] | 퍼플렉서티 193.58\\n| 에폭 14 |  반복 1 / 19 | 시간 4[s] | 퍼플렉서티 187.42\\n| 에폭 15 |  반복 1 / 19 | 시간 4[s] | 퍼플렉서티 187.22\\n| 에폭 16 |  반복 1 / 19 | 시간 5[s] | 퍼플렉서티 191.49\\n| 에폭 17 |  반복 1 / 19 | 시간 6[s] | 퍼플렉서티 189.51\\n| 에폭 18 |  반복 1 / 19 | 시간 6[s] | 퍼플렉서티 185.56\\n| 에폭 19 |  반복 1 / 19 | 시간 6[s] | 퍼플렉서티 181.39\\n| 에폭 20 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 182.44\\n| 에폭 21 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 179.45\\n| 에폭 22 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 178.03\\n| 에폭 23 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 179.69\\n| 에폭 24 |  반복 1 / 19 | 시간 8[s] | 퍼플렉서티 177.88\\n| 에폭 25 |  반복 1 / 19 | 시간 8[s] | 퍼플렉서티 172.77\\n| 에폭 26 |  반복 1 / 19 | 시간 8[s] | 퍼플렉서티 174.31\\n| 에폭 27 |  반복 1 / 19 | 시간 8[s] | 퍼플렉서티 172.37\\n| 에폭 28 |  반복 1 / 19 | 시간 9[s] | 퍼플렉서티 171.74\\n| 에폭 29 |  반복 1 / 19 | 시간 9[s] | 퍼플렉서티 165.65\\n| 에폭 30 |  반복 1 / 19 | 시간 9[s] | 퍼플렉서티 162.83\\n| 에폭 31 |  반복 1 / 19 | 시간 10[s] | 퍼플렉서티 162.01\\n| 에폭 32 |  반복 1 / 19 | 시간 10[s] | 퍼플렉서티 157.29\\n| 에폭 33 |  반복 1 / 19 | 시간 10[s] | 퍼플렉서티 156.05\\n| 에폭 34 |  반복 1 / 19 | 시간 10[s] | 퍼플렉서티 155.94\\n| 에폭 35 |  반복 1 / 19 | 시간 11[s] | 퍼플렉서티 148.33\\n| 에폭 36 |  반복 1 / 19 | 시간 11[s] | 퍼플렉서티 145.93\\n| 에폭 37 |  반복 1 / 19 | 시간 11[s] | 퍼플렉서티 147.12\\n| 에폭 38 |  반복 1 / 19 | 시간 12[s] | 퍼플렉서티 137.58\\n| 에폭 39 |  반복 1 / 19 | 시간 12[s] | 퍼플렉서티 133.29\\n| 에폭 40 |  반복 1 / 19 | 시간 12[s] | 퍼플렉서티 129.35\\n| 에폭 41 |  반복 1 / 19 | 시간 12[s] | 퍼플렉서티 124.51\\n| 에폭 42 |  반복 1 / 19 | 시간 13[s] | 퍼플렉서티 124.36\\n| 에폭 43 |  반복 1 / 19 | 시간 13[s] | 퍼플렉서티 118.16\\n| 에폭 44 |  반복 1 / 19 | 시간 13[s] | 퍼플렉서티 113.97\\n| 에폭 45 |  반복 1 / 19 | 시간 14[s] | 퍼플렉서티 106.71\\n| 에폭 46 |  반복 1 / 19 | 시간 14[s] | 퍼플렉서티 103.94\\n| 에폭 47 |  반복 1 / 19 | 시간 14[s] | 퍼플렉서티 103.66\\n| 에폭 48 |  반복 1 / 19 | 시간 14[s] | 퍼플렉서티 97.71\\n| 에폭 49 |  반복 1 / 19 | 시간 15[s] | 퍼플렉서티 92.86\\n| 에폭 50 |  반복 1 / 19 | 시간 15[s] | 퍼플렉서티 88.03\\n| 에폭 51 |  반복 1 / 19 | 시간 15[s] | 퍼플렉서티 85.11\\n| 에폭 52 |  반복 1 / 19 | 시간 15[s] | 퍼플렉서티 81.13\\n| 에폭 53 |  반복 1 / 19 | 시간 15[s] | 퍼플렉서티 78.45\\n| 에폭 54 |  반복 1 / 19 | 시간 16[s] | 퍼플렉서티 74.04\\n| 에폭 55 |  반복 1 / 19 | 시간 16[s] | 퍼플렉서티 71.39\\n| 에폭 56 |  반복 1 / 19 | 시간 16[s] | 퍼플렉서티 67.06\\n| 에폭 57 |  반복 1 / 19 | 시간 16[s] | 퍼플렉서티 67.58\\n| 에폭 58 |  반복 1 / 19 | 시간 17[s] | 퍼플렉서티 60.83\\n| 에폭 59 |  반복 1 / 19 | 시간 17[s] | 퍼플렉서티 60.12\\n| 에폭 60 |  반복 1 / 19 | 시간 17[s] | 퍼플렉서티 55.34\\n| 에폭 61 |  반복 1 / 19 | 시간 17[s] | 퍼플렉서티 51.87\\n| 에폭 62 |  반복 1 / 19 | 시간 17[s] | 퍼플렉서티 49.43\\n| 에폭 63 |  반복 1 / 19 | 시간 17[s] | 퍼플렉서티 49.02\\n| 에폭 64 |  반복 1 / 19 | 시간 18[s] | 퍼플렉서티 45.92\\n| 에폭 65 |  반복 1 / 19 | 시간 18[s] | 퍼플렉서티 41.99\\n| 에폭 66 |  반복 1 / 19 | 시간 18[s] | 퍼플렉서티 39.18\\n| 에폭 67 |  반복 1 / 19 | 시간 19[s] | 퍼플렉서티 37.11\\n| 에폭 68 |  반복 1 / 19 | 시간 19[s] | 퍼플렉서티 37.12\\n| 에폭 69 |  반복 1 / 19 | 시간 19[s] | 퍼플렉서티 34.00\\n| 에폭 70 |  반복 1 / 19 | 시간 19[s] | 퍼플렉서티 31.95\\n| 에폭 71 |  반복 1 / 19 | 시간 20[s] | 퍼플렉서티 29.78\\n| 에폭 72 |  반복 1 / 19 | 시간 21[s] | 퍼플렉서티 28.03\\n| 에폭 73 |  반복 1 / 19 | 시간 22[s] | 퍼플렉서티 27.81\\n| 에폭 74 |  반복 1 / 19 | 시간 23[s] | 퍼플렉서티 25.21\\n| 에폭 75 |  반복 1 / 19 | 시간 24[s] | 퍼플렉서티 24.42\\n| 에폭 76 |  반복 1 / 19 | 시간 25[s] | 퍼플렉서티 22.28\\n| 에폭 77 |  반복 1 / 19 | 시간 25[s] | 퍼플렉서티 21.84\\n| 에폭 78 |  반복 1 / 19 | 시간 27[s] | 퍼플렉서티 20.99\\n| 에폭 79 |  반복 1 / 19 | 시간 27[s] | 퍼플렉서티 18.94\\n| 에폭 80 |  반복 1 / 19 | 시간 27[s] | 퍼플렉서티 17.64\\n| 에폭 81 |  반복 1 / 19 | 시간 27[s] | 퍼플렉서티 16.60\\n| 에폭 82 |  반복 1 / 19 | 시간 28[s] | 퍼플렉서티 17.15\\n| 에폭 83 |  반복 1 / 19 | 시간 29[s] | 퍼플렉서티 16.31\\n| 에폭 84 |  반복 1 / 19 | 시간 29[s] | 퍼플렉서티 15.28\\n| 에폭 85 |  반복 1 / 19 | 시간 29[s] | 퍼플렉서티 13.64\\n| 에폭 86 |  반복 1 / 19 | 시간 29[s] | 퍼플렉서티 13.32\\n| 에폭 87 |  반복 1 / 19 | 시간 30[s] | 퍼플렉서티 12.73\\n| 에폭 88 |  반복 1 / 19 | 시간 30[s] | 퍼플렉서티 11.60\\n| 에폭 89 |  반복 1 / 19 | 시간 30[s] | 퍼플렉서티 11.32\\n| 에폭 90 |  반복 1 / 19 | 시간 31[s] | 퍼플렉서티 10.35\\n| 에폭 91 |  반복 1 / 19 | 시간 31[s] | 퍼플렉서티 10.10\\n| 에폭 92 |  반복 1 / 19 | 시간 32[s] | 퍼플렉서티 9.75\\n| 에폭 93 |  반복 1 / 19 | 시간 32[s] | 퍼플렉서티 9.44\\n| 에폭 94 |  반복 1 / 19 | 시간 32[s] | 퍼플렉서티 8.23\\n| 에폭 95 |  반복 1 / 19 | 시간 32[s] | 퍼플렉서티 7.90\\n| 에폭 96 |  반복 1 / 19 | 시간 33[s] | 퍼플렉서티 7.98\\n| 에폭 97 |  반복 1 / 19 | 시간 33[s] | 퍼플렉서티 7.10\\n| 에폭 98 |  반복 1 / 19 | 시간 33[s] | 퍼플렉서티 6.94\\n| 에폭 99 |  반복 1 / 19 | 시간 33[s] | 퍼플렉서티 6.79\\n| 에폭 100 |  반복 1 / 19 | 시간 34[s] | 퍼플렉서티 6.65\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "| 에폭 1 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 417.40\n",
    "| 에폭 2 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 388.67\n",
    "| 에폭 3 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 269.74\n",
    "| 에폭 4 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 221.38\n",
    "| 에폭 5 |  반복 1 / 19 | 시간 0[s] | 퍼플렉서티 211.50\n",
    "| 에폭 6 |  반복 1 / 19 | 시간 1[s] | 퍼플렉서티 210.37\n",
    "| 에폭 7 |  반복 1 / 19 | 시간 1[s] | 퍼플렉서티 201.75\n",
    "| 에폭 8 |  반복 1 / 19 | 시간 1[s] | 퍼플렉서티 201.22\n",
    "| 에폭 9 |  반복 1 / 19 | 시간 1[s] | 퍼플렉서티 195.44\n",
    "| 에폭 10 |  반복 1 / 19 | 시간 2[s] | 퍼플렉서티 191.58\n",
    "| 에폭 11 |  반복 1 / 19 | 시간 3[s] | 퍼플렉서티 193.02\n",
    "| 에폭 12 |  반복 1 / 19 | 시간 3[s] | 퍼플렉서티 190.21\n",
    "| 에폭 13 |  반복 1 / 19 | 시간 3[s] | 퍼플렉서티 193.58\n",
    "| 에폭 14 |  반복 1 / 19 | 시간 4[s] | 퍼플렉서티 187.42\n",
    "| 에폭 15 |  반복 1 / 19 | 시간 4[s] | 퍼플렉서티 187.22\n",
    "| 에폭 16 |  반복 1 / 19 | 시간 5[s] | 퍼플렉서티 191.49\n",
    "| 에폭 17 |  반복 1 / 19 | 시간 6[s] | 퍼플렉서티 189.51\n",
    "| 에폭 18 |  반복 1 / 19 | 시간 6[s] | 퍼플렉서티 185.56\n",
    "| 에폭 19 |  반복 1 / 19 | 시간 6[s] | 퍼플렉서티 181.39\n",
    "| 에폭 20 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 182.44\n",
    "| 에폭 21 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 179.45\n",
    "| 에폭 22 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 178.03\n",
    "| 에폭 23 |  반복 1 / 19 | 시간 7[s] | 퍼플렉서티 179.69\n",
    "| 에폭 24 |  반복 1 / 19 | 시간 8[s] | 퍼플렉서티 177.88\n",
    "| 에폭 25 |  반복 1 / 19 | 시간 8[s] | 퍼플렉서티 172.77\n",
    "| 에폭 26 |  반복 1 / 19 | 시간 8[s] | 퍼플렉서티 174.31\n",
    "| 에폭 27 |  반복 1 / 19 | 시간 8[s] | 퍼플렉서티 172.37\n",
    "| 에폭 28 |  반복 1 / 19 | 시간 9[s] | 퍼플렉서티 171.74\n",
    "| 에폭 29 |  반복 1 / 19 | 시간 9[s] | 퍼플렉서티 165.65\n",
    "| 에폭 30 |  반복 1 / 19 | 시간 9[s] | 퍼플렉서티 162.83\n",
    "| 에폭 31 |  반복 1 / 19 | 시간 10[s] | 퍼플렉서티 162.01\n",
    "| 에폭 32 |  반복 1 / 19 | 시간 10[s] | 퍼플렉서티 157.29\n",
    "| 에폭 33 |  반복 1 / 19 | 시간 10[s] | 퍼플렉서티 156.05\n",
    "| 에폭 34 |  반복 1 / 19 | 시간 10[s] | 퍼플렉서티 155.94\n",
    "| 에폭 35 |  반복 1 / 19 | 시간 11[s] | 퍼플렉서티 148.33\n",
    "| 에폭 36 |  반복 1 / 19 | 시간 11[s] | 퍼플렉서티 145.93\n",
    "| 에폭 37 |  반복 1 / 19 | 시간 11[s] | 퍼플렉서티 147.12\n",
    "| 에폭 38 |  반복 1 / 19 | 시간 12[s] | 퍼플렉서티 137.58\n",
    "| 에폭 39 |  반복 1 / 19 | 시간 12[s] | 퍼플렉서티 133.29\n",
    "| 에폭 40 |  반복 1 / 19 | 시간 12[s] | 퍼플렉서티 129.35\n",
    "| 에폭 41 |  반복 1 / 19 | 시간 12[s] | 퍼플렉서티 124.51\n",
    "| 에폭 42 |  반복 1 / 19 | 시간 13[s] | 퍼플렉서티 124.36\n",
    "| 에폭 43 |  반복 1 / 19 | 시간 13[s] | 퍼플렉서티 118.16\n",
    "| 에폭 44 |  반복 1 / 19 | 시간 13[s] | 퍼플렉서티 113.97\n",
    "| 에폭 45 |  반복 1 / 19 | 시간 14[s] | 퍼플렉서티 106.71\n",
    "| 에폭 46 |  반복 1 / 19 | 시간 14[s] | 퍼플렉서티 103.94\n",
    "| 에폭 47 |  반복 1 / 19 | 시간 14[s] | 퍼플렉서티 103.66\n",
    "| 에폭 48 |  반복 1 / 19 | 시간 14[s] | 퍼플렉서티 97.71\n",
    "| 에폭 49 |  반복 1 / 19 | 시간 15[s] | 퍼플렉서티 92.86\n",
    "| 에폭 50 |  반복 1 / 19 | 시간 15[s] | 퍼플렉서티 88.03\n",
    "| 에폭 51 |  반복 1 / 19 | 시간 15[s] | 퍼플렉서티 85.11\n",
    "| 에폭 52 |  반복 1 / 19 | 시간 15[s] | 퍼플렉서티 81.13\n",
    "| 에폭 53 |  반복 1 / 19 | 시간 15[s] | 퍼플렉서티 78.45\n",
    "| 에폭 54 |  반복 1 / 19 | 시간 16[s] | 퍼플렉서티 74.04\n",
    "| 에폭 55 |  반복 1 / 19 | 시간 16[s] | 퍼플렉서티 71.39\n",
    "| 에폭 56 |  반복 1 / 19 | 시간 16[s] | 퍼플렉서티 67.06\n",
    "| 에폭 57 |  반복 1 / 19 | 시간 16[s] | 퍼플렉서티 67.58\n",
    "| 에폭 58 |  반복 1 / 19 | 시간 17[s] | 퍼플렉서티 60.83\n",
    "| 에폭 59 |  반복 1 / 19 | 시간 17[s] | 퍼플렉서티 60.12\n",
    "| 에폭 60 |  반복 1 / 19 | 시간 17[s] | 퍼플렉서티 55.34\n",
    "| 에폭 61 |  반복 1 / 19 | 시간 17[s] | 퍼플렉서티 51.87\n",
    "| 에폭 62 |  반복 1 / 19 | 시간 17[s] | 퍼플렉서티 49.43\n",
    "| 에폭 63 |  반복 1 / 19 | 시간 17[s] | 퍼플렉서티 49.02\n",
    "| 에폭 64 |  반복 1 / 19 | 시간 18[s] | 퍼플렉서티 45.92\n",
    "| 에폭 65 |  반복 1 / 19 | 시간 18[s] | 퍼플렉서티 41.99\n",
    "| 에폭 66 |  반복 1 / 19 | 시간 18[s] | 퍼플렉서티 39.18\n",
    "| 에폭 67 |  반복 1 / 19 | 시간 19[s] | 퍼플렉서티 37.11\n",
    "| 에폭 68 |  반복 1 / 19 | 시간 19[s] | 퍼플렉서티 37.12\n",
    "| 에폭 69 |  반복 1 / 19 | 시간 19[s] | 퍼플렉서티 34.00\n",
    "| 에폭 70 |  반복 1 / 19 | 시간 19[s] | 퍼플렉서티 31.95\n",
    "| 에폭 71 |  반복 1 / 19 | 시간 20[s] | 퍼플렉서티 29.78\n",
    "| 에폭 72 |  반복 1 / 19 | 시간 21[s] | 퍼플렉서티 28.03\n",
    "| 에폭 73 |  반복 1 / 19 | 시간 22[s] | 퍼플렉서티 27.81\n",
    "| 에폭 74 |  반복 1 / 19 | 시간 23[s] | 퍼플렉서티 25.21\n",
    "| 에폭 75 |  반복 1 / 19 | 시간 24[s] | 퍼플렉서티 24.42\n",
    "| 에폭 76 |  반복 1 / 19 | 시간 25[s] | 퍼플렉서티 22.28\n",
    "| 에폭 77 |  반복 1 / 19 | 시간 25[s] | 퍼플렉서티 21.84\n",
    "| 에폭 78 |  반복 1 / 19 | 시간 27[s] | 퍼플렉서티 20.99\n",
    "| 에폭 79 |  반복 1 / 19 | 시간 27[s] | 퍼플렉서티 18.94\n",
    "| 에폭 80 |  반복 1 / 19 | 시간 27[s] | 퍼플렉서티 17.64\n",
    "| 에폭 81 |  반복 1 / 19 | 시간 27[s] | 퍼플렉서티 16.60\n",
    "| 에폭 82 |  반복 1 / 19 | 시간 28[s] | 퍼플렉서티 17.15\n",
    "| 에폭 83 |  반복 1 / 19 | 시간 29[s] | 퍼플렉서티 16.31\n",
    "| 에폭 84 |  반복 1 / 19 | 시간 29[s] | 퍼플렉서티 15.28\n",
    "| 에폭 85 |  반복 1 / 19 | 시간 29[s] | 퍼플렉서티 13.64\n",
    "| 에폭 86 |  반복 1 / 19 | 시간 29[s] | 퍼플렉서티 13.32\n",
    "| 에폭 87 |  반복 1 / 19 | 시간 30[s] | 퍼플렉서티 12.73\n",
    "| 에폭 88 |  반복 1 / 19 | 시간 30[s] | 퍼플렉서티 11.60\n",
    "| 에폭 89 |  반복 1 / 19 | 시간 30[s] | 퍼플렉서티 11.32\n",
    "| 에폭 90 |  반복 1 / 19 | 시간 31[s] | 퍼플렉서티 10.35\n",
    "| 에폭 91 |  반복 1 / 19 | 시간 31[s] | 퍼플렉서티 10.10\n",
    "| 에폭 92 |  반복 1 / 19 | 시간 32[s] | 퍼플렉서티 9.75\n",
    "| 에폭 93 |  반복 1 / 19 | 시간 32[s] | 퍼플렉서티 9.44\n",
    "| 에폭 94 |  반복 1 / 19 | 시간 32[s] | 퍼플렉서티 8.23\n",
    "| 에폭 95 |  반복 1 / 19 | 시간 32[s] | 퍼플렉서티 7.90\n",
    "| 에폭 96 |  반복 1 / 19 | 시간 33[s] | 퍼플렉서티 7.98\n",
    "| 에폭 97 |  반복 1 / 19 | 시간 33[s] | 퍼플렉서티 7.10\n",
    "| 에폭 98 |  반복 1 / 19 | 시간 33[s] | 퍼플렉서티 6.94\n",
    "| 에폭 99 |  반복 1 / 19 | 시간 33[s] | 퍼플렉서티 6.79\n",
    "| 에폭 100 |  반복 1 / 19 | 시간 34[s] | 퍼플렉서티 6.65\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_custom_loop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "말뭉치 크기: 1000, 어휘 수: 418\n",
      "| 에폭 1 | 퍼플렉서티 392.73\n",
      "| 에폭 2 | 퍼플렉서티 272.01\n",
      "| 에폭 3 | 퍼플렉서티 229.42\n",
      "| 에폭 4 | 퍼플렉서티 218.85\n",
      "| 에폭 5 | 퍼플렉서티 207.97\n",
      "| 에폭 6 | 퍼플렉서티 204.74\n",
      "| 에폭 7 | 퍼플렉서티 200.02\n",
      "| 에폭 8 | 퍼플렉서티 197.43\n",
      "| 에폭 9 | 퍼플렉서티 192.05\n",
      "| 에폭 10 | 퍼플렉서티 192.27\n",
      "| 에폭 11 | 퍼플렉서티 189.15\n",
      "| 에폭 12 | 퍼플렉서티 192.33\n",
      "| 에폭 13 | 퍼플렉서티 189.11\n",
      "| 에폭 14 | 퍼플렉서티 189.45\n",
      "| 에폭 15 | 퍼플렉서티 188.09\n",
      "| 에폭 16 | 퍼플렉서티 184.72\n",
      "| 에폭 17 | 퍼플렉서티 183.09\n",
      "| 에폭 18 | 퍼플렉서티 180.70\n",
      "| 에폭 19 | 퍼플렉서티 181.07\n",
      "| 에폭 20 | 퍼플렉서티 183.36\n",
      "| 에폭 21 | 퍼플렉서티 180.19\n",
      "| 에폭 22 | 퍼플렉서티 175.71\n",
      "| 에폭 23 | 퍼플렉서티 174.92\n",
      "| 에폭 24 | 퍼플렉서티 173.96\n",
      "| 에폭 25 | 퍼플렉서티 172.74\n",
      "| 에폭 26 | 퍼플렉서티 170.59\n",
      "| 에폭 27 | 퍼플렉서티 166.03\n",
      "| 에폭 28 | 퍼플렉서티 163.82\n",
      "| 에폭 29 | 퍼플렉서티 163.78\n",
      "| 에폭 30 | 퍼플렉서티 155.20\n",
      "| 에폭 31 | 퍼플렉서티 157.31\n",
      "| 에폭 32 | 퍼플렉서티 152.22\n",
      "| 에폭 33 | 퍼플렉서티 150.69\n",
      "| 에폭 34 | 퍼플렉서티 144.60\n",
      "| 에폭 35 | 퍼플렉서티 143.27\n",
      "| 에폭 36 | 퍼플렉서티 136.49\n",
      "| 에폭 37 | 퍼플렉서티 132.10\n",
      "| 에폭 38 | 퍼플렉서티 127.76\n",
      "| 에폭 39 | 퍼플렉서티 121.69\n",
      "| 에폭 40 | 퍼플렉서티 118.90\n",
      "| 에폭 41 | 퍼플렉서티 117.72\n",
      "| 에폭 42 | 퍼플렉서티 111.00\n",
      "| 에폭 43 | 퍼플렉서티 106.07\n",
      "| 에폭 44 | 퍼플렉서티 100.92\n",
      "| 에폭 45 | 퍼플렉서티 97.40\n",
      "| 에폭 46 | 퍼플렉서티 98.17\n",
      "| 에폭 47 | 퍼플렉서티 90.00\n",
      "| 에폭 48 | 퍼플렉서티 85.24\n",
      "| 에폭 49 | 퍼플렉서티 80.44\n",
      "| 에폭 50 | 퍼플렉서티 78.16\n",
      "| 에폭 51 | 퍼플렉서티 74.41\n",
      "| 에폭 52 | 퍼플렉서티 71.32\n",
      "| 에폭 53 | 퍼플렉서티 66.89\n",
      "| 에폭 54 | 퍼플렉서티 63.88\n",
      "| 에폭 55 | 퍼플렉서티 62.11\n",
      "| 에폭 56 | 퍼플렉서티 58.80\n",
      "| 에폭 57 | 퍼플렉서티 54.57\n",
      "| 에폭 58 | 퍼플렉서티 51.36\n",
      "| 에폭 59 | 퍼플렉서티 49.48\n",
      "| 에폭 60 | 퍼플렉서티 46.73\n",
      "| 에폭 61 | 퍼플렉서티 45.87\n",
      "| 에폭 62 | 퍼플렉서티 42.12\n",
      "| 에폭 63 | 퍼플렉서티 39.67\n",
      "| 에폭 64 | 퍼플렉서티 36.73\n",
      "| 에폭 65 | 퍼플렉서티 36.31\n",
      "| 에폭 66 | 퍼플렉서티 33.56\n",
      "| 에폭 67 | 퍼플렉서티 32.45\n",
      "| 에폭 68 | 퍼플렉서티 29.80\n",
      "| 에폭 69 | 퍼플렉서티 27.86\n",
      "| 에폭 70 | 퍼플렉서티 26.77\n",
      "| 에폭 71 | 퍼플렉서티 25.17\n",
      "| 에폭 72 | 퍼플렉서티 24.48\n",
      "| 에폭 73 | 퍼플렉서티 23.01\n",
      "| 에폭 74 | 퍼플렉서티 21.05\n",
      "| 에폭 75 | 퍼플렉서티 20.48\n",
      "| 에폭 76 | 퍼플렉서티 19.20\n",
      "| 에폭 77 | 퍼플렉서티 17.84\n",
      "| 에폭 78 | 퍼플렉서티 17.24\n",
      "| 에폭 79 | 퍼플렉서티 15.69\n",
      "| 에폭 80 | 퍼플렉서티 15.01\n",
      "| 에폭 81 | 퍼플렉서티 14.29\n",
      "| 에폭 82 | 퍼플렉서티 14.06\n",
      "| 에폭 83 | 퍼플렉서티 12.53\n",
      "| 에폭 84 | 퍼플렉서티 12.42\n",
      "| 에폭 85 | 퍼플렉서티 12.05\n",
      "| 에폭 86 | 퍼플렉서티 11.45\n",
      "| 에폭 87 | 퍼플렉서티 10.68\n",
      "| 에폭 88 | 퍼플렉서티 9.98\n",
      "| 에폭 89 | 퍼플렉서티 9.60\n",
      "| 에폭 90 | 퍼플렉서티 9.22\n",
      "| 에폭 91 | 퍼플렉서티 8.73\n",
      "| 에폭 92 | 퍼플렉서티 8.49\n",
      "| 에폭 93 | 퍼플렉서티 8.27\n",
      "| 에폭 94 | 퍼플렉서티 7.63\n",
      "| 에폭 95 | 퍼플렉서티 7.32\n",
      "| 에폭 96 | 퍼플렉서티 6.79\n",
      "| 에폭 97 | 퍼플렉서티 6.35\n",
      "| 에폭 98 | 퍼플렉서티 5.91\n",
      "| 에폭 99 | 퍼플렉서티 5.87\n",
      "| 에폭 100 | 퍼플렉서티 5.56\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#from common.optimizer import SGD\n",
    "from dataset import ptb\n",
    "#from simple_rnnlm import SimpleRnnlm\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "batch_size = 10\n",
    "wordvec_size = 100\n",
    "hidden_size = 100 # RNN의 은닉 상태 벡터의 원소 수\n",
    "time_size = 5     # Truncated BPTT가 한 번에 펼치는 시간 크기\n",
    "lr = 0.1\n",
    "max_epoch = 100\n",
    "\n",
    "# 학습 데이터 읽기(전체 중 1000개만)\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_size = 1000\n",
    "corpus = corpus[:corpus_size]\n",
    "vocab_size = int(max(corpus) + 1)\n",
    "\n",
    "xs = corpus[:-1]  # 입력\n",
    "ts = corpus[1:]   # 출력(정답 레이블)\n",
    "data_size = len(xs)\n",
    "print('말뭉치 크기: %d, 어휘 수: %d' % (corpus_size, vocab_size))\n",
    "\n",
    "# 학습 시 사용하는 변수\n",
    "max_iters = data_size // (batch_size * time_size)\n",
    "time_idx = 0\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "ppl_list = []\n",
    "\n",
    "# 모델 생성\n",
    "model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)\n",
    "\n",
    "# 미니배치의 각 샘플의 읽기 시작 위치를 계산\n",
    "jump = (corpus_size - 1) // batch_size\n",
    "offsets = [i * jump for i in range(batch_size)]\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for iter in range(max_iters):\n",
    "        # 미니배치 취득\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
    "                batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
    "            time_idx += 1\n",
    "\n",
    "        # 기울기를 구하여 매개변수 갱신\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        optimizer.update(model.params, model.grads)\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "\n",
    "    # 에폭마다 퍼플렉서티 평가\n",
    "    ppl = np.exp(total_loss / loss_count)\n",
    "    print('| 에폭 %d | 퍼플렉서티 %.2f'\n",
    "          % (epoch+1, ppl))\n",
    "    ppl_list.append(float(ppl))\n",
    "    total_loss, loss_count = 0, 0\n",
    "\n",
    "# 그래프 그리기\n",
    "x = np.arange(len(ppl_list))\n",
    "plt.plot(x, ppl_list, label='train')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('perplexity')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n말뭉치 크기: 1000, 어휘 수: 418\\n| 에폭 1 | 퍼플렉서티 392.73\\n| 에폭 2 | 퍼플렉서티 272.01\\n| 에폭 3 | 퍼플렉서티 229.42\\n| 에폭 4 | 퍼플렉서티 218.85\\n| 에폭 5 | 퍼플렉서티 207.97\\n| 에폭 6 | 퍼플렉서티 204.74\\n| 에폭 7 | 퍼플렉서티 200.02\\n| 에폭 8 | 퍼플렉서티 197.43\\n| 에폭 9 | 퍼플렉서티 192.05\\n| 에폭 10 | 퍼플렉서티 192.27\\n| 에폭 11 | 퍼플렉서티 189.15\\n| 에폭 12 | 퍼플렉서티 192.33\\n| 에폭 13 | 퍼플렉서티 189.11\\n| 에폭 14 | 퍼플렉서티 189.45\\n| 에폭 15 | 퍼플렉서티 188.09\\n| 에폭 16 | 퍼플렉서티 184.72\\n| 에폭 17 | 퍼플렉서티 183.09\\n| 에폭 18 | 퍼플렉서티 180.70\\n| 에폭 19 | 퍼플렉서티 181.07\\n| 에폭 20 | 퍼플렉서티 183.36\\n| 에폭 21 | 퍼플렉서티 180.19\\n| 에폭 22 | 퍼플렉서티 175.71\\n| 에폭 23 | 퍼플렉서티 174.92\\n| 에폭 24 | 퍼플렉서티 173.96\\n| 에폭 25 | 퍼플렉서티 172.74\\n| 에폭 26 | 퍼플렉서티 170.59\\n| 에폭 27 | 퍼플렉서티 166.03\\n| 에폭 28 | 퍼플렉서티 163.82\\n| 에폭 29 | 퍼플렉서티 163.78\\n| 에폭 30 | 퍼플렉서티 155.20\\n| 에폭 31 | 퍼플렉서티 157.31\\n| 에폭 32 | 퍼플렉서티 152.22\\n| 에폭 33 | 퍼플렉서티 150.69\\n| 에폭 34 | 퍼플렉서티 144.60\\n| 에폭 35 | 퍼플렉서티 143.27\\n| 에폭 36 | 퍼플렉서티 136.49\\n| 에폭 37 | 퍼플렉서티 132.10\\n| 에폭 38 | 퍼플렉서티 127.76\\n| 에폭 39 | 퍼플렉서티 121.69\\n| 에폭 40 | 퍼플렉서티 118.90\\n| 에폭 41 | 퍼플렉서티 117.72\\n| 에폭 42 | 퍼플렉서티 111.00\\n| 에폭 43 | 퍼플렉서티 106.07\\n| 에폭 44 | 퍼플렉서티 100.92\\n| 에폭 45 | 퍼플렉서티 97.40\\n| 에폭 46 | 퍼플렉서티 98.17\\n| 에폭 47 | 퍼플렉서티 90.00\\n| 에폭 48 | 퍼플렉서티 85.24\\n| 에폭 49 | 퍼플렉서티 80.44\\n| 에폭 50 | 퍼플렉서티 78.16\\n| 에폭 51 | 퍼플렉서티 74.41\\n| 에폭 52 | 퍼플렉서티 71.32\\n| 에폭 53 | 퍼플렉서티 66.89\\n| 에폭 54 | 퍼플렉서티 63.88\\n| 에폭 55 | 퍼플렉서티 62.11\\n| 에폭 56 | 퍼플렉서티 58.80\\n| 에폭 57 | 퍼플렉서티 54.57\\n| 에폭 58 | 퍼플렉서티 51.36\\n| 에폭 59 | 퍼플렉서티 49.48\\n| 에폭 60 | 퍼플렉서티 46.73\\n| 에폭 61 | 퍼플렉서티 45.87\\n| 에폭 62 | 퍼플렉서티 42.12\\n| 에폭 63 | 퍼플렉서티 39.67\\n| 에폭 64 | 퍼플렉서티 36.73\\n| 에폭 65 | 퍼플렉서티 36.31\\n| 에폭 66 | 퍼플렉서티 33.56\\n| 에폭 67 | 퍼플렉서티 32.45\\n| 에폭 68 | 퍼플렉서티 29.80\\n| 에폭 69 | 퍼플렉서티 27.86\\n| 에폭 70 | 퍼플렉서티 26.77\\n| 에폭 71 | 퍼플렉서티 25.17\\n| 에폭 72 | 퍼플렉서티 24.48\\n| 에폭 73 | 퍼플렉서티 23.01\\n| 에폭 74 | 퍼플렉서티 21.05\\n| 에폭 75 | 퍼플렉서티 20.48\\n| 에폭 76 | 퍼플렉서티 19.20\\n| 에폭 77 | 퍼플렉서티 17.84\\n| 에폭 78 | 퍼플렉서티 17.24\\n| 에폭 79 | 퍼플렉서티 15.69\\n| 에폭 80 | 퍼플렉서티 15.01\\n| 에폭 81 | 퍼플렉서티 14.29\\n| 에폭 82 | 퍼플렉서티 14.06\\n| 에폭 83 | 퍼플렉서티 12.53\\n| 에폭 84 | 퍼플렉서티 12.42\\n| 에폭 85 | 퍼플렉서티 12.05\\n| 에폭 86 | 퍼플렉서티 11.45\\n| 에폭 87 | 퍼플렉서티 10.68\\n| 에폭 88 | 퍼플렉서티 9.98\\n| 에폭 89 | 퍼플렉서티 9.60\\n| 에폭 90 | 퍼플렉서티 9.22\\n| 에폭 91 | 퍼플렉서티 8.73\\n| 에폭 92 | 퍼플렉서티 8.49\\n| 에폭 93 | 퍼플렉서티 8.27\\n| 에폭 94 | 퍼플렉서티 7.63\\n| 에폭 95 | 퍼플렉서티 7.32\\n| 에폭 96 | 퍼플렉서티 6.79\\n| 에폭 97 | 퍼플렉서티 6.35\\n| 에폭 98 | 퍼플렉서티 5.91\\n| 에폭 99 | 퍼플렉서티 5.87\\n| 에폭 100 | 퍼플렉서티 5.56\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "말뭉치 크기: 1000, 어휘 수: 418\n",
    "| 에폭 1 | 퍼플렉서티 392.73\n",
    "| 에폭 2 | 퍼플렉서티 272.01\n",
    "| 에폭 3 | 퍼플렉서티 229.42\n",
    "| 에폭 4 | 퍼플렉서티 218.85\n",
    "| 에폭 5 | 퍼플렉서티 207.97\n",
    "| 에폭 6 | 퍼플렉서티 204.74\n",
    "| 에폭 7 | 퍼플렉서티 200.02\n",
    "| 에폭 8 | 퍼플렉서티 197.43\n",
    "| 에폭 9 | 퍼플렉서티 192.05\n",
    "| 에폭 10 | 퍼플렉서티 192.27\n",
    "| 에폭 11 | 퍼플렉서티 189.15\n",
    "| 에폭 12 | 퍼플렉서티 192.33\n",
    "| 에폭 13 | 퍼플렉서티 189.11\n",
    "| 에폭 14 | 퍼플렉서티 189.45\n",
    "| 에폭 15 | 퍼플렉서티 188.09\n",
    "| 에폭 16 | 퍼플렉서티 184.72\n",
    "| 에폭 17 | 퍼플렉서티 183.09\n",
    "| 에폭 18 | 퍼플렉서티 180.70\n",
    "| 에폭 19 | 퍼플렉서티 181.07\n",
    "| 에폭 20 | 퍼플렉서티 183.36\n",
    "| 에폭 21 | 퍼플렉서티 180.19\n",
    "| 에폭 22 | 퍼플렉서티 175.71\n",
    "| 에폭 23 | 퍼플렉서티 174.92\n",
    "| 에폭 24 | 퍼플렉서티 173.96\n",
    "| 에폭 25 | 퍼플렉서티 172.74\n",
    "| 에폭 26 | 퍼플렉서티 170.59\n",
    "| 에폭 27 | 퍼플렉서티 166.03\n",
    "| 에폭 28 | 퍼플렉서티 163.82\n",
    "| 에폭 29 | 퍼플렉서티 163.78\n",
    "| 에폭 30 | 퍼플렉서티 155.20\n",
    "| 에폭 31 | 퍼플렉서티 157.31\n",
    "| 에폭 32 | 퍼플렉서티 152.22\n",
    "| 에폭 33 | 퍼플렉서티 150.69\n",
    "| 에폭 34 | 퍼플렉서티 144.60\n",
    "| 에폭 35 | 퍼플렉서티 143.27\n",
    "| 에폭 36 | 퍼플렉서티 136.49\n",
    "| 에폭 37 | 퍼플렉서티 132.10\n",
    "| 에폭 38 | 퍼플렉서티 127.76\n",
    "| 에폭 39 | 퍼플렉서티 121.69\n",
    "| 에폭 40 | 퍼플렉서티 118.90\n",
    "| 에폭 41 | 퍼플렉서티 117.72\n",
    "| 에폭 42 | 퍼플렉서티 111.00\n",
    "| 에폭 43 | 퍼플렉서티 106.07\n",
    "| 에폭 44 | 퍼플렉서티 100.92\n",
    "| 에폭 45 | 퍼플렉서티 97.40\n",
    "| 에폭 46 | 퍼플렉서티 98.17\n",
    "| 에폭 47 | 퍼플렉서티 90.00\n",
    "| 에폭 48 | 퍼플렉서티 85.24\n",
    "| 에폭 49 | 퍼플렉서티 80.44\n",
    "| 에폭 50 | 퍼플렉서티 78.16\n",
    "| 에폭 51 | 퍼플렉서티 74.41\n",
    "| 에폭 52 | 퍼플렉서티 71.32\n",
    "| 에폭 53 | 퍼플렉서티 66.89\n",
    "| 에폭 54 | 퍼플렉서티 63.88\n",
    "| 에폭 55 | 퍼플렉서티 62.11\n",
    "| 에폭 56 | 퍼플렉서티 58.80\n",
    "| 에폭 57 | 퍼플렉서티 54.57\n",
    "| 에폭 58 | 퍼플렉서티 51.36\n",
    "| 에폭 59 | 퍼플렉서티 49.48\n",
    "| 에폭 60 | 퍼플렉서티 46.73\n",
    "| 에폭 61 | 퍼플렉서티 45.87\n",
    "| 에폭 62 | 퍼플렉서티 42.12\n",
    "| 에폭 63 | 퍼플렉서티 39.67\n",
    "| 에폭 64 | 퍼플렉서티 36.73\n",
    "| 에폭 65 | 퍼플렉서티 36.31\n",
    "| 에폭 66 | 퍼플렉서티 33.56\n",
    "| 에폭 67 | 퍼플렉서티 32.45\n",
    "| 에폭 68 | 퍼플렉서티 29.80\n",
    "| 에폭 69 | 퍼플렉서티 27.86\n",
    "| 에폭 70 | 퍼플렉서티 26.77\n",
    "| 에폭 71 | 퍼플렉서티 25.17\n",
    "| 에폭 72 | 퍼플렉서티 24.48\n",
    "| 에폭 73 | 퍼플렉서티 23.01\n",
    "| 에폭 74 | 퍼플렉서티 21.05\n",
    "| 에폭 75 | 퍼플렉서티 20.48\n",
    "| 에폭 76 | 퍼플렉서티 19.20\n",
    "| 에폭 77 | 퍼플렉서티 17.84\n",
    "| 에폭 78 | 퍼플렉서티 17.24\n",
    "| 에폭 79 | 퍼플렉서티 15.69\n",
    "| 에폭 80 | 퍼플렉서티 15.01\n",
    "| 에폭 81 | 퍼플렉서티 14.29\n",
    "| 에폭 82 | 퍼플렉서티 14.06\n",
    "| 에폭 83 | 퍼플렉서티 12.53\n",
    "| 에폭 84 | 퍼플렉서티 12.42\n",
    "| 에폭 85 | 퍼플렉서티 12.05\n",
    "| 에폭 86 | 퍼플렉서티 11.45\n",
    "| 에폭 87 | 퍼플렉서티 10.68\n",
    "| 에폭 88 | 퍼플렉서티 9.98\n",
    "| 에폭 89 | 퍼플렉서티 9.60\n",
    "| 에폭 90 | 퍼플렉서티 9.22\n",
    "| 에폭 91 | 퍼플렉서티 8.73\n",
    "| 에폭 92 | 퍼플렉서티 8.49\n",
    "| 에폭 93 | 퍼플렉서티 8.27\n",
    "| 에폭 94 | 퍼플렉서티 7.63\n",
    "| 에폭 95 | 퍼플렉서티 7.32\n",
    "| 에폭 96 | 퍼플렉서티 6.79\n",
    "| 에폭 97 | 퍼플렉서티 6.35\n",
    "| 에폭 98 | 퍼플렉서티 5.91\n",
    "| 에폭 99 | 퍼플렉서티 5.87\n",
    "| 에폭 100 | 퍼플렉서티 5.56\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# common_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================#\n",
    "#base_model.py\n",
    "#================================================================================#\n",
    "\n",
    "\n",
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import pickle\n",
    "#from common.np import *\n",
    "#from common.util import to_gpu, to_cpu\n",
    "\n",
    "\n",
    "class BaseModel:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = None, None\n",
    "\n",
    "    def forward(self, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save_params(self, file_name=None):\n",
    "        if file_name is None:\n",
    "            file_name = self.__class__.__name__ + '.pkl'\n",
    "\n",
    "        params = [p.astype(np.float16) for p in self.params]\n",
    "        if GPU:\n",
    "            params = [to_cpu(p) for p in params]\n",
    "\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=None):\n",
    "        if file_name is None:\n",
    "            file_name = self.__class__.__name__ + '.pkl'\n",
    "\n",
    "        if '/' in file_name:\n",
    "            file_name = file_name.replace('/', os.sep)\n",
    "\n",
    "        if not os.path.exists(file_name):\n",
    "            raise IOError('No file: ' + file_name)\n",
    "\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "\n",
    "        params = [p.astype('f') for p in params]\n",
    "        if GPU:\n",
    "            params = [to_gpu(p) for p in params]\n",
    "\n",
    "        for i, param in enumerate(self.params):\n",
    "            param[...] = params[i]\n",
    "\n",
    "\n",
    "#================================================================================#\n",
    "#config.py\n",
    "#================================================================================#\n",
    "# coding: utf-8\n",
    "\n",
    "GPU = False\n",
    "\n",
    "#================================================================================#\n",
    "#functions.py\n",
    "#================================================================================#\n",
    "# coding: utf-8\n",
    "#from common.np import *\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 정답 데이터가 원핫 벡터일 경우 정답 레이블 인덱스로 변환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "#================================================================================#\n",
    "#layers.py\n",
    "#================================================================================#\n",
    "\n",
    "# coding: utf-8\n",
    "#from common.np import *  # import numpy as np\n",
    "#from common.config import GPU\n",
    "#from common.functions import softmax, cross_entropy_error\n",
    "\n",
    "\n",
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.out = softmax(x)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = self.out * dout\n",
    "        sumdx = np.sum(dx, axis=1, keepdims=True)\n",
    "        dx -= self.out * sumdx\n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None  # softmax의 출력\n",
    "        self.t = None  # 정답 레이블\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "\n",
    "        # 정답 레이블이 원핫 벡터일 경우 정답의 인덱스로 변환\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "\n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = self.y.copy()\n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx = dx / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx\n",
    "\n",
    "#================================================================================#\n",
    "#layers.py\n",
    "#================================================================================#\n",
    "\n",
    "class SigmoidWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.loss = None\n",
    "        self.y = None  # sigmoid의 출력\n",
    "        self.t = None  # 정답 데이터\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "        self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = (self.y - self.t) * dout / batch_size\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Dropout:\n",
    "    '''\n",
    "    http://arxiv.org/abs/1207.0580\n",
    "    '''\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.params, self.grads = [], []\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "        return None\n",
    "\n",
    "\n",
    "#================================================================================#\n",
    "#np.py\n",
    "#================================================================================#\n",
    "\n",
    "# coding: utf-8\n",
    "#from common.config import GPU\n",
    "\n",
    "\n",
    "if GPU:\n",
    "    import cupy as np\n",
    "    np.cuda.set_allocator(np.cuda.MemoryPool().malloc)\n",
    "    np.add.at = np.scatter_add\n",
    "\n",
    "    print('\\033[92m' + '-' * 60 + '\\033[0m')\n",
    "    print(' ' * 23 + '\\033[92mGPU Mode (cupy)\\033[0m')\n",
    "    print('\\033[92m' + '-' * 60 + '\\033[0m\\n')\n",
    "else:\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "#================================================================================#\n",
    "#optimizer.py\n",
    "#================================================================================#\n",
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "#from common.np import *\n",
    "\n",
    "\n",
    "class SGD:\n",
    "    '''\n",
    "    확률적 경사하강법(Stochastic Gradient Descent)\n",
    "    '''\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]\n",
    "\n",
    "\n",
    "class Momentum:\n",
    "    '''\n",
    "    모멘텀 SGG(Momentum SGD)\n",
    "    '''\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = []\n",
    "            for param in params:\n",
    "                self.v.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.v[i] = self.momentum * self.v[i] - self.lr * grads[i]\n",
    "            params[i] += self.v[i]\n",
    "\n",
    "\n",
    "class Nesterov:\n",
    "    '''\n",
    "    네스테로프 가속 경사(NAG; Nesterov's Accelerated Gradient) (http://arxiv.org/abs/1212.0901)\n",
    "    '네스테로프 모멘텀 최적화'라고도 한다.\n",
    "    '''\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = []\n",
    "            for param in params:\n",
    "                self.v.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.v[i] *= self.momentum\n",
    "            self.v[i] -= self.lr * grads[i]\n",
    "            params[i] += self.momentum * self.momentum * self.v[i]\n",
    "            params[i] -= (1 + self.momentum) * self.lr * grads[i]\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "    '''\n",
    "    AdaGrad\n",
    "    '''\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = []\n",
    "            for param in params:\n",
    "                self.h.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.h[i] += grads[i] * grads[i]\n",
    "            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)\n",
    "\n",
    "\n",
    "class RMSprop:\n",
    "    '''\n",
    "    RMSprop\n",
    "    '''\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = []\n",
    "            for param in params:\n",
    "                self.h.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.h[i] *= self.decay_rate\n",
    "            self.h[i] += (1 - self.decay_rate) * grads[i] * grads[i]\n",
    "            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    '''\n",
    "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
    "    '''\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "            \n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)\n",
    "\n",
    "#================================================================================#\n",
    "#time_layers.py\n",
    "#================================================================================#\n",
    "# coding: utf-8\n",
    "#from common.np import *  # import numpy as np (or import cupy as np)\n",
    "#from common.layers import *\n",
    "#from common.functions import sigmoid\n",
    "\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
    "        h_next = np.tanh(t)\n",
    "\n",
    "        self.cache = (x, h_prev, h_next)\n",
    "        return h_next\n",
    "\n",
    "    def backward(self, dh_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, h_next = self.cache\n",
    "\n",
    "        dt = dh_next * (1 - h_next ** 2)\n",
    "        db = np.sum(dt, axis=0)\n",
    "        dWh = np.dot(h_prev.T, dt)\n",
    "        dh_prev = np.dot(dt, Wh.T)\n",
    "        dWx = np.dot(x.T, dt)\n",
    "        dx = np.dot(dt, Wx.T)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        return dx, dh_prev\n",
    "\n",
    "\n",
    "class TimeRNN:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.dh = None, None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        D, H = Wx.shape\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = RNN(*self.params)\n",
    "            self.h = layer.forward(xs[:, t, :], self.h)\n",
    "            hs[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D, H = Wx.shape\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh = 0\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
    "            dxs[:, t, :] = dx\n",
    "\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        '''\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Wx: 입력 x에 대한 가중치 매개변수(4개분의 가중치가 담겨 있음)\n",
    "        Wh: 은닉 상태 h에 대한 가장추 매개변수(4개분의 가중치가 담겨 있음)\n",
    "        b: 편향（4개분의 편향이 담겨 있음）\n",
    "        '''\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, H = h_prev.shape\n",
    "\n",
    "        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n",
    "\n",
    "        f = A[:, :H]\n",
    "        g = A[:, H:2*H]\n",
    "        i = A[:, 2*H:3*H]\n",
    "        o = A[:, 3*H:]\n",
    "\n",
    "        f = sigmoid(f)\n",
    "        g = np.tanh(g)\n",
    "        i = sigmoid(i)\n",
    "        o = sigmoid(o)\n",
    "\n",
    "        c_next = f * c_prev + g * i\n",
    "        h_next = o * np.tanh(c_next)\n",
    "\n",
    "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "    def backward(self, dh_next, dc_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
    "\n",
    "        tanh_c_next = np.tanh(c_next)\n",
    "\n",
    "        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "\n",
    "        dc_prev = ds * f\n",
    "\n",
    "        di = ds * g\n",
    "        df = ds * c_prev\n",
    "        do = dh_next * tanh_c_next\n",
    "        dg = ds * i\n",
    "\n",
    "        di *= i * (1 - i)\n",
    "        df *= f * (1 - f)\n",
    "        do *= o * (1 - o)\n",
    "        dg *= (1 - g ** 2)\n",
    "\n",
    "        dA = np.hstack((df, dg, di, do))\n",
    "\n",
    "        dWh = np.dot(h_prev.T, dA)\n",
    "        dWx = np.dot(x.T, dA)\n",
    "        db = dA.sum(axis=0)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        dx = np.dot(dA, Wx.T)\n",
    "        dh_prev = np.dot(dA, Wh.T)\n",
    "\n",
    "        return dx, dh_prev, dc_prev\n",
    "\n",
    "\n",
    "class TimeLSTM:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.c = None, None\n",
    "        self.dh = None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        H = Wh.shape[0]\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "        if not self.stateful or self.c is None:\n",
    "            self.c = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = LSTM(*self.params)\n",
    "            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n",
    "            hs[:, t, :] = self.h\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D = Wx.shape[0]\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh, dc = 0, 0\n",
    "\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n",
    "            dxs[:, t, :] = dx\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h, c=None):\n",
    "        self.h, self.c = h, c\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h, self.c = None, None\n",
    "\n",
    "\n",
    "class TimeEmbedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.layers = None\n",
    "        self.W = W\n",
    "\n",
    "    def forward(self, xs):\n",
    "        N, T = xs.shape\n",
    "        V, D = self.W.shape\n",
    "\n",
    "        out = np.empty((N, T, D), dtype='f')\n",
    "        self.layers = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Embedding(self.W)\n",
    "            out[:, t, :] = layer.forward(xs[:, t])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, D = dout.shape\n",
    "\n",
    "        grad = 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            layer.backward(dout[:, t, :])\n",
    "            grad += layer.grads[0]\n",
    "\n",
    "        self.grads[0][...] = grad\n",
    "        return None\n",
    "\n",
    "\n",
    "class TimeAffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        rx = x.reshape(N*T, -1)\n",
    "        out = np.dot(rx, W) + b\n",
    "        self.x = x\n",
    "        return out.reshape(N, T, -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        x = self.x\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        dout = dout.reshape(N*T, -1)\n",
    "        rx = x.reshape(N*T, -1)\n",
    "\n",
    "        db = np.sum(dout, axis=0)\n",
    "        dW = np.dot(rx.T, dout)\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dx = dx.reshape(*x.shape)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class TimeSoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        self.ignore_label = -1\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T, V = xs.shape\n",
    "\n",
    "        if ts.ndim == 3:  # 정답 레이블이 원핫 벡터인 경우\n",
    "            ts = ts.argmax(axis=2)\n",
    "\n",
    "        mask = (ts != self.ignore_label)\n",
    "\n",
    "        # 배치용과 시계열용을 정리(reshape)\n",
    "        xs = xs.reshape(N * T, V)\n",
    "        ts = ts.reshape(N * T)\n",
    "        mask = mask.reshape(N * T)\n",
    "\n",
    "        ys = softmax(xs)\n",
    "        ls = np.log(ys[np.arange(N * T), ts])\n",
    "        ls *= mask  # ignore_label에 해당하는 데이터는 손실을 0으로 설정\n",
    "        loss = -np.sum(ls)\n",
    "        loss /= mask.sum()\n",
    "\n",
    "        self.cache = (ts, ys, mask, (N, T, V))\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ts, ys, mask, (N, T, V) = self.cache\n",
    "\n",
    "        dx = ys\n",
    "        dx[np.arange(N * T), ts] -= 1\n",
    "        dx *= dout\n",
    "        dx /= mask.sum()\n",
    "        dx *= mask[:, np.newaxis]  # ignore_labelㅇㅔ 해당하는 데이터는 기울기를 0으로 설정\n",
    "\n",
    "        dx = dx.reshape((N, T, V))\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class TimeDropout:\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.params, self.grads = [], []\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "        self.train_flg = True\n",
    "\n",
    "    def forward(self, xs):\n",
    "        if self.train_flg:\n",
    "            flg = np.random.rand(*xs.shape) > self.dropout_ratio\n",
    "            scale = 1 / (1.0 - self.dropout_ratio)\n",
    "            self.mask = flg.astype(np.float32) * scale\n",
    "\n",
    "            return xs * self.mask\n",
    "        else:\n",
    "            return xs\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "\n",
    "class TimeBiLSTM:\n",
    "    def __init__(self, Wx1, Wh1, b1,\n",
    "                 Wx2, Wh2, b2, stateful=False):\n",
    "        self.forward_lstm = TimeLSTM(Wx1, Wh1, b1, stateful)\n",
    "        self.backward_lstm = TimeLSTM(Wx2, Wh2, b2, stateful)\n",
    "        self.params = self.forward_lstm.params + self.backward_lstm.params\n",
    "        self.grads = self.forward_lstm.grads + self.backward_lstm.grads\n",
    "\n",
    "    def forward(self, xs):\n",
    "        o1 = self.forward_lstm.forward(xs)\n",
    "        o2 = self.backward_lstm.forward(xs[:, ::-1])\n",
    "        o2 = o2[:, ::-1]\n",
    "\n",
    "        out = np.concatenate((o1, o2), axis=2)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        H = dhs.shape[2] // 2\n",
    "        do1 = dhs[:, :, :H]\n",
    "        do2 = dhs[:, :, H:]\n",
    "\n",
    "        dxs1 = self.forward_lstm.backward(do1)\n",
    "        do2 = do2[:, ::-1]\n",
    "        dxs2 = self.backward_lstm.backward(do2)\n",
    "        dxs2 = dxs2[:, ::-1]\n",
    "        dxs = dxs1 + dxs2\n",
    "        return dxs\n",
    "\n",
    "# ====================================================================== #\n",
    "# 이 아래의 계층들은 책에서 설명하지 않았거나\n",
    "# 처리 속도보다는 쉽게 이해할 수 있도록 구현했습니다.\n",
    "#\n",
    "# TimeSigmoidWithLoss: 시계열 데이터용 시그모이드 + 손실 계층\n",
    "# GRU: GRU 계층\n",
    "# TimeGRU: 시계열 데이터용 GRU 계층\n",
    "# BiTimeLSTM: 양방향 LSTM 계층\n",
    "# Simple_TimeSoftmaxWithLoss：간단한 TimeSoftmaxWithLoss 계층의 구현\n",
    "# Simple_TimeAffine: 간단한 TimeAffine 계층의 구현\n",
    "# ====================================================================== #\n",
    "\n",
    "\n",
    "class TimeSigmoidWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.xs_shape = None\n",
    "        self.layers = None\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T = xs.shape\n",
    "        self.xs_shape = xs.shape\n",
    "\n",
    "        self.layers = []\n",
    "        loss = 0\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = SigmoidWithLoss()\n",
    "            loss += layer.forward(xs[:, t], ts[:, t])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return loss / T\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        N, T = self.xs_shape\n",
    "        dxs = np.empty(self.xs_shape, dtype='f')\n",
    "\n",
    "        dout *= 1/T\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dxs[:, t] = layer.backward(dout)\n",
    "\n",
    "        return dxs\n",
    "\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, Wx, Wh):\n",
    "        '''\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Wx: 입력 x에 대한 가중치 매개변수(3개 분의 가중치가 담겨 있음)\n",
    "        Wh: 은닉 상태 h에 대한 가중치 매개변수(3개 분의 가중치가 담겨 있음)\n",
    "        '''\n",
    "        self.Wx, self.Wh = Wx, Wh\n",
    "        self.dWx, self.dWh = None, None\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        H, H3 = self.Wh.shape\n",
    "        Wxz, Wxr, Wx = self.Wx[:, :H], self.Wx[:, H:2 * H], self.Wx[:, 2 * H:]\n",
    "        Whz, Whr, Wh = self.Wh[:, :H], self.Wh[:, H:2 * H], self.Wh[:, 2 * H:]\n",
    "\n",
    "        z = sigmoid(np.dot(x, Wxz) + np.dot(h_prev, Whz))\n",
    "        r = sigmoid(np.dot(x, Wxr) + np.dot(h_prev, Whr))\n",
    "        h_hat = np.tanh(np.dot(x, Wx) + np.dot(r*h_prev, Wh))\n",
    "        h_next = (1-z) * h_prev + z * h_hat\n",
    "\n",
    "        self.cache = (x, h_prev, z, r, h_hat)\n",
    "\n",
    "        return h_next\n",
    "\n",
    "    def backward(self, dh_next):\n",
    "        H, H3 = self.Wh.shape\n",
    "        Wxz, Wxr, Wx = self.Wx[:, :H], self.Wx[:, H:2 * H], self.Wx[:, 2 * H:]\n",
    "        Whz, Whr, Wh = self.Wh[:, :H], self.Wh[:, H:2 * H], self.Wh[:, 2 * H:]\n",
    "        x, h_prev, z, r, h_hat = self.cache\n",
    "\n",
    "        dh_hat =dh_next * z\n",
    "        dh_prev = dh_next * (1-z)\n",
    "\n",
    "        # tanh\n",
    "        dt = dh_hat * (1 - h_hat ** 2)\n",
    "        dWh = np.dot((r * h_prev).T, dt)\n",
    "        dhr = np.dot(dt, Wh.T)\n",
    "        dWx = np.dot(x.T, dt)\n",
    "        dx = np.dot(dt, Wx.T)\n",
    "        dh_prev += r * dhr\n",
    "\n",
    "        # update gate(z)\n",
    "        dz = dh_next * h_hat - dh_next * h_prev\n",
    "        dt = dz * z * (1-z)\n",
    "        dWhz = np.dot(h_prev.T, dt)\n",
    "        dh_prev += np.dot(dt, Whz.T)\n",
    "        dWxz = np.dot(x.T, dt)\n",
    "        dx += np.dot(dt, Wxz.T)\n",
    "\n",
    "        # rest gate(r)\n",
    "        dr = dhr * h_prev\n",
    "        dt = dr * r * (1-r)\n",
    "        dWhr = np.dot(h_prev.T, dt)\n",
    "        dh_prev += np.dot(dt, Whr.T)\n",
    "        dWxr = np.dot(x.T, dt)\n",
    "        dx += np.dot(dt, Wxr.T)\n",
    "\n",
    "        self.dWx = np.hstack((dWxz, dWxr, dWx))\n",
    "        self.dWh = np.hstack((dWhz, dWhr, dWh))\n",
    "\n",
    "        return dx, dh_prev\n",
    "\n",
    "\n",
    "class TimeGRU:\n",
    "    def __init__(self, Wx, Wh, stateful=False):\n",
    "        self.Wx, self.Wh = Wx, Wh\n",
    "        selfdWx, self.dWh = None, None\n",
    "        self.layers = None\n",
    "        self.h, self.dh = None, None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        N, T, D = xs.shape\n",
    "        H, H3 = self.Wh.shape\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = GRU(self.Wx, self.Wh)\n",
    "            self.h = layer.forward(xs[:, t, :], self.h)\n",
    "            hs[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        N, T, H = dhs.shape\n",
    "        D = self.Wx.shape[0]\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        self.dWx, self.dWh = 0, 0\n",
    "\n",
    "        dh = 0\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
    "\n",
    "            dxs[:, t, :] = dx\n",
    "            self.dWx += layer.dWx\n",
    "            self.dWh += layer.dWh\n",
    "\n",
    "        self.dh = dh\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "\n",
    "\n",
    "class Simple_TimeSoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T, V = xs.shape\n",
    "        layers = []\n",
    "        loss = 0\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = SoftmaxWithLoss()\n",
    "            loss += layer.forward(xs[:, t, :], ts[:, t])\n",
    "            layers.append(layer)\n",
    "        loss /= T\n",
    "\n",
    "        self.cache = (layers, xs)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        layers, xs = self.cache\n",
    "        N, T, V = xs.shape\n",
    "        dxs = np.empty(xs.shape, dtype='f')\n",
    "\n",
    "        dout *= 1/T\n",
    "        for t in range(T):\n",
    "            layer = layers[t]\n",
    "            dxs[:, t, :] = layer.backward(dout)\n",
    "\n",
    "        return dxs\n",
    "\n",
    "\n",
    "class Simple_TimeAffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W, self.b = W, b\n",
    "        self.dW, self.db = None, None\n",
    "        self.layers = None\n",
    "\n",
    "    def forward(self, xs):\n",
    "        N, T, D = xs.shape\n",
    "        D, M = self.W.shape\n",
    "\n",
    "        self.layers = []\n",
    "        out = np.empty((N, T, M), dtype='f')\n",
    "        for t in range(T):\n",
    "            layer = Affine(self.W, self.b)\n",
    "            out[:, t, :] = layer.forward(xs[:, t, :])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, M = dout.shape\n",
    "        D, M = self.W.shape\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        self.dW, self.db = 0, 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dxs[:, t, :] = layer.backward(dout[:, t, :])\n",
    "\n",
    "            self.dW += layer.dW\n",
    "            self.db += layer.db\n",
    "\n",
    "        return dxs\n",
    "\n",
    "#================================================================================#\n",
    "#trainer.py\n",
    "#================================================================================#\n",
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "#from common.np import *  # import numpy as np\n",
    "#from common.util import clip_grads\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            # 뒤섞기\n",
    "            idx = numpy.random.permutation(numpy.arange(data_size))\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "                # 기울기 구해 매개변수 갱신\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.params, model.grads)  # 공유된 가중치를 하나로 모음\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # 평가\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    avg_loss = total_loss / loss_count\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print('| 에폭 %d |  반복 %d / %d | 시간 %d[s] | 손실 %.2f'\n",
    "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
    "                    self.loss_list.append(float(avg_loss))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = numpy.arange(len(self.loss_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.loss_list, label='train')\n",
    "        plt.xlabel('반복 (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('손실')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class RnnlmTrainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.time_idx = None\n",
    "        self.ppl_list = None\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def get_batch(self, x, t, batch_size, time_size):\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "\n",
    "        data_size = len(x)\n",
    "        jump = data_size // batch_size\n",
    "        offsets = [i * jump for i in range(batch_size)]  # 배치에서 각 샘플을 읽기 시작하는 위치\n",
    "\n",
    "        for time in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, time] = x[(offset + self.time_idx) % data_size]\n",
    "                batch_t[i, time] = t[(offset + self.time_idx) % data_size]\n",
    "            self.time_idx += 1\n",
    "        return batch_x, batch_t\n",
    "\n",
    "    def fit(self, xs, ts, max_epoch=10, batch_size=20, time_size=35,\n",
    "            max_grad=None, eval_interval=20):\n",
    "        data_size = len(xs)\n",
    "        max_iters = data_size // (batch_size * time_size)\n",
    "        self.time_idx = 0\n",
    "        self.ppl_list = []\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            for iters in range(max_iters):\n",
    "                batch_x, batch_t = self.get_batch(xs, ts, batch_size, time_size)\n",
    "\n",
    "                # 기울기를 구해 매개변수 갱신\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.params, model.grads)  # 공유된 가중치를 하나로 모음\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # 퍼플렉서티 평가\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    ppl = np.exp(total_loss / loss_count)\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print('| 에폭 %d |  반복 %d / %d | 시간 %d[s] | 퍼플렉서티 %.2f'\n",
    "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, ppl))\n",
    "                    self.ppl_list.append(float(ppl))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = numpy.arange(len(self.ppl_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.ppl_list, label='train')\n",
    "        plt.xlabel('반복 (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('퍼플렉서티')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def remove_duplicate(params, grads):\n",
    "    '''\n",
    "    매개변수 배열 중 중복되는 가중치를 하나로 모아\n",
    "    그 가중치에 대응하는 기울기를 더한다.\n",
    "    '''\n",
    "    params, grads = params[:], grads[:]  # copy list\n",
    "\n",
    "    while True:\n",
    "        find_flg = False\n",
    "        L = len(params)\n",
    "\n",
    "        for i in range(0, L - 1):\n",
    "            for j in range(i + 1, L):\n",
    "                # 가중치 공유 시\n",
    "                if params[i] is params[j]:\n",
    "                    grads[i] += grads[j]  # 경사를 더함\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                # 가중치를 전치행렬로 공유하는 경우(weight tying)\n",
    "                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
    "                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
    "                    grads[i] += grads[j].T\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "\n",
    "                if find_flg: break\n",
    "            if find_flg: break\n",
    "\n",
    "        if not find_flg: break\n",
    "\n",
    "    return params, grads\n",
    "\n",
    "\n",
    "#================================================================================#\n",
    "#util.py\n",
    "#================================================================================#\n",
    "\n",
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "#from common.np import *\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "\n",
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    '''코사인 유사도 산출\n",
    "\n",
    "    :param x: 벡터\n",
    "    :param y: 벡터\n",
    "    :param eps: '0으로 나누기'를 방지하기 위한 작은 값\n",
    "    :return:\n",
    "    '''\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
    "    return np.dot(nx, ny)\n",
    "\n",
    "\n",
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    '''유사 단어 검색\n",
    "\n",
    "    :param query: 쿼리(텍스트)\n",
    "    :param word_to_id: 단어에서 단어 ID로 변환하는 딕셔너리\n",
    "    :param id_to_word: 단어 ID에서 단어로 변환하는 딕셔너리\n",
    "    :param word_matrix: 단어 벡터를 정리한 행렬. 각 행에 해당 단어 벡터가 저장되어 있다고 가정한다.\n",
    "    :param top: 상위 몇 개까지 출력할 지 지정\n",
    "    '''\n",
    "    if query not in word_to_id:\n",
    "        print('%s(을)를 찾을 수 없습니다.' % query)\n",
    "        return\n",
    "\n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    vocab_size = len(id_to_word)\n",
    "\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "\n",
    "    # 코사인 유사도를 기준으로 내림차순으로 출력\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "\n",
    "\n",
    "def convert_one_hot(corpus, vocab_size):\n",
    "    '''원핫 표현으로 변환\n",
    "\n",
    "    :param corpus: 단어 ID 목록(1차원 또는 2차원 넘파이 배열)\n",
    "    :param vocab_size: 어휘 수\n",
    "    :return: 원핫 표현(2차원 또는 3차원 넘파이 배열)\n",
    "    '''\n",
    "    N = corpus.shape[0]\n",
    "\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    '''동시발생 행렬 생성\n",
    "\n",
    "    :param corpus: 말뭉치(단어 ID 목록)\n",
    "    :param vocab_size: 어휘 수\n",
    "    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n",
    "    :return: 동시발생 행렬\n",
    "    '''\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "\n",
    "    return co_matrix\n",
    "\n",
    "\n",
    "def ppmi(C, verbose=False, eps = 1e-8):\n",
    "    '''PPMI(점별 상호정보량) 생성\n",
    "\n",
    "    :param C: 동시발생 행렬\n",
    "    :param verbose: 진행 상황을 출력할지 여부\n",
    "    :return:\n",
    "    '''\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis=0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
    "            M[i, j] = max(0, pmi)\n",
    "\n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total//100) == 0:\n",
    "                    print('%.1f%% 완료' % (100*cnt/total))\n",
    "    return M\n",
    "\n",
    "\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    '''맥락과 타깃 생성\n",
    "\n",
    "    :param corpus: 말뭉치(단어 ID 목록)\n",
    "    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n",
    "    :return:\n",
    "    '''\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "\n",
    "    return np.array(contexts), np.array(target)\n",
    "\n",
    "\n",
    "def to_cpu(x):\n",
    "    import numpy\n",
    "    if type(x) == numpy.ndarray:\n",
    "        return x\n",
    "    return np.asnumpy(x)\n",
    "\n",
    "\n",
    "def to_gpu(x):\n",
    "    import cupy\n",
    "    if type(x) == cupy.ndarray:\n",
    "        return x\n",
    "    return cupy.asarray(x)\n",
    "\n",
    "\n",
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate\n",
    "\n",
    "\n",
    "def eval_perplexity(model, corpus, batch_size=10, time_size=35):\n",
    "    print('퍼플렉서티 평가 중 ...')\n",
    "    corpus_size = len(corpus)\n",
    "    total_loss, loss_cnt = 0, 0\n",
    "    max_iters = (corpus_size - 1) // (batch_size * time_size)\n",
    "    jump = (corpus_size - 1) // batch_size\n",
    "\n",
    "    for iters in range(max_iters):\n",
    "        xs = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "        ts = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "        time_offset = iters * time_size\n",
    "        offsets = [time_offset + (i * jump) for i in range(batch_size)]\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                xs[i, t] = corpus[(offset + t) % corpus_size]\n",
    "                ts[i, t] = corpus[(offset + t + 1) % corpus_size]\n",
    "\n",
    "        try:\n",
    "            loss = model.forward(xs, ts, train_flg=False)\n",
    "        except TypeError:\n",
    "            loss = model.forward(xs, ts)\n",
    "        total_loss += loss\n",
    "\n",
    "        sys.stdout.write('\\r%d / %d' % (iters, max_iters))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    print('')\n",
    "    ppl = np.exp(total_loss / max_iters)\n",
    "    return ppl\n",
    "\n",
    "\n",
    "def eval_seq2seq(model, question, correct, id_to_char,\n",
    "                 verbos=False, is_reverse=False):\n",
    "    correct = correct.flatten()\n",
    "    # 머릿글자\n",
    "    start_id = correct[0]\n",
    "    correct = correct[1:]\n",
    "    guess = model.generate(question, start_id, len(correct))\n",
    "\n",
    "    # 문자열로 변환\n",
    "    question = ''.join([id_to_char[int(c)] for c in question.flatten()])\n",
    "    correct = ''.join([id_to_char[int(c)] for c in correct])\n",
    "    guess = ''.join([id_to_char[int(c)] for c in guess])\n",
    "\n",
    "    if verbos:\n",
    "        if is_reverse:\n",
    "            question = question[::-1]\n",
    "\n",
    "        colors = {'ok': '\\033[92m', 'fail': '\\033[91m', 'close': '\\033[0m'}\n",
    "        print('Q', question)\n",
    "        print('T', correct)\n",
    "\n",
    "        is_windows = os.name == 'nt'\n",
    "\n",
    "        if correct == guess:\n",
    "            mark = colors['ok'] + '☑' + colors['close']\n",
    "            if is_windows:\n",
    "                mark = 'O'\n",
    "            print(mark + ' ' + guess)\n",
    "        else:\n",
    "            mark = colors['fail'] + '☒' + colors['close']\n",
    "            if is_windows:\n",
    "                mark = 'X'\n",
    "            print(mark + ' ' + guess)\n",
    "        print('---')\n",
    "\n",
    "    return 1 if guess == correct else 0\n",
    "\n",
    "\n",
    "def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):\n",
    "    for word in (a, b, c):\n",
    "        if word not in word_to_id:\n",
    "            print('%s(을)를 찾을 수 없습니다.' % word)\n",
    "            return\n",
    "\n",
    "    print('\\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')\n",
    "    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n",
    "    query_vec = b_vec - a_vec + c_vec\n",
    "    query_vec = normalize(query_vec)\n",
    "\n",
    "    similarity = np.dot(word_matrix, query_vec)\n",
    "\n",
    "    if answer is not None:\n",
    "        print(\"==>\" + answer + \":\" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))\n",
    "\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if np.isnan(similarity[i]):\n",
    "            continue\n",
    "        if id_to_word[i] in (a, b, c):\n",
    "            continue\n",
    "        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    if x.ndim == 2:\n",
    "        s = np.sqrt((x * x).sum(1))\n",
    "        x /= s.reshape((s.shape[0], 1))\n",
    "    elif x.ndim == 1:\n",
    "        s = np.sqrt((x * x).sum())\n",
    "        x /= s\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
