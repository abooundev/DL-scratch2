{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attention_layer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "#from common.np import *  # import numpy as np\n",
    "#from common.layers import Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ar = a.reshape(N, T, 1)#.repeat(T, axis=1)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "\n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "\n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)\n",
    "\n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        hr = h.reshape(N, 1, H)#.repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:,t,:] = dh\n",
    "\n",
    "        return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attention_seq2seq.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "#from common.time_layers import *\n",
    "#from ch07.seq2seq import Encoder, Seq2seq\n",
    "#from ch08.attention_layer import TimeAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:,-1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)\n",
    "        out = np.concatenate((c, dec_hs), axis=2)\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../ch07')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import sequence\n",
    "#from common.optimizer import Adam\n",
    "#from common.trainer import Trainer\n",
    "#from common.util import eval_seq2seq\n",
    "#from attention_seq2seq import AttentionSeq2seq\n",
    "#from ch07.seq2seq import Seq2seq\n",
    "#from ch07.peeky_seq2seq import PeekySeq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose, is_reverse=True)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('정확도 %.3f%%' % (acc * 100))\n",
    "model.save_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 그리기\n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('에폭')\n",
    "plt.ylabel('정확도')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
    "| 에폭 1 |  반복 21 / 351 | 시간 20[s] | 손실 3.09\n",
    "| 에폭 1 |  반복 41 / 351 | 시간 41[s] | 손실 1.90\n",
    "| 에폭 1 |  반복 61 / 351 | 시간 61[s] | 손실 1.72\n",
    "| 에폭 1 |  반복 81 / 351 | 시간 81[s] | 손실 1.46\n",
    "| 에폭 1 |  반복 101 / 351 | 시간 100[s] | 손실 1.19\n",
    "| 에폭 1 |  반복 121 / 351 | 시간 120[s] | 손실 1.14\n",
    "| 에폭 1 |  반복 141 / 351 | 시간 140[s] | 손실 1.09\n",
    "| 에폭 1 |  반복 161 / 351 | 시간 159[s] | 손실 1.06\n",
    "| 에폭 1 |  반복 181 / 351 | 시간 177[s] | 손실 1.04\n",
    "| 에폭 1 |  반복 201 / 351 | 시간 195[s] | 손실 1.03\n",
    "| 에폭 1 |  반복 221 / 351 | 시간 217[s] | 손실 1.02\n",
    "| 에폭 1 |  반복 241 / 351 | 시간 233[s] | 손실 1.02\n",
    "| 에폭 1 |  반복 261 / 351 | 시간 247[s] | 손실 1.01\n",
    "| 에폭 1 |  반복 281 / 351 | 시간 261[s] | 손실 1.00\n",
    "| 에폭 1 |  반복 301 / 351 | 시간 275[s] | 손실 1.00\n",
    "| 에폭 1 |  반복 321 / 351 | 시간 289[s] | 손실 1.00\n",
    "| 에폭 1 |  반복 341 / 351 | 시간 303[s] | 손실 1.00\n",
    "Q 10/15/94                     \n",
    "T 1994-10-15\n",
    "☒ 1978-08-11\n",
    "---\n",
    "Q thursday, november 13, 2008  \n",
    "T 2008-11-13\n",
    "☒ 1978-08-11\n",
    "---\n",
    "Q Mar 25, 2003                 \n",
    "T 2003-03-25\n",
    "☒ 1978-08-11\n",
    "---\n",
    "Q Tuesday, November 22, 2016   \n",
    "T 2016-11-22\n",
    "☒ 1978-08-11\n",
    "---\n",
    "Q Saturday, July 18, 1970      \n",
    "T 1970-07-18\n",
    "☒ 1978-08-11\n",
    "---\n",
    "Q october 6, 1992              \n",
    "T 1992-10-06\n",
    "☒ 1978-08-11\n",
    "---\n",
    "Q 8/23/08                      \n",
    "T 2008-08-23\n",
    "☒ 1978-08-11\n",
    "---\n",
    "Q 8/30/07                      \n",
    "T 2007-08-30\n",
    "☒ 1978-08-11\n",
    "---\n",
    "Q 10/28/13                     \n",
    "T 2013-10-28\n",
    "☒ 1978-08-11\n",
    "---\n",
    "Q sunday, november 6, 2016     \n",
    "T 2016-11-06\n",
    "☒ 1978-08-11\n",
    "---\n",
    "정확도 0.000%\n",
    "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 1.00\n",
    "| 에폭 2 |  반복 21 / 351 | 시간 14[s] | 손실 1.00\n",
    "| 에폭 2 |  반복 41 / 351 | 시간 28[s] | 손실 0.99\n",
    "| 에폭 2 |  반복 61 / 351 | 시간 43[s] | 손실 0.99\n",
    "| 에폭 2 |  반복 81 / 351 | 시간 57[s] | 손실 0.99\n",
    "| 에폭 2 |  반복 101 / 351 | 시간 71[s] | 손실 0.99\n",
    "| 에폭 2 |  반복 121 / 351 | 시간 85[s] | 손실 0.99\n",
    "| 에폭 2 |  반복 141 / 351 | 시간 99[s] | 손실 0.98\n",
    "| 에폭 2 |  반복 161 / 351 | 시간 113[s] | 손실 0.98\n",
    "| 에폭 2 |  반복 181 / 351 | 시간 127[s] | 손실 0.97\n",
    "| 에폭 2 |  반복 201 / 351 | 시간 141[s] | 손실 0.95\n",
    "| 에폭 2 |  반복 221 / 351 | 시간 156[s] | 손실 0.94\n",
    "| 에폭 2 |  반복 241 / 351 | 시간 170[s] | 손실 0.90\n",
    "| 에폭 2 |  반복 261 / 351 | 시간 184[s] | 손실 0.83\n",
    "| 에폭 2 |  반복 281 / 351 | 시간 198[s] | 손실 0.74\n",
    "| 에폭 2 |  반복 301 / 351 | 시간 212[s] | 손실 0.66\n",
    "| 에폭 2 |  반복 321 / 351 | 시간 226[s] | 손실 0.58\n",
    "| 에폭 2 |  반복 341 / 351 | 시간 240[s] | 손실 0.46\n",
    "Q 10/15/94                     \n",
    "T 1994-10-15\n",
    "☑ 1994-10-15\n",
    "---\n",
    "Q thursday, november 13, 2008  \n",
    "T 2008-11-13\n",
    "☒ 2006-11-13\n",
    "---\n",
    "Q Mar 25, 2003                 \n",
    "T 2003-03-25\n",
    "☑ 2003-03-25\n",
    "---\n",
    "Q Tuesday, November 22, 2016   \n",
    "T 2016-11-22\n",
    "☑ 2016-11-22\n",
    "---\n",
    "Q Saturday, July 18, 1970      \n",
    "T 1970-07-18\n",
    "☑ 1970-07-18\n",
    "---\n",
    "Q october 6, 1992              \n",
    "T 1992-10-06\n",
    "☑ 1992-10-06\n",
    "---\n",
    "Q 8/23/08                      \n",
    "T 2008-08-23\n",
    "☑ 2008-08-23\n",
    "---\n",
    "Q 8/30/07                      \n",
    "T 2007-08-30\n",
    "☒ 2007-08-09\n",
    "---\n",
    "Q 10/28/13                     \n",
    "T 2013-10-28\n",
    "☒ 1983-10-28\n",
    "---\n",
    "Q sunday, november 6, 2016     \n",
    "T 2016-11-06\n",
    "☒ 2016-11-08\n",
    "---\n",
    "정확도 51.560%\n",
    "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.35\n",
    "| 에폭 3 |  반복 21 / 351 | 시간 9[s] | 손실 0.30\n",
    "| 에폭 3 |  반복 41 / 351 | 시간 18[s] | 손실 0.21\n",
    "| 에폭 3 |  반복 61 / 351 | 시간 27[s] | 손실 0.14\n",
    "| 에폭 3 |  반복 81 / 351 | 시간 36[s] | 손실 0.09\n",
    "| 에폭 3 |  반복 101 / 351 | 시간 45[s] | 손실 0.07\n",
    "| 에폭 3 |  반복 121 / 351 | 시간 54[s] | 손실 0.05\n",
    "| 에폭 3 |  반복 141 / 351 | 시간 63[s] | 손실 0.04\n",
    "| 에폭 3 |  반복 161 / 351 | 시간 72[s] | 손실 0.03\n",
    "| 에폭 3 |  반복 181 / 351 | 시간 81[s] | 손실 0.03\n",
    "| 에폭 3 |  반복 201 / 351 | 시간 90[s] | 손실 0.02\n",
    "| 에폭 3 |  반복 221 / 351 | 시간 99[s] | 손실 0.02\n",
    "| 에폭 3 |  반복 241 / 351 | 시간 107[s] | 손실 0.02\n",
    "| 에폭 3 |  반복 261 / 351 | 시간 116[s] | 손실 0.01\n",
    "| 에폭 3 |  반복 281 / 351 | 시간 125[s] | 손실 0.01\n",
    "| 에폭 3 |  반복 301 / 351 | 시간 134[s] | 손실 0.01\n",
    "| 에폭 3 |  반복 321 / 351 | 시간 143[s] | 손실 0.01\n",
    "| 에폭 3 |  반복 341 / 351 | 시간 152[s] | 손실 0.01\n",
    "Q 10/15/94                     \n",
    "T 1994-10-15\n",
    "☑ 1994-10-15\n",
    "---\n",
    "Q thursday, november 13, 2008  \n",
    "T 2008-11-13\n",
    "☑ 2008-11-13\n",
    "---\n",
    "Q Mar 25, 2003                 \n",
    "T 2003-03-25\n",
    "☑ 2003-03-25\n",
    "---\n",
    "Q Tuesday, November 22, 2016   \n",
    "T 2016-11-22\n",
    "☑ 2016-11-22\n",
    "---\n",
    "Q Saturday, July 18, 1970      \n",
    "T 1970-07-18\n",
    "☑ 1970-07-18\n",
    "---\n",
    "Q october 6, 1992              \n",
    "T 1992-10-06\n",
    "☑ 1992-10-06\n",
    "---\n",
    "Q 8/23/08                      \n",
    "T 2008-08-23\n",
    "☑ 2008-08-23\n",
    "---\n",
    "Q 8/30/07                      \n",
    "T 2007-08-30\n",
    "☑ 2007-08-30\n",
    "---\n",
    "Q 10/28/13                     \n",
    "T 2013-10-28\n",
    "☑ 2013-10-28\n",
    "---\n",
    "Q sunday, november 6, 2016     \n",
    "T 2016-11-06\n",
    "☑ 2016-11-06\n",
    "---\n",
    "정확도 99.900%\n",
    "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
    "| 에폭 4 |  반복 21 / 351 | 시간 9[s] | 손실 0.01\n",
    "| 에폭 4 |  반복 41 / 351 | 시간 18[s] | 손실 0.01\n",
    "| 에폭 4 |  반복 61 / 351 | 시간 27[s] | 손실 0.01\n",
    "| 에폭 4 |  반복 81 / 351 | 시간 36[s] | 손실 0.01\n",
    "| 에폭 4 |  반복 101 / 351 | 시간 45[s] | 손실 0.01\n",
    "| 에폭 4 |  반복 121 / 351 | 시간 54[s] | 손실 0.00\n",
    "| 에폭 4 |  반복 141 / 351 | 시간 63[s] | 손실 0.01\n",
    "| 에폭 4 |  반복 161 / 351 | 시간 72[s] | 손실 0.00\n",
    "| 에폭 4 |  반복 181 / 351 | 시간 81[s] | 손실 0.00\n",
    "| 에폭 4 |  반복 201 / 351 | 시간 90[s] | 손실 0.00\n",
    "| 에폭 4 |  반복 221 / 351 | 시간 99[s] | 손실 0.00\n",
    "| 에폭 4 |  반복 241 / 351 | 시간 108[s] | 손실 0.00\n",
    "| 에폭 4 |  반복 261 / 351 | 시간 117[s] | 손실 0.00\n",
    "| 에폭 4 |  반복 281 / 351 | 시간 126[s] | 손실 0.01\n",
    "| 에폭 4 |  반복 301 / 351 | 시간 135[s] | 손실 0.00\n",
    "| 에폭 4 |  반복 321 / 351 | 시간 144[s] | 손실 0.00\n",
    "| 에폭 4 |  반복 341 / 351 | 시간 153[s] | 손실 0.00\n",
    "Q 10/15/94                     \n",
    "T 1994-10-15\n",
    "☑ 1994-10-15\n",
    "---\n",
    "Q thursday, november 13, 2008  \n",
    "T 2008-11-13\n",
    "☑ 2008-11-13\n",
    "---\n",
    "Q Mar 25, 2003                 \n",
    "T 2003-03-25\n",
    "☑ 2003-03-25\n",
    "---\n",
    "Q Tuesday, November 22, 2016   \n",
    "T 2016-11-22\n",
    "☑ 2016-11-22\n",
    "---\n",
    "Q Saturday, July 18, 1970      \n",
    "T 1970-07-18\n",
    "☑ 1970-07-18\n",
    "---\n",
    "Q october 6, 1992              \n",
    "T 1992-10-06\n",
    "☑ 1992-10-06\n",
    "---\n",
    "Q 8/23/08                      \n",
    "T 2008-08-23\n",
    "☑ 2008-08-23\n",
    "---\n",
    "Q 8/30/07                      \n",
    "T 2007-08-30\n",
    "☑ 2007-08-30\n",
    "---\n",
    "Q 10/28/13                     \n",
    "T 2013-10-28\n",
    "☑ 2013-10-28\n",
    "---\n",
    "Q sunday, november 6, 2016     \n",
    "T 2016-11-06\n",
    "☑ 2016-11-06\n",
    "---\n",
    "정확도 99.900%\n",
    "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
    "| 에폭 5 |  반복 21 / 351 | 시간 9[s] | 손실 0.00\n",
    "| 에폭 5 |  반복 41 / 351 | 시간 18[s] | 손실 0.00\n",
    "| 에폭 5 |  반복 61 / 351 | 시간 27[s] | 손실 0.00\n",
    "| 에폭 5 |  반복 81 / 351 | 시간 36[s] | 손실 0.00\n",
    "| 에폭 5 |  반복 101 / 351 | 시간 45[s] | 손실 0.00\n",
    "| 에폭 5 |  반복 121 / 351 | 시간 54[s] | 손실 0.00\n",
    "| 에폭 5 |  반복 141 / 351 | 시간 63[s] | 손실 0.00\n",
    "| 에폭 5 |  반복 161 / 351 | 시간 72[s] | 손실 0.00\n",
    "| 에폭 5 |  반복 181 / 351 | 시간 81[s] | 손실 0.00\n",
    "| 에폭 5 |  반복 201 / 351 | 시간 90[s] | 손실 0.00\n",
    "| 에폭 5 |  반복 221 / 351 | 시간 99[s] | 손실 0.00\n",
    "| 에폭 5 |  반복 241 / 351 | 시간 108[s] | 손실 0.00\n",
    "| 에폭 5 |  반복 261 / 351 | 시간 116[s] | 손실 0.00\n",
    "| 에폭 5 |  반복 281 / 351 | 시간 125[s] | 손실 0.00\n",
    "| 에폭 5 |  반복 301 / 351 | 시간 134[s] | 손실 0.00\n",
    "| 에폭 5 |  반복 321 / 351 | 시간 143[s] | 손실 0.00\n",
    "| 에폭 5 |  반복 341 / 351 | 시간 152[s] | 손실 0.00\n",
    "Q 10/15/94                     \n",
    "T 1994-10-15\n",
    "☑ 1994-10-15\n",
    "---\n",
    "Q thursday, november 13, 2008  \n",
    "T 2008-11-13\n",
    "☑ 2008-11-13\n",
    "---\n",
    "Q Mar 25, 2003                 \n",
    "T 2003-03-25\n",
    "☑ 2003-03-25\n",
    "---\n",
    "Q Tuesday, November 22, 2016   \n",
    "T 2016-11-22\n",
    "☑ 2016-11-22\n",
    "---\n",
    "Q Saturday, July 18, 1970      \n",
    "T 1970-07-18\n",
    "☑ 1970-07-18\n",
    "---\n",
    "Q october 6, 1992              \n",
    "T 1992-10-06\n",
    "☑ 1992-10-06\n",
    "---\n",
    "Q 8/23/08                      \n",
    "T 2008-08-23\n",
    "☑ 2008-08-23\n",
    "---\n",
    "Q 8/30/07                      \n",
    "T 2007-08-30\n",
    "☑ 2007-08-30\n",
    "---\n",
    "Q 10/28/13                     \n",
    "T 2013-10-28\n",
    "☑ 2013-10-28\n",
    "---\n",
    "Q sunday, november 6, 2016     \n",
    "T 2016-11-06\n",
    "☑ 2016-11-06\n",
    "---\n",
    "정확도 99.920%\n",
    "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
    "| 에폭 6 |  반복 21 / 351 | 시간 9[s] | 손실 0.00\n",
    "| 에폭 6 |  반복 41 / 351 | 시간 18[s] | 손실 0.00\n",
    "| 에폭 6 |  반복 61 / 351 | 시간 27[s] | 손실 0.00\n",
    "| 에폭 6 |  반복 81 / 351 | 시간 36[s] | 손실 0.00\n",
    "| 에폭 6 |  반복 101 / 351 | 시간 45[s] | 손실 0.00\n",
    "| 에폭 6 |  반복 121 / 351 | 시간 53[s] | 손실 0.00\n",
    "| 에폭 6 |  반복 141 / 351 | 시간 62[s] | 손실 0.00\n",
    "| 에폭 6 |  반복 161 / 351 | 시간 71[s] | 손실 0.00\n",
    "| 에폭 6 |  반복 181 / 351 | 시간 80[s] | 손실 0.00\n",
    "| 에폭 6 |  반복 201 / 351 | 시간 89[s] | 손실 0.00\n",
    "| 에폭 6 |  반복 221 / 351 | 시간 98[s] | 손실 0.00\n",
    "| 에폭 6 |  반복 241 / 351 | 시간 107[s] | 손실 0.00\n",
    "| 에폭 6 |  반복 261 / 351 | 시간 117[s] | 손실 0.00\n",
    "| 에폭 6 |  반복 281 / 351 | 시간 125[s] | 손실 0.00\n",
    "| 에폭 6 |  반복 301 / 351 | 시간 134[s] | 손실 0.00\n",
    "| 에폭 6 |  반복 321 / 351 | 시간 143[s] | 손실 0.00\n",
    "| 에폭 6 |  반복 341 / 351 | 시간 152[s] | 손실 0.00\n",
    "Q 10/15/94                     \n",
    "T 1994-10-15\n",
    "☑ 1994-10-15\n",
    "---\n",
    "Q thursday, november 13, 2008  \n",
    "T 2008-11-13\n",
    "☑ 2008-11-13\n",
    "---\n",
    "Q Mar 25, 2003                 \n",
    "T 2003-03-25\n",
    "☑ 2003-03-25\n",
    "---\n",
    "Q Tuesday, November 22, 2016   \n",
    "T 2016-11-22\n",
    "☑ 2016-11-22\n",
    "---\n",
    "Q Saturday, July 18, 1970      \n",
    "T 1970-07-18\n",
    "☑ 1970-07-18\n",
    "---\n",
    "Q october 6, 1992              \n",
    "T 1992-10-06\n",
    "☑ 1992-10-06\n",
    "---\n",
    "Q 8/23/08                      \n",
    "T 2008-08-23\n",
    "☑ 2008-08-23\n",
    "---\n",
    "Q 8/30/07                      \n",
    "T 2007-08-30\n",
    "☑ 2007-08-30\n",
    "---\n",
    "Q 10/28/13                     \n",
    "T 2013-10-28\n",
    "☑ 2013-10-28\n",
    "---\n",
    "Q sunday, november 6, 2016     \n",
    "T 2016-11-06\n",
    "☑ 2016-11-06\n",
    "---\n",
    "정확도 99.920%\n",
    "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
    "| 에폭 7 |  반복 21 / 351 | 시간 9[s] | 손실 0.00\n",
    "| 에폭 7 |  반복 41 / 351 | 시간 18[s] | 손실 0.00\n",
    "| 에폭 7 |  반복 61 / 351 | 시간 27[s] | 손실 0.00\n",
    "| 에폭 7 |  반복 81 / 351 | 시간 36[s] | 손실 0.00\n",
    "| 에폭 7 |  반복 101 / 351 | 시간 44[s] | 손실 0.00\n",
    "| 에폭 7 |  반복 121 / 351 | 시간 53[s] | 손실 0.00\n",
    "| 에폭 7 |  반복 141 / 351 | 시간 62[s] | 손실 0.00\n",
    "| 에폭 7 |  반복 161 / 351 | 시간 71[s] | 손실 0.00\n",
    "| 에폭 7 |  반복 181 / 351 | 시간 80[s] | 손실 0.00\n",
    "| 에폭 7 |  반복 201 / 351 | 시간 89[s] | 손실 0.00\n",
    "| 에폭 7 |  반복 221 / 351 | 시간 98[s] | 손실 0.00\n",
    "| 에폭 7 |  반복 241 / 351 | 시간 107[s] | 손실 0.00\n",
    "| 에폭 7 |  반복 261 / 351 | 시간 116[s] | 손실 0.00\n",
    "| 에폭 7 |  반복 281 / 351 | 시간 125[s] | 손실 0.00\n",
    "| 에폭 7 |  반복 301 / 351 | 시간 133[s] | 손실 0.00\n",
    "| 에폭 7 |  반복 321 / 351 | 시간 142[s] | 손실 0.00\n",
    "| 에폭 7 |  반복 341 / 351 | 시간 152[s] | 손실 0.00\n",
    "Q 10/15/94                     \n",
    "T 1994-10-15\n",
    "☑ 1994-10-15\n",
    "---\n",
    "Q thursday, november 13, 2008  \n",
    "T 2008-11-13\n",
    "☑ 2008-11-13\n",
    "---\n",
    "Q Mar 25, 2003                 \n",
    "T 2003-03-25\n",
    "☑ 2003-03-25\n",
    "---\n",
    "Q Tuesday, November 22, 2016   \n",
    "T 2016-11-22\n",
    "☑ 2016-11-22\n",
    "---\n",
    "Q Saturday, July 18, 1970      \n",
    "T 1970-07-18\n",
    "☑ 1970-07-18\n",
    "---\n",
    "Q october 6, 1992              \n",
    "T 1992-10-06\n",
    "☑ 1992-10-06\n",
    "---\n",
    "Q 8/23/08                      \n",
    "T 2008-08-23\n",
    "☑ 2008-08-23\n",
    "---\n",
    "Q 8/30/07                      \n",
    "T 2007-08-30\n",
    "☑ 2007-08-30\n",
    "---\n",
    "Q 10/28/13                     \n",
    "T 2013-10-28\n",
    "☑ 2013-10-28\n",
    "---\n",
    "Q sunday, november 6, 2016     \n",
    "T 2016-11-06\n",
    "☑ 2016-11-06\n",
    "---\n",
    "정확도 99.920%\n",
    "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
    "| 에폭 8 |  반복 21 / 351 | 시간 9[s] | 손실 0.00\n",
    "| 에폭 8 |  반복 41 / 351 | 시간 18[s] | 손실 0.00\n",
    "| 에폭 8 |  반복 61 / 351 | 시간 27[s] | 손실 0.00\n",
    "| 에폭 8 |  반복 81 / 351 | 시간 35[s] | 손실 0.00\n",
    "| 에폭 8 |  반복 101 / 351 | 시간 44[s] | 손실 0.00\n",
    "| 에폭 8 |  반복 121 / 351 | 시간 53[s] | 손실 0.00\n",
    "| 에폭 8 |  반복 141 / 351 | 시간 62[s] | 손실 0.00\n",
    "| 에폭 8 |  반복 161 / 351 | 시간 71[s] | 손실 0.00\n",
    "| 에폭 8 |  반복 181 / 351 | 시간 80[s] | 손실 0.00\n",
    "| 에폭 8 |  반복 201 / 351 | 시간 89[s] | 손실 0.00\n",
    "| 에폭 8 |  반복 221 / 351 | 시간 98[s] | 손실 0.00\n",
    "| 에폭 8 |  반복 241 / 351 | 시간 106[s] | 손실 0.00\n",
    "| 에폭 8 |  반복 261 / 351 | 시간 115[s] | 손실 0.00\n",
    "| 에폭 8 |  반복 281 / 351 | 시간 124[s] | 손실 0.00\n",
    "| 에폭 8 |  반복 301 / 351 | 시간 133[s] | 손실 0.00\n",
    "| 에폭 8 |  반복 321 / 351 | 시간 142[s] | 손실 0.00\n",
    "| 에폭 8 |  반복 341 / 351 | 시간 150[s] | 손실 0.00\n",
    "Q 10/15/94                     \n",
    "T 1994-10-15\n",
    "☑ 1994-10-15\n",
    "---\n",
    "Q thursday, november 13, 2008  \n",
    "T 2008-11-13\n",
    "☑ 2008-11-13\n",
    "---\n",
    "Q Mar 25, 2003                 \n",
    "T 2003-03-25\n",
    "☑ 2003-03-25\n",
    "---\n",
    "Q Tuesday, November 22, 2016   \n",
    "T 2016-11-22\n",
    "☑ 2016-11-22\n",
    "---\n",
    "Q Saturday, July 18, 1970      \n",
    "T 1970-07-18\n",
    "☑ 1970-07-18\n",
    "---\n",
    "Q october 6, 1992              \n",
    "T 1992-10-06\n",
    "☑ 1992-10-06\n",
    "---\n",
    "Q 8/23/08                      \n",
    "T 2008-08-23\n",
    "☑ 2008-08-23\n",
    "---\n",
    "Q 8/30/07                      \n",
    "T 2007-08-30\n",
    "☑ 2007-08-30\n",
    "---\n",
    "Q 10/28/13                     \n",
    "T 2013-10-28\n",
    "☑ 2013-10-28\n",
    "---\n",
    "Q sunday, november 6, 2016     \n",
    "T 2016-11-06\n",
    "☑ 2016-11-06\n",
    "---\n",
    "정확도 99.960%\n",
    "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
    "| 에폭 9 |  반복 21 / 351 | 시간 9[s] | 손실 0.00\n",
    "| 에폭 9 |  반복 41 / 351 | 시간 18[s] | 손실 0.00\n",
    "| 에폭 9 |  반복 61 / 351 | 시간 27[s] | 손실 0.00\n",
    "| 에폭 9 |  반복 81 / 351 | 시간 36[s] | 손실 0.00\n",
    "| 에폭 9 |  반복 101 / 351 | 시간 44[s] | 손실 0.00\n",
    "| 에폭 9 |  반복 121 / 351 | 시간 53[s] | 손실 0.00\n",
    "| 에폭 9 |  반복 141 / 351 | 시간 62[s] | 손실 0.00\n",
    "| 에폭 9 |  반복 161 / 351 | 시간 71[s] | 손실 0.00\n",
    "| 에폭 9 |  반복 181 / 351 | 시간 80[s] | 손실 0.00\n",
    "| 에폭 9 |  반복 201 / 351 | 시간 89[s] | 손실 0.00\n",
    "| 에폭 9 |  반복 221 / 351 | 시간 98[s] | 손실 0.00\n",
    "| 에폭 9 |  반복 241 / 351 | 시간 107[s] | 손실 0.00\n",
    "| 에폭 9 |  반복 261 / 351 | 시간 116[s] | 손실 0.00\n",
    "| 에폭 9 |  반복 281 / 351 | 시간 125[s] | 손실 0.00\n",
    "| 에폭 9 |  반복 301 / 351 | 시간 133[s] | 손실 0.00\n",
    "| 에폭 9 |  반복 321 / 351 | 시간 142[s] | 손실 0.00\n",
    "| 에폭 9 |  반복 341 / 351 | 시간 151[s] | 손실 0.00\n",
    "Q 10/15/94                     \n",
    "T 1994-10-15\n",
    "☑ 1994-10-15\n",
    "---\n",
    "Q thursday, november 13, 2008  \n",
    "T 2008-11-13\n",
    "☑ 2008-11-13\n",
    "---\n",
    "Q Mar 25, 2003                 \n",
    "T 2003-03-25\n",
    "☑ 2003-03-25\n",
    "---\n",
    "Q Tuesday, November 22, 2016   \n",
    "T 2016-11-22\n",
    "☑ 2016-11-22\n",
    "---\n",
    "Q Saturday, July 18, 1970      \n",
    "T 1970-07-18\n",
    "☑ 1970-07-18\n",
    "---\n",
    "Q october 6, 1992              \n",
    "T 1992-10-06\n",
    "☑ 1992-10-06\n",
    "---\n",
    "Q 8/23/08                      \n",
    "T 2008-08-23\n",
    "☑ 2008-08-23\n",
    "---\n",
    "Q 8/30/07                      \n",
    "T 2007-08-30\n",
    "☑ 2007-08-30\n",
    "---\n",
    "Q 10/28/13                     \n",
    "T 2013-10-28\n",
    "☑ 2013-10-28\n",
    "---\n",
    "Q sunday, november 6, 2016     \n",
    "T 2016-11-06\n",
    "☑ 2016-11-06\n",
    "---\n",
    "정확도 99.960%\n",
    "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
    "| 에폭 10 |  반복 21 / 351 | 시간 9[s] | 손실 0.00\n",
    "| 에폭 10 |  반복 41 / 351 | 시간 18[s] | 손실 0.00\n",
    "| 에폭 10 |  반복 61 / 351 | 시간 27[s] | 손실 0.00\n",
    "| 에폭 10 |  반복 81 / 351 | 시간 36[s] | 손실 0.00\n",
    "| 에폭 10 |  반복 101 / 351 | 시간 44[s] | 손실 0.00\n",
    "| 에폭 10 |  반복 121 / 351 | 시간 53[s] | 손실 0.00\n",
    "| 에폭 10 |  반복 141 / 351 | 시간 62[s] | 손실 0.00\n",
    "| 에폭 10 |  반복 161 / 351 | 시간 71[s] | 손실 0.00\n",
    "| 에폭 10 |  반복 181 / 351 | 시간 80[s] | 손실 0.00\n",
    "| 에폭 10 |  반복 201 / 351 | 시간 89[s] | 손실 0.00\n",
    "| 에폭 10 |  반복 221 / 351 | 시간 98[s] | 손실 0.00\n",
    "| 에폭 10 |  반복 241 / 351 | 시간 107[s] | 손실 0.00\n",
    "| 에폭 10 |  반복 261 / 351 | 시간 116[s] | 손실 0.00\n",
    "| 에폭 10 |  반복 281 / 351 | 시간 124[s] | 손실 0.00\n",
    "| 에폭 10 |  반복 301 / 351 | 시간 133[s] | 손실 0.00\n",
    "| 에폭 10 |  반복 321 / 351 | 시간 142[s] | 손실 0.00\n",
    "| 에폭 10 |  반복 341 / 351 | 시간 151[s] | 손실 0.00\n",
    "Q 10/15/94                     \n",
    "T 1994-10-15\n",
    "☑ 1994-10-15\n",
    "---\n",
    "Q thursday, november 13, 2008  \n",
    "T 2008-11-13\n",
    "☑ 2008-11-13\n",
    "---\n",
    "Q Mar 25, 2003                 \n",
    "T 2003-03-25\n",
    "☑ 2003-03-25\n",
    "---\n",
    "Q Tuesday, November 22, 2016   \n",
    "T 2016-11-22\n",
    "☑ 2016-11-22\n",
    "---\n",
    "Q Saturday, July 18, 1970      \n",
    "T 1970-07-18\n",
    "☑ 1970-07-18\n",
    "---\n",
    "Q october 6, 1992              \n",
    "T 1992-10-06\n",
    "☑ 1992-10-06\n",
    "---\n",
    "Q 8/23/08                      \n",
    "T 2008-08-23\n",
    "☑ 2008-08-23\n",
    "---\n",
    "Q 8/30/07                      \n",
    "T 2007-08-30\n",
    "☑ 2007-08-30\n",
    "---\n",
    "Q 10/28/13                     \n",
    "T 2013-10-28\n",
    "☑ 2013-10-28\n",
    "---\n",
    "Q sunday, november 6, 2016     \n",
    "T 2016-11-06\n",
    "☑ 2016-11-06\n",
    "---\n",
    "정확도 99.960%\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize_attention.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from dataset import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "#from attention_seq2seq import AttentionSeq2seq\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 입력 문장 반전\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "model.load_params()\n",
    "\n",
    "_idx = 0\n",
    "def visualize(attention_map, row_labels, column_labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pcolor(attention_map, cmap=plt.cm.Greys_r, vmin=0.0, vmax=1.0)\n",
    "\n",
    "    ax.patch.set_facecolor('black')\n",
    "    ax.set_yticks(np.arange(attention_map.shape[0])+0.5, minor=False)\n",
    "    ax.set_xticks(np.arange(attention_map.shape[1])+0.5, minor=False)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xticklabels(row_labels, minor=False)\n",
    "    ax.set_yticklabels(column_labels, minor=False)\n",
    "\n",
    "    global _idx\n",
    "    _idx += 1\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "np.random.seed(1984)\n",
    "for _ in range(5):\n",
    "    idx = [np.random.randint(0, len(x_test))]\n",
    "    x = x_test[idx]\n",
    "    t = t_test[idx]\n",
    "\n",
    "    model.forward(x, t)\n",
    "    d = model.decoder.attention.attention_weights\n",
    "    d = np.array(d)\n",
    "    attention_map = d.reshape(d.shape[0], d.shape[2])\n",
    "\n",
    "    # 출력하기 위해 반전\n",
    "    attention_map = attention_map[:,::-1]\n",
    "    x = x[:,::-1]\n",
    "\n",
    "    row_labels = [id_to_char[i] for i in x[0]]\n",
    "    column_labels = [id_to_char[i] for i in t[0]]\n",
    "    column_labels = column_labels[1:]\n",
    "\n",
    "    visualize(attention_map, row_labels, column_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# common_Encoder_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================#\n",
    "#from ch07.seq2seq import Encoder, Seq2seq\n",
    "#================================================================================#\n",
    "\n",
    "\n",
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "#from common.time_layers import *\n",
    "#from common.base_model import BaseModel\n",
    "\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)\n",
    "\n",
    "        self.params = self.embed.params + self.lstm.params\n",
    "        self.grads = self.embed.grads + self.lstm.grads\n",
    "        self.hs = None\n",
    "\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        self.hs = hs\n",
    "        return hs[:, -1, :]\n",
    "\n",
    "    def backward(self, dh):\n",
    "        dhs = np.zeros_like(self.hs)\n",
    "        dhs[:, -1, :] = dh\n",
    "\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout\n",
    "\n",
    "\n",
    "class Decoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in (self.embed, self.lstm, self.affine):\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, h):\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        out = self.lstm.forward(out)\n",
    "        score = self.affine.forward(out)\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        dout = self.lstm.backward(dout)\n",
    "        dout = self.embed.backward(dout)\n",
    "        dh = self.lstm.dh\n",
    "        return dh\n",
    "\n",
    "    def generate(self, h, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array(sample_id).reshape((1, 1))\n",
    "            out = self.embed.forward(x)\n",
    "            out = self.lstm.forward(out)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(int(sample_id))\n",
    "\n",
    "        return sampled\n",
    "\n",
    "\n",
    "class Seq2seq(BaseModel):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = Encoder(V, D, H)\n",
    "        self.decoder = Decoder(V, D, H)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]\n",
    "\n",
    "        h = self.encoder.forward(xs)\n",
    "        score = self.decoder.forward(decoder_xs, h)\n",
    "        loss = self.softmax.forward(score, decoder_ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.softmax.backward(dout)\n",
    "        dh = self.decoder.backward(dout)\n",
    "        dout = self.encoder.backward(dh)\n",
    "        return dout\n",
    "\n",
    "    def generate(self, xs, start_id, sample_size):\n",
    "        h = self.encoder.forward(xs)\n",
    "        sampled = self.decoder.generate(h, start_id, sample_size)\n",
    "        return sampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================#\n",
    "#from ch07.peeky_seq2seq import PeekySeq2seq\n",
    "#================================================================================#\n",
    "\n",
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "#from common.time_layers import *\n",
    "#from seq2seq import Seq2seq, Encoder\n",
    "\n",
    "\n",
    "class PeekyDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(H + D, 4 * H) / np.sqrt(H + D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(H + H, V) / np.sqrt(H + H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in (self.embed, self.lstm, self.affine):\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, xs, h):\n",
    "        N, T = xs.shape\n",
    "        N, H = h.shape\n",
    "\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        hs = np.repeat(h, T, axis=0).reshape(N, T, H)\n",
    "        out = np.concatenate((hs, out), axis=2)\n",
    "\n",
    "        out = self.lstm.forward(out)\n",
    "        out = np.concatenate((hs, out), axis=2)\n",
    "\n",
    "        score = self.affine.forward(out)\n",
    "        self.cache = H\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        H = self.cache\n",
    "\n",
    "        dout = self.affine.backward(dscore)\n",
    "        dout, dhs0 = dout[:, :, H:], dout[:, :, :H]\n",
    "        dout = self.lstm.backward(dout)\n",
    "        dembed, dhs1 = dout[:, :, H:], dout[:, :, :H]\n",
    "        self.embed.backward(dembed)\n",
    "\n",
    "        dhs = dhs0 + dhs1\n",
    "        dh = self.lstm.dh + np.sum(dhs, axis=1)\n",
    "        return dh\n",
    "\n",
    "    def generate(self, h, start_id, sample_size):\n",
    "        sampled = []\n",
    "        char_id = start_id\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        H = h.shape[1]\n",
    "        peeky_h = h.reshape(1, 1, H)\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([char_id]).reshape((1, 1))\n",
    "            out = self.embed.forward(x)\n",
    "\n",
    "            out = np.concatenate((peeky_h, out), axis=2)\n",
    "            out = self.lstm.forward(out)\n",
    "            out = np.concatenate((peeky_h, out), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            char_id = np.argmax(score.flatten())\n",
    "            sampled.append(char_id)\n",
    "\n",
    "        return sampled\n",
    "\n",
    "\n",
    "class PeekySeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = Encoder(V, D, H)\n",
    "        self.decoder = PeekyDecoder(V, D, H)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# common_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================================#\n",
    "#base_model.py\n",
    "#================================================================================#\n",
    "\n",
    "\n",
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import pickle\n",
    "#from common.np import *\n",
    "#from common.util import to_gpu, to_cpu\n",
    "\n",
    "\n",
    "class BaseModel:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = None, None\n",
    "\n",
    "    def forward(self, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save_params(self, file_name=None):\n",
    "        if file_name is None:\n",
    "            file_name = self.__class__.__name__ + '.pkl'\n",
    "\n",
    "        params = [p.astype(np.float16) for p in self.params]\n",
    "        if GPU:\n",
    "            params = [to_cpu(p) for p in params]\n",
    "\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=None):\n",
    "        if file_name is None:\n",
    "            file_name = self.__class__.__name__ + '.pkl'\n",
    "\n",
    "        if '/' in file_name:\n",
    "            file_name = file_name.replace('/', os.sep)\n",
    "\n",
    "        if not os.path.exists(file_name):\n",
    "            raise IOError('No file: ' + file_name)\n",
    "\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "\n",
    "        params = [p.astype('f') for p in params]\n",
    "        if GPU:\n",
    "            params = [to_gpu(p) for p in params]\n",
    "\n",
    "        for i, param in enumerate(self.params):\n",
    "            param[...] = params[i]\n",
    "\n",
    "\n",
    "#================================================================================#\n",
    "#config.py\n",
    "#================================================================================#\n",
    "# coding: utf-8\n",
    "\n",
    "GPU = False\n",
    "\n",
    "#================================================================================#\n",
    "#functions.py\n",
    "#================================================================================#\n",
    "# coding: utf-8\n",
    "#from common.np import *\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 정답 데이터가 원핫 벡터일 경우 정답 레이블 인덱스로 변환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "#================================================================================#\n",
    "#layers.py\n",
    "#================================================================================#\n",
    "\n",
    "# coding: utf-8\n",
    "#from common.np import *  # import numpy as np\n",
    "#from common.config import GPU\n",
    "#from common.functions import softmax, cross_entropy_error\n",
    "\n",
    "\n",
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.out = softmax(x)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = self.out * dout\n",
    "        sumdx = np.sum(dx, axis=1, keepdims=True)\n",
    "        dx -= self.out * sumdx\n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None  # softmax의 출력\n",
    "        self.t = None  # 정답 레이블\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "\n",
    "        # 정답 레이블이 원핫 벡터일 경우 정답의 인덱스로 변환\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "\n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = self.y.copy()\n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx = dx / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx\n",
    "\n",
    "#================================================================================#\n",
    "#layers.py\n",
    "#================================================================================#\n",
    "\n",
    "class SigmoidWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.loss = None\n",
    "        self.y = None  # sigmoid의 출력\n",
    "        self.t = None  # 정답 데이터\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "        self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = (self.y - self.t) * dout / batch_size\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Dropout:\n",
    "    '''\n",
    "    http://arxiv.org/abs/1207.0580\n",
    "    '''\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.params, self.grads = [], []\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "\n",
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "        return None\n",
    "\n",
    "\n",
    "#================================================================================#\n",
    "#np.py\n",
    "#================================================================================#\n",
    "\n",
    "# coding: utf-8\n",
    "#from common.config import GPU\n",
    "\n",
    "\n",
    "if GPU:\n",
    "    import cupy as np\n",
    "    np.cuda.set_allocator(np.cuda.MemoryPool().malloc)\n",
    "    np.add.at = np.scatter_add\n",
    "\n",
    "    print('\\033[92m' + '-' * 60 + '\\033[0m')\n",
    "    print(' ' * 23 + '\\033[92mGPU Mode (cupy)\\033[0m')\n",
    "    print('\\033[92m' + '-' * 60 + '\\033[0m\\n')\n",
    "else:\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "#================================================================================#\n",
    "#optimizer.py\n",
    "#================================================================================#\n",
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "#from common.np import *\n",
    "\n",
    "\n",
    "class SGD:\n",
    "    '''\n",
    "    확률적 경사하강법(Stochastic Gradient Descent)\n",
    "    '''\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]\n",
    "\n",
    "\n",
    "class Momentum:\n",
    "    '''\n",
    "    모멘텀 SGG(Momentum SGD)\n",
    "    '''\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = []\n",
    "            for param in params:\n",
    "                self.v.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.v[i] = self.momentum * self.v[i] - self.lr * grads[i]\n",
    "            params[i] += self.v[i]\n",
    "\n",
    "\n",
    "class Nesterov:\n",
    "    '''\n",
    "    네스테로프 가속 경사(NAG; Nesterov's Accelerated Gradient) (http://arxiv.org/abs/1212.0901)\n",
    "    '네스테로프 모멘텀 최적화'라고도 한다.\n",
    "    '''\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = []\n",
    "            for param in params:\n",
    "                self.v.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.v[i] *= self.momentum\n",
    "            self.v[i] -= self.lr * grads[i]\n",
    "            params[i] += self.momentum * self.momentum * self.v[i]\n",
    "            params[i] -= (1 + self.momentum) * self.lr * grads[i]\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "    '''\n",
    "    AdaGrad\n",
    "    '''\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = []\n",
    "            for param in params:\n",
    "                self.h.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.h[i] += grads[i] * grads[i]\n",
    "            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)\n",
    "\n",
    "\n",
    "class RMSprop:\n",
    "    '''\n",
    "    RMSprop\n",
    "    '''\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = []\n",
    "            for param in params:\n",
    "                self.h.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.h[i] *= self.decay_rate\n",
    "            self.h[i] += (1 - self.decay_rate) * grads[i] * grads[i]\n",
    "            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    '''\n",
    "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
    "    '''\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "            \n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)\n",
    "\n",
    "#================================================================================#\n",
    "#time_layers.py\n",
    "#================================================================================#\n",
    "# coding: utf-8\n",
    "#from common.np import *  # import numpy as np (or import cupy as np)\n",
    "#from common.layers import *\n",
    "#from common.functions import sigmoid\n",
    "\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
    "        h_next = np.tanh(t)\n",
    "\n",
    "        self.cache = (x, h_prev, h_next)\n",
    "        return h_next\n",
    "\n",
    "    def backward(self, dh_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, h_next = self.cache\n",
    "\n",
    "        dt = dh_next * (1 - h_next ** 2)\n",
    "        db = np.sum(dt, axis=0)\n",
    "        dWh = np.dot(h_prev.T, dt)\n",
    "        dh_prev = np.dot(dt, Wh.T)\n",
    "        dWx = np.dot(x.T, dt)\n",
    "        dx = np.dot(dt, Wx.T)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        return dx, dh_prev\n",
    "\n",
    "\n",
    "class TimeRNN:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.dh = None, None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        D, H = Wx.shape\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = RNN(*self.params)\n",
    "            self.h = layer.forward(xs[:, t, :], self.h)\n",
    "            hs[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D, H = Wx.shape\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh = 0\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
    "            dxs[:, t, :] = dx\n",
    "\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        '''\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Wx: 입력 x에 대한 가중치 매개변수(4개분의 가중치가 담겨 있음)\n",
    "        Wh: 은닉 상태 h에 대한 가장추 매개변수(4개분의 가중치가 담겨 있음)\n",
    "        b: 편향（4개분의 편향이 담겨 있음）\n",
    "        '''\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, H = h_prev.shape\n",
    "\n",
    "        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n",
    "\n",
    "        f = A[:, :H]\n",
    "        g = A[:, H:2*H]\n",
    "        i = A[:, 2*H:3*H]\n",
    "        o = A[:, 3*H:]\n",
    "\n",
    "        f = sigmoid(f)\n",
    "        g = np.tanh(g)\n",
    "        i = sigmoid(i)\n",
    "        o = sigmoid(o)\n",
    "\n",
    "        c_next = f * c_prev + g * i\n",
    "        h_next = o * np.tanh(c_next)\n",
    "\n",
    "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "    def backward(self, dh_next, dc_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
    "\n",
    "        tanh_c_next = np.tanh(c_next)\n",
    "\n",
    "        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "\n",
    "        dc_prev = ds * f\n",
    "\n",
    "        di = ds * g\n",
    "        df = ds * c_prev\n",
    "        do = dh_next * tanh_c_next\n",
    "        dg = ds * i\n",
    "\n",
    "        di *= i * (1 - i)\n",
    "        df *= f * (1 - f)\n",
    "        do *= o * (1 - o)\n",
    "        dg *= (1 - g ** 2)\n",
    "\n",
    "        dA = np.hstack((df, dg, di, do))\n",
    "\n",
    "        dWh = np.dot(h_prev.T, dA)\n",
    "        dWx = np.dot(x.T, dA)\n",
    "        db = dA.sum(axis=0)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        dx = np.dot(dA, Wx.T)\n",
    "        dh_prev = np.dot(dA, Wh.T)\n",
    "\n",
    "        return dx, dh_prev, dc_prev\n",
    "\n",
    "\n",
    "class TimeLSTM:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.c = None, None\n",
    "        self.dh = None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        H = Wh.shape[0]\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "        if not self.stateful or self.c is None:\n",
    "            self.c = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = LSTM(*self.params)\n",
    "            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n",
    "            hs[:, t, :] = self.h\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D = Wx.shape[0]\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh, dc = 0, 0\n",
    "\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n",
    "            dxs[:, t, :] = dx\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h, c=None):\n",
    "        self.h, self.c = h, c\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h, self.c = None, None\n",
    "\n",
    "\n",
    "class TimeEmbedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.layers = None\n",
    "        self.W = W\n",
    "\n",
    "    def forward(self, xs):\n",
    "        N, T = xs.shape\n",
    "        V, D = self.W.shape\n",
    "\n",
    "        out = np.empty((N, T, D), dtype='f')\n",
    "        self.layers = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Embedding(self.W)\n",
    "            out[:, t, :] = layer.forward(xs[:, t])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, D = dout.shape\n",
    "\n",
    "        grad = 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            layer.backward(dout[:, t, :])\n",
    "            grad += layer.grads[0]\n",
    "\n",
    "        self.grads[0][...] = grad\n",
    "        return None\n",
    "\n",
    "\n",
    "class TimeAffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        rx = x.reshape(N*T, -1)\n",
    "        out = np.dot(rx, W) + b\n",
    "        self.x = x\n",
    "        return out.reshape(N, T, -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        x = self.x\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        dout = dout.reshape(N*T, -1)\n",
    "        rx = x.reshape(N*T, -1)\n",
    "\n",
    "        db = np.sum(dout, axis=0)\n",
    "        dW = np.dot(rx.T, dout)\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dx = dx.reshape(*x.shape)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class TimeSoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        self.ignore_label = -1\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T, V = xs.shape\n",
    "\n",
    "        if ts.ndim == 3:  # 정답 레이블이 원핫 벡터인 경우\n",
    "            ts = ts.argmax(axis=2)\n",
    "\n",
    "        mask = (ts != self.ignore_label)\n",
    "\n",
    "        # 배치용과 시계열용을 정리(reshape)\n",
    "        xs = xs.reshape(N * T, V)\n",
    "        ts = ts.reshape(N * T)\n",
    "        mask = mask.reshape(N * T)\n",
    "\n",
    "        ys = softmax(xs)\n",
    "        ls = np.log(ys[np.arange(N * T), ts])\n",
    "        ls *= mask  # ignore_label에 해당하는 데이터는 손실을 0으로 설정\n",
    "        loss = -np.sum(ls)\n",
    "        loss /= mask.sum()\n",
    "\n",
    "        self.cache = (ts, ys, mask, (N, T, V))\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ts, ys, mask, (N, T, V) = self.cache\n",
    "\n",
    "        dx = ys\n",
    "        dx[np.arange(N * T), ts] -= 1\n",
    "        dx *= dout\n",
    "        dx /= mask.sum()\n",
    "        dx *= mask[:, np.newaxis]  # ignore_labelㅇㅔ 해당하는 데이터는 기울기를 0으로 설정\n",
    "\n",
    "        dx = dx.reshape((N, T, V))\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class TimeDropout:\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.params, self.grads = [], []\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "        self.train_flg = True\n",
    "\n",
    "    def forward(self, xs):\n",
    "        if self.train_flg:\n",
    "            flg = np.random.rand(*xs.shape) > self.dropout_ratio\n",
    "            scale = 1 / (1.0 - self.dropout_ratio)\n",
    "            self.mask = flg.astype(np.float32) * scale\n",
    "\n",
    "            return xs * self.mask\n",
    "        else:\n",
    "            return xs\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "\n",
    "class TimeBiLSTM:\n",
    "    def __init__(self, Wx1, Wh1, b1,\n",
    "                 Wx2, Wh2, b2, stateful=False):\n",
    "        self.forward_lstm = TimeLSTM(Wx1, Wh1, b1, stateful)\n",
    "        self.backward_lstm = TimeLSTM(Wx2, Wh2, b2, stateful)\n",
    "        self.params = self.forward_lstm.params + self.backward_lstm.params\n",
    "        self.grads = self.forward_lstm.grads + self.backward_lstm.grads\n",
    "\n",
    "    def forward(self, xs):\n",
    "        o1 = self.forward_lstm.forward(xs)\n",
    "        o2 = self.backward_lstm.forward(xs[:, ::-1])\n",
    "        o2 = o2[:, ::-1]\n",
    "\n",
    "        out = np.concatenate((o1, o2), axis=2)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        H = dhs.shape[2] // 2\n",
    "        do1 = dhs[:, :, :H]\n",
    "        do2 = dhs[:, :, H:]\n",
    "\n",
    "        dxs1 = self.forward_lstm.backward(do1)\n",
    "        do2 = do2[:, ::-1]\n",
    "        dxs2 = self.backward_lstm.backward(do2)\n",
    "        dxs2 = dxs2[:, ::-1]\n",
    "        dxs = dxs1 + dxs2\n",
    "        return dxs\n",
    "\n",
    "# ====================================================================== #\n",
    "# 이 아래의 계층들은 책에서 설명하지 않았거나\n",
    "# 처리 속도보다는 쉽게 이해할 수 있도록 구현했습니다.\n",
    "#\n",
    "# TimeSigmoidWithLoss: 시계열 데이터용 시그모이드 + 손실 계층\n",
    "# GRU: GRU 계층\n",
    "# TimeGRU: 시계열 데이터용 GRU 계층\n",
    "# BiTimeLSTM: 양방향 LSTM 계층\n",
    "# Simple_TimeSoftmaxWithLoss：간단한 TimeSoftmaxWithLoss 계층의 구현\n",
    "# Simple_TimeAffine: 간단한 TimeAffine 계층의 구현\n",
    "# ====================================================================== #\n",
    "\n",
    "\n",
    "class TimeSigmoidWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.xs_shape = None\n",
    "        self.layers = None\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T = xs.shape\n",
    "        self.xs_shape = xs.shape\n",
    "\n",
    "        self.layers = []\n",
    "        loss = 0\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = SigmoidWithLoss()\n",
    "            loss += layer.forward(xs[:, t], ts[:, t])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return loss / T\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        N, T = self.xs_shape\n",
    "        dxs = np.empty(self.xs_shape, dtype='f')\n",
    "\n",
    "        dout *= 1/T\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dxs[:, t] = layer.backward(dout)\n",
    "\n",
    "        return dxs\n",
    "\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, Wx, Wh):\n",
    "        '''\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        Wx: 입력 x에 대한 가중치 매개변수(3개 분의 가중치가 담겨 있음)\n",
    "        Wh: 은닉 상태 h에 대한 가중치 매개변수(3개 분의 가중치가 담겨 있음)\n",
    "        '''\n",
    "        self.Wx, self.Wh = Wx, Wh\n",
    "        self.dWx, self.dWh = None, None\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        H, H3 = self.Wh.shape\n",
    "        Wxz, Wxr, Wx = self.Wx[:, :H], self.Wx[:, H:2 * H], self.Wx[:, 2 * H:]\n",
    "        Whz, Whr, Wh = self.Wh[:, :H], self.Wh[:, H:2 * H], self.Wh[:, 2 * H:]\n",
    "\n",
    "        z = sigmoid(np.dot(x, Wxz) + np.dot(h_prev, Whz))\n",
    "        r = sigmoid(np.dot(x, Wxr) + np.dot(h_prev, Whr))\n",
    "        h_hat = np.tanh(np.dot(x, Wx) + np.dot(r*h_prev, Wh))\n",
    "        h_next = (1-z) * h_prev + z * h_hat\n",
    "\n",
    "        self.cache = (x, h_prev, z, r, h_hat)\n",
    "\n",
    "        return h_next\n",
    "\n",
    "    def backward(self, dh_next):\n",
    "        H, H3 = self.Wh.shape\n",
    "        Wxz, Wxr, Wx = self.Wx[:, :H], self.Wx[:, H:2 * H], self.Wx[:, 2 * H:]\n",
    "        Whz, Whr, Wh = self.Wh[:, :H], self.Wh[:, H:2 * H], self.Wh[:, 2 * H:]\n",
    "        x, h_prev, z, r, h_hat = self.cache\n",
    "\n",
    "        dh_hat =dh_next * z\n",
    "        dh_prev = dh_next * (1-z)\n",
    "\n",
    "        # tanh\n",
    "        dt = dh_hat * (1 - h_hat ** 2)\n",
    "        dWh = np.dot((r * h_prev).T, dt)\n",
    "        dhr = np.dot(dt, Wh.T)\n",
    "        dWx = np.dot(x.T, dt)\n",
    "        dx = np.dot(dt, Wx.T)\n",
    "        dh_prev += r * dhr\n",
    "\n",
    "        # update gate(z)\n",
    "        dz = dh_next * h_hat - dh_next * h_prev\n",
    "        dt = dz * z * (1-z)\n",
    "        dWhz = np.dot(h_prev.T, dt)\n",
    "        dh_prev += np.dot(dt, Whz.T)\n",
    "        dWxz = np.dot(x.T, dt)\n",
    "        dx += np.dot(dt, Wxz.T)\n",
    "\n",
    "        # rest gate(r)\n",
    "        dr = dhr * h_prev\n",
    "        dt = dr * r * (1-r)\n",
    "        dWhr = np.dot(h_prev.T, dt)\n",
    "        dh_prev += np.dot(dt, Whr.T)\n",
    "        dWxr = np.dot(x.T, dt)\n",
    "        dx += np.dot(dt, Wxr.T)\n",
    "\n",
    "        self.dWx = np.hstack((dWxz, dWxr, dWx))\n",
    "        self.dWh = np.hstack((dWhz, dWhr, dWh))\n",
    "\n",
    "        return dx, dh_prev\n",
    "\n",
    "\n",
    "class TimeGRU:\n",
    "    def __init__(self, Wx, Wh, stateful=False):\n",
    "        self.Wx, self.Wh = Wx, Wh\n",
    "        selfdWx, self.dWh = None, None\n",
    "        self.layers = None\n",
    "        self.h, self.dh = None, None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        N, T, D = xs.shape\n",
    "        H, H3 = self.Wh.shape\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = GRU(self.Wx, self.Wh)\n",
    "            self.h = layer.forward(xs[:, t, :], self.h)\n",
    "            hs[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        N, T, H = dhs.shape\n",
    "        D = self.Wx.shape[0]\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        self.dWx, self.dWh = 0, 0\n",
    "\n",
    "        dh = 0\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
    "\n",
    "            dxs[:, t, :] = dx\n",
    "            self.dWx += layer.dWx\n",
    "            self.dWh += layer.dWh\n",
    "\n",
    "        self.dh = dh\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "\n",
    "\n",
    "class Simple_TimeSoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T, V = xs.shape\n",
    "        layers = []\n",
    "        loss = 0\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = SoftmaxWithLoss()\n",
    "            loss += layer.forward(xs[:, t, :], ts[:, t])\n",
    "            layers.append(layer)\n",
    "        loss /= T\n",
    "\n",
    "        self.cache = (layers, xs)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        layers, xs = self.cache\n",
    "        N, T, V = xs.shape\n",
    "        dxs = np.empty(xs.shape, dtype='f')\n",
    "\n",
    "        dout *= 1/T\n",
    "        for t in range(T):\n",
    "            layer = layers[t]\n",
    "            dxs[:, t, :] = layer.backward(dout)\n",
    "\n",
    "        return dxs\n",
    "\n",
    "\n",
    "class Simple_TimeAffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W, self.b = W, b\n",
    "        self.dW, self.db = None, None\n",
    "        self.layers = None\n",
    "\n",
    "    def forward(self, xs):\n",
    "        N, T, D = xs.shape\n",
    "        D, M = self.W.shape\n",
    "\n",
    "        self.layers = []\n",
    "        out = np.empty((N, T, M), dtype='f')\n",
    "        for t in range(T):\n",
    "            layer = Affine(self.W, self.b)\n",
    "            out[:, t, :] = layer.forward(xs[:, t, :])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, M = dout.shape\n",
    "        D, M = self.W.shape\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        self.dW, self.db = 0, 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dxs[:, t, :] = layer.backward(dout[:, t, :])\n",
    "\n",
    "            self.dW += layer.dW\n",
    "            self.db += layer.db\n",
    "\n",
    "        return dxs\n",
    "\n",
    "#================================================================================#\n",
    "#trainer.py\n",
    "#================================================================================#\n",
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "#from common.np import *  # import numpy as np\n",
    "#from common.util import clip_grads\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            # 뒤섞기\n",
    "            idx = numpy.random.permutation(numpy.arange(data_size))\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "                # 기울기 구해 매개변수 갱신\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.params, model.grads)  # 공유된 가중치를 하나로 모음\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # 평가\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    avg_loss = total_loss / loss_count\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print('| 에폭 %d |  반복 %d / %d | 시간 %d[s] | 손실 %.2f'\n",
    "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
    "                    self.loss_list.append(float(avg_loss))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = numpy.arange(len(self.loss_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.loss_list, label='train')\n",
    "        plt.xlabel('반복 (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('손실')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class RnnlmTrainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.time_idx = None\n",
    "        self.ppl_list = None\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def get_batch(self, x, t, batch_size, time_size):\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "\n",
    "        data_size = len(x)\n",
    "        jump = data_size // batch_size\n",
    "        offsets = [i * jump for i in range(batch_size)]  # 배치에서 각 샘플을 읽기 시작하는 위치\n",
    "\n",
    "        for time in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, time] = x[(offset + self.time_idx) % data_size]\n",
    "                batch_t[i, time] = t[(offset + self.time_idx) % data_size]\n",
    "            self.time_idx += 1\n",
    "        return batch_x, batch_t\n",
    "\n",
    "    def fit(self, xs, ts, max_epoch=10, batch_size=20, time_size=35,\n",
    "            max_grad=None, eval_interval=20):\n",
    "        data_size = len(xs)\n",
    "        max_iters = data_size // (batch_size * time_size)\n",
    "        self.time_idx = 0\n",
    "        self.ppl_list = []\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            for iters in range(max_iters):\n",
    "                batch_x, batch_t = self.get_batch(xs, ts, batch_size, time_size)\n",
    "\n",
    "                # 기울기를 구해 매개변수 갱신\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.params, model.grads)  # 공유된 가중치를 하나로 모음\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # 퍼플렉서티 평가\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    ppl = np.exp(total_loss / loss_count)\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print('| 에폭 %d |  반복 %d / %d | 시간 %d[s] | 퍼플렉서티 %.2f'\n",
    "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, ppl))\n",
    "                    self.ppl_list.append(float(ppl))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = numpy.arange(len(self.ppl_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.ppl_list, label='train')\n",
    "        plt.xlabel('반복 (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('퍼플렉서티')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def remove_duplicate(params, grads):\n",
    "    '''\n",
    "    매개변수 배열 중 중복되는 가중치를 하나로 모아\n",
    "    그 가중치에 대응하는 기울기를 더한다.\n",
    "    '''\n",
    "    params, grads = params[:], grads[:]  # copy list\n",
    "\n",
    "    while True:\n",
    "        find_flg = False\n",
    "        L = len(params)\n",
    "\n",
    "        for i in range(0, L - 1):\n",
    "            for j in range(i + 1, L):\n",
    "                # 가중치 공유 시\n",
    "                if params[i] is params[j]:\n",
    "                    grads[i] += grads[j]  # 경사를 더함\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                # 가중치를 전치행렬로 공유하는 경우(weight tying)\n",
    "                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
    "                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
    "                    grads[i] += grads[j].T\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "\n",
    "                if find_flg: break\n",
    "            if find_flg: break\n",
    "\n",
    "        if not find_flg: break\n",
    "\n",
    "    return params, grads\n",
    "\n",
    "\n",
    "#================================================================================#\n",
    "#util.py\n",
    "#================================================================================#\n",
    "\n",
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "#from common.np import *\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word\n",
    "\n",
    "\n",
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    '''코사인 유사도 산출\n",
    "\n",
    "    :param x: 벡터\n",
    "    :param y: 벡터\n",
    "    :param eps: '0으로 나누기'를 방지하기 위한 작은 값\n",
    "    :return:\n",
    "    '''\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
    "    return np.dot(nx, ny)\n",
    "\n",
    "\n",
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    '''유사 단어 검색\n",
    "\n",
    "    :param query: 쿼리(텍스트)\n",
    "    :param word_to_id: 단어에서 단어 ID로 변환하는 딕셔너리\n",
    "    :param id_to_word: 단어 ID에서 단어로 변환하는 딕셔너리\n",
    "    :param word_matrix: 단어 벡터를 정리한 행렬. 각 행에 해당 단어 벡터가 저장되어 있다고 가정한다.\n",
    "    :param top: 상위 몇 개까지 출력할 지 지정\n",
    "    '''\n",
    "    if query not in word_to_id:\n",
    "        print('%s(을)를 찾을 수 없습니다.' % query)\n",
    "        return\n",
    "\n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    vocab_size = len(id_to_word)\n",
    "\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "\n",
    "    # 코사인 유사도를 기준으로 내림차순으로 출력\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "\n",
    "\n",
    "def convert_one_hot(corpus, vocab_size):\n",
    "    '''원핫 표현으로 변환\n",
    "\n",
    "    :param corpus: 단어 ID 목록(1차원 또는 2차원 넘파이 배열)\n",
    "    :param vocab_size: 어휘 수\n",
    "    :return: 원핫 표현(2차원 또는 3차원 넘파이 배열)\n",
    "    '''\n",
    "    N = corpus.shape[0]\n",
    "\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    '''동시발생 행렬 생성\n",
    "\n",
    "    :param corpus: 말뭉치(단어 ID 목록)\n",
    "    :param vocab_size: 어휘 수\n",
    "    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n",
    "    :return: 동시발생 행렬\n",
    "    '''\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "\n",
    "    return co_matrix\n",
    "\n",
    "\n",
    "def ppmi(C, verbose=False, eps = 1e-8):\n",
    "    '''PPMI(점별 상호정보량) 생성\n",
    "\n",
    "    :param C: 동시발생 행렬\n",
    "    :param verbose: 진행 상황을 출력할지 여부\n",
    "    :return:\n",
    "    '''\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis=0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
    "            M[i, j] = max(0, pmi)\n",
    "\n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total//100) == 0:\n",
    "                    print('%.1f%% 완료' % (100*cnt/total))\n",
    "    return M\n",
    "\n",
    "\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    '''맥락과 타깃 생성\n",
    "\n",
    "    :param corpus: 말뭉치(단어 ID 목록)\n",
    "    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n",
    "    :return:\n",
    "    '''\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "\n",
    "    return np.array(contexts), np.array(target)\n",
    "\n",
    "\n",
    "def to_cpu(x):\n",
    "    import numpy\n",
    "    if type(x) == numpy.ndarray:\n",
    "        return x\n",
    "    return np.asnumpy(x)\n",
    "\n",
    "\n",
    "def to_gpu(x):\n",
    "    import cupy\n",
    "    if type(x) == cupy.ndarray:\n",
    "        return x\n",
    "    return cupy.asarray(x)\n",
    "\n",
    "\n",
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate\n",
    "\n",
    "\n",
    "def eval_perplexity(model, corpus, batch_size=10, time_size=35):\n",
    "    print('퍼플렉서티 평가 중 ...')\n",
    "    corpus_size = len(corpus)\n",
    "    total_loss, loss_cnt = 0, 0\n",
    "    max_iters = (corpus_size - 1) // (batch_size * time_size)\n",
    "    jump = (corpus_size - 1) // batch_size\n",
    "\n",
    "    for iters in range(max_iters):\n",
    "        xs = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "        ts = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "        time_offset = iters * time_size\n",
    "        offsets = [time_offset + (i * jump) for i in range(batch_size)]\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                xs[i, t] = corpus[(offset + t) % corpus_size]\n",
    "                ts[i, t] = corpus[(offset + t + 1) % corpus_size]\n",
    "\n",
    "        try:\n",
    "            loss = model.forward(xs, ts, train_flg=False)\n",
    "        except TypeError:\n",
    "            loss = model.forward(xs, ts)\n",
    "        total_loss += loss\n",
    "\n",
    "        sys.stdout.write('\\r%d / %d' % (iters, max_iters))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    print('')\n",
    "    ppl = np.exp(total_loss / max_iters)\n",
    "    return ppl\n",
    "\n",
    "\n",
    "def eval_seq2seq(model, question, correct, id_to_char,\n",
    "                 verbos=False, is_reverse=False):\n",
    "    correct = correct.flatten()\n",
    "    # 머릿글자\n",
    "    start_id = correct[0]\n",
    "    correct = correct[1:]\n",
    "    guess = model.generate(question, start_id, len(correct))\n",
    "\n",
    "    # 문자열로 변환\n",
    "    question = ''.join([id_to_char[int(c)] for c in question.flatten()])\n",
    "    correct = ''.join([id_to_char[int(c)] for c in correct])\n",
    "    guess = ''.join([id_to_char[int(c)] for c in guess])\n",
    "\n",
    "    if verbos:\n",
    "        if is_reverse:\n",
    "            question = question[::-1]\n",
    "\n",
    "        colors = {'ok': '\\033[92m', 'fail': '\\033[91m', 'close': '\\033[0m'}\n",
    "        print('Q', question)\n",
    "        print('T', correct)\n",
    "\n",
    "        is_windows = os.name == 'nt'\n",
    "\n",
    "        if correct == guess:\n",
    "            mark = colors['ok'] + '☑' + colors['close']\n",
    "            if is_windows:\n",
    "                mark = 'O'\n",
    "            print(mark + ' ' + guess)\n",
    "        else:\n",
    "            mark = colors['fail'] + '☒' + colors['close']\n",
    "            if is_windows:\n",
    "                mark = 'X'\n",
    "            print(mark + ' ' + guess)\n",
    "        print('---')\n",
    "\n",
    "    return 1 if guess == correct else 0\n",
    "\n",
    "\n",
    "def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):\n",
    "    for word in (a, b, c):\n",
    "        if word not in word_to_id:\n",
    "            print('%s(을)를 찾을 수 없습니다.' % word)\n",
    "            return\n",
    "\n",
    "    print('\\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')\n",
    "    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n",
    "    query_vec = b_vec - a_vec + c_vec\n",
    "    query_vec = normalize(query_vec)\n",
    "\n",
    "    similarity = np.dot(word_matrix, query_vec)\n",
    "\n",
    "    if answer is not None:\n",
    "        print(\"==>\" + answer + \":\" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))\n",
    "\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if np.isnan(similarity[i]):\n",
    "            continue\n",
    "        if id_to_word[i] in (a, b, c):\n",
    "            continue\n",
    "        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    if x.ndim == 2:\n",
    "        s = np.sqrt((x * x).sum(1))\n",
    "        x /= s.reshape((s.shape[0], 1))\n",
    "    elif x.ndim == 1:\n",
    "        s = np.sqrt((x * x).sum())\n",
    "        x /= s\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
